{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_from_scratch_without_numpy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isshiki/neural-network-by-code/blob/main/01-forward-prop/nn_from_scratch_without_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2021-2022 Digital Advantage - Deep Insider."
      ],
      "metadata": {
        "id": "I8yy30L04inZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_QLRIOK4fVd"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "連載『ニューラルネットワーク入門』\n",
        "# 「コードで必ず分かるニューラルネットワーク（DNN）の逆伝播」\n"
      ],
      "metadata": {
        "id": "6Dza91D34tmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table valign=\"middle\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://atmarkit.itmedia.co.jp/ait/articles/2202/09/news027.html\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/manabu.svg\"/>Deep Insiderで記事を読む</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/isshiki/neural-network-by-code/blob/main/01-forward-prop/nn_from_scratch_without_numpy.ipynb\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/gcolab.svg\" />Google Colabで実行する</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://studiolab.sagemaker.aws/import/github/isshiki/neural-network-by-code/tree/main/01-forward-prop/nn_from_scratch_without_numpy.ipynb\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/astudiolab.svg\" />AWS  SageMaker Studio Labで実行する</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/isshiki/neural-network-by-code/blob/main/01-forward-prop/nn_from_scratch_without_numpy.ipynb\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/github.svg\" />GitHubでソースコードを見る</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "XyoG00sPzIZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■本ノートブックの目的"
      ],
      "metadata": {
        "id": "_V_STJSgw46b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットワーク（以下、ニューラルネット）の仕組みを、数学理論からではなく**Pythonコードから学ぶ**ことを狙っています。「難しい高校以降の数学は苦手だけど、コードなら読めるぜ！」という方にピッタリです。"
      ],
      "metadata": {
        "id": "ZAlfbhv1w6mI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●本ノートブックの特徴"
      ],
      "metadata": {
        "id": "jQlmkwbNxUle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 線形代数（linear algebra、行列演算）を使いません。つまり、NumPyを使いません。\n",
        "- 基本的に掛け算や足し算などの中学までの数学のみで、ニューラルネットのロジックをコーディングしていきます。\n",
        "- ※微分の導関数は、そのままコードとして記載することで、微分の計算は取り上げません。\n",
        "\n",
        "![図1　線形代数を使わないことによるメリット](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/01-forward-prop/images/01.png)"
      ],
      "metadata": {
        "id": "8X8ZSGoFxciU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■ニューラルネットワークの図"
      ],
      "metadata": {
        "id": "rjtsPfbhwyOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "基本的なニューラルネット（この例では、入力層：2、隠れ層：3、出力層：1）の図を確認しておきます。\n",
        "\n",
        "![図2　ニューラルネットワークの図（左：横描き、右：縦描き）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/01-forward-prop/images/02.png)"
      ],
      "metadata": {
        "id": "0dKydpNZydl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■訓練（学習）処理全体の実装"
      ],
      "metadata": {
        "id": "sT1m3Bseyytq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "深層「学習」のメインは、ニューラルネットの「訓練」処理ですよね。ここから書いていきます。"
      ],
      "metadata": {
        "id": "32U5YXJyQU08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず仮で、空の関数を定義して、コードが実行できるようにしておく\n",
        "def forward_prop(cache_mode=False):\n",
        "    \" 順伝播を行う関数。\"\n",
        "    return None, None, None\n",
        "\n",
        "y_true = 1.0  # 正解値\n",
        "def back_prop(y_true, cached_outs, cached_sums):\n",
        "    \" 逆伝播を行う関数。\"\n",
        "    return None, None\n",
        "\n",
        "LEARNING_RATE = 0.1 # 学習率（lr）\n",
        "def update_params(grads_w, grads_b, lr=0.1):\n",
        "    \" パラメーター（重みとバイアス）を更新する関数。\"\n",
        "    return None, None\n",
        "\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "# 訓練処理\n",
        "y_pred, cached_outs, cached_sums = forward_prop(cache_mode=True)  # （1）\n",
        "grads_w, grads_b = back_prop(y_true, cached_outs, cached_sums)  # （2）\n",
        "weights, biases = update_params(grads_w, grads_b, LEARNING_RATE)  # （3）\n",
        "\n",
        "print(f'予測値：{y_pred}')  # 予測値： None\n",
        "print(f'正解値：{y_true}')  # 正解値： 1.0"
      ],
      "metadata": {
        "id": "2GJPJd3EjnLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットの訓練に必要なことは、以下の3つだけ。\n",
        "\n",
        "1. **順伝播：** `forward_prop()◎`数として実装\n",
        "2. **逆伝播：** `back_prop()`関数として実装。損失（予測と正解の誤差）の計算はここで行う\n",
        "3. **パラメーター（重みとバイアス）の更新：** `update_params()`関数として実装。これによりモデルが**最適化**される\n"
      ],
      "metadata": {
        "id": "mtBTnQCjQuhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図3　訓練（学習）処理を示したニューラルネットワーク図](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/01-forward-prop/images/03.png)"
      ],
      "metadata": {
        "id": "yv5BZKpHPXIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##■モデルの定義と、仮の訓練データ"
      ],
      "metadata": {
        "id": "uL1t6_R_5vX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "入力層のノードが2個、隠れ層のノードが3個、出力層のノードが1個のモデル（`model`変数）を定義しましょう。"
      ],
      "metadata": {
        "id": "tZ76R4MbRKmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ニューラルネットワークは3層構成\n",
        "layers = [\n",
        "    2,  # 入力層の入力（特徴量）の数\n",
        "    3,  # 隠れ層1のノード（ニューロン）の数\n",
        "    1]  # 出力層のノードの数\n",
        "\n",
        "# 重みとバイアスの初期値\n",
        "weights = [\n",
        "    [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]], # 入力層→隠れ層1\n",
        "    [[0.0, 0.0, 0.0]] # 隠れ層1→出力層\n",
        "]\n",
        "biases = [\n",
        "    [0.0, 0.0, 0.0],  # 隠れ層1\n",
        "    [0.0]  # 出力層\n",
        "]\n",
        "\n",
        "# モデルを定義\n",
        "model = (layers, weights, biases)\n",
        "\n",
        "# 仮の訓練データ（1件分）を準備\n",
        "x = [0.05, 0.1]  # x_1とx_2の2つの特徴量"
      ],
      "metadata": {
        "id": "7sW0f2uH50eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■ステップ1. 順伝播の実装"
      ],
      "metadata": {
        "id": "Xm1tlcIiQOcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●1つのノードにおける順伝播の処理"
      ],
      "metadata": {
        "id": "1nxXuQ5nHW58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットの最小単位である「1つのノード」における順伝播の処理をコーディングしましょう。"
      ],
      "metadata": {
        "id": "e5qxZXE2RVQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず仮で、空の関数を定義して、コードが実行できるようにしておく\n",
        "def summation(x,weights, bias):\n",
        "    \" 重み付き線形和の関数。\"\n",
        "    return 0.0\n",
        "\n",
        "def sigmoid(x):\n",
        "    \" シグモイド関数。\"\n",
        "    return 0.0\n",
        "\n",
        "def identity(x):\n",
        "    \" 恒等関数。\"\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "w = [0.0, 0.0]  # 重み（仮の値）\n",
        "b = 0.0  # バイアス（仮の値）\n",
        "\n",
        "next_x = x  # 訓練データをノードへの入力に使う\n",
        "\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "# 1つのノードの処理（1）： 重み付き線形和\n",
        "node_sum = summation(next_x, w, b)\n",
        "\n",
        "# 1つのノードの処理（2）： 活性化関数\n",
        "is_hidden_layer = True\n",
        "if is_hidden_layer:\n",
        "    # 隠れ層（シグモイド関数）\n",
        "    node_out = sigmoid(node_sum)\n",
        "else:\n",
        "    # 出力層（恒等関数）\n",
        "    node_out = identity(node_sum)"
      ],
      "metadata": {
        "id": "I4-XMugzbUGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1つのノードの順伝播処理に必要なことは、以下の2つの数学関数だけ。\n",
        "\n",
        "1. **重み付き線形和の関数：** `summation()`関数として実装\n",
        "2. **活性化関数：** ここでは`sigmoid()`関数や`identity()`関数として実装"
      ],
      "metadata": {
        "id": "tPBzLn3hW0zU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図4　1つのニューロンにおける順伝播の処理を示した図](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/01-forward-prop/images/04.png)"
      ],
      "metadata": {
        "id": "V7DKfcYyPf1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●重み付き線形和"
      ],
      "metadata": {
        "id": "kCQTEdpGcv7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "重み付き線形和（weighted linear summation、以下では「線形和」と表記）とは、とは、あるノードへの複数の入力（$x_1$、$x_2$など）に、それぞれの重み（$w_1$、$w_2$など）を掛けて足し合わせて、最後にバイアス（$b$）を足した値です（上の図の左）。"
      ],
      "metadata": {
        "id": "VX8bRXh2Riek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summation(x, weights, bias):\n",
        "    \"\"\"\n",
        "    重み付き線形和の関数。\n",
        "    ※1データ分、つまりxとweightsは「一次元リスト」という前提。\n",
        "    - 引数：\n",
        "    x： 入力データをリスト値（各要素はfloat値）で指定する。\n",
        "    weights： 重みをリスト値（各要素はfloat値）で指定する。\n",
        "    bias： バイアスをfloat値で指定する。\n",
        "    - 戻り値：\n",
        "    線形和の計算結果をfloat値で返す。\n",
        "    \"\"\"\n",
        "    linear_sum = 0.0\n",
        "    for x_i, w_i in zip(x, weights):\n",
        "        linear_sum += x_i * w_i  # iは「番号」（数学は基本的に1スタート）\n",
        "        # print(f'x_i({x_i})×w_i({w_i})＋', end='')\n",
        "    linear_sum += bias\n",
        "    # print(f'b({bias})', end='')\n",
        "    return linear_sum\n",
        "\n",
        "# 線形代数を使う場合のコード例：\n",
        "# linear_sum = np.dot(x, weights) + bias"
      ],
      "metadata": {
        "id": "DuXPPnolcHHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ついで、次回の逆伝播（の中で使う偏微分）で必要となる線形和（linear **sum**mation）の偏導関数（partial **der**ivative function）を実装しておきます。"
      ],
      "metadata": {
        "id": "VxKE5sNZSFq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_der(x, weights, bias, with_respect_to='w'):\n",
        "    \"\"\"\n",
        "    重み付き線形和の関数の偏導関数。\n",
        "    ※1データ分、つまりxとweightsは「一次元リスト」という前提。\n",
        "    - 引数：\n",
        "    x： 入力データをリスト値で指定する。\n",
        "    weights：  重みをリスト値で指定する。\n",
        "    bias: バイアスをfloat値で指定する。\n",
        "    with_respect_to: 何に関して偏微分するかを指定する。\n",
        "        'w'＝ 重み、'b'＝ バイアス、'x'＝ 入力。\n",
        "    - 戻り値：\n",
        "    with_respect_toが'w'や'x'の場合はリスト値で、'b'の場合はfloat値で\n",
        "        線形和の偏微分の計算結果（偏微分係数）を返す。\n",
        "    \"\"\"    \n",
        "    if with_respect_to == 'w':\n",
        "        return x  # 線形和uを各重みw_iで偏微分するとx_iになる（iはノード番号）\n",
        "    elif with_respect_to == 'b':\n",
        "        return 1.0  # 線形和uをバイアスbで偏微分すると1になる\n",
        "    elif with_respect_to == 'x':\n",
        "        return weights  # 線形和uを各入力x_iで偏微分するとw_iになる"
      ],
      "metadata": {
        "id": "Ob-1saVLdtre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●活性化関数：シグモイド関数"
      ],
      "metadata": {
        "id": "09TqjNnXidxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "隠れ層では、最も基礎的なシグモイド関数（Sigmoid function）を固定的に使うことにします。導関数も実装しておきます。"
      ],
      "metadata": {
        "id": "HeCjZU9STJOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    シグモイド関数。\n",
        "    - 引数：\n",
        "    x： 入力データをfloat値で指定する。\n",
        "    - 戻り値：\n",
        "    シグモイド関数の計算結果をfloat値で返す。\n",
        "    \"\"\"\n",
        "    return 1.0 / (1.0 + math.exp(-x))\n",
        "\n",
        "# 線形代数の場合はmathをnpに変える（事前にimport numpy as np）"
      ],
      "metadata": {
        "id": "RAXhKRDyif96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_der(x):\n",
        "    \"\"\"\n",
        "    シグモイド関数の（偏）導関数。\n",
        "    - 引数：\n",
        "    x： 入力データをfloat値で指定する。\n",
        "    - 戻り値：\n",
        "    シグモイド関数の（偏）微分の計算結果（微分係数）をfloat値で返す。\n",
        "    \"\"\"\n",
        "    output = sigmoid(x)\n",
        "    return output * (1.0 - output)"
      ],
      "metadata": {
        "id": "dPghF0t5iiMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●活性化関数：恒等関数"
      ],
      "metadata": {
        "id": "aNH6Mq72jsTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "出力層では、回帰問題をイメージして、そのままの値を出力する活性化関数である恒等関数（Identity function）を使用します。導関数も実装しておきます。"
      ],
      "metadata": {
        "id": "rPevusfVTaz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identity(x):\n",
        "    \"\"\"\n",
        "    恒等関数の関数。\n",
        "    - 引数：\n",
        "    x： 入力データをfloat値で指定する。\n",
        "    - 戻り値：\n",
        "    恒等関数の計算結果（そのまま）をfloat値で返す。\n",
        "    \"\"\"\n",
        "    return x"
      ],
      "metadata": {
        "id": "8oVoxKncjrzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_der(x):\n",
        "    \"\"\"\n",
        "    恒等関数の（偏）導関数。\n",
        "    - 引数：\n",
        "    x： 入力データをfloat値で指定する。\n",
        "    - 戻り値：\n",
        "    恒等関数の（偏）微分の計算結果（微分係数）をfloat値で返す。\n",
        "    \"\"\"\n",
        "    return 1.0"
      ],
      "metadata": {
        "id": "B_4avMTJjvog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●順伝播の処理全体の実装"
      ],
      "metadata": {
        "id": "j2nmNTn0OmPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットには、層があり、その中に複数のノードが存在するという構造です。  従って、\n",
        "\n",
        "- 各層を1つずつ処理する`for`ループと、  \n",
        "  - 層の中のノードを1つずつ処理する`for`ループの2段階構造が必要で、\n",
        "    - その中に「1つのノードにおける順伝播の処理」\n",
        "\n",
        "を記述すればよいわけです。"
      ],
      "metadata": {
        "id": "_jmw6snqTsJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(layers, weights, biases, x, cache_mode=False):\n",
        "    \"\"\"\n",
        "    順伝播を行う関数。\n",
        "    - 引数：\n",
        "    (layers, weights, biases)： モデルを指定する。\n",
        "    x： 入力データを指定する。\n",
        "    cache_mode： 予測時はFalse、訓練時はTrueにする。これにより戻り値が変わる。\n",
        "    - 戻り値：\n",
        "    cache_modeがFalse時は予測値のみを返す。True時は、予測値だけでなく、\n",
        "        キャッシュに記録済みの線形和（Σ）値と、活性化関数の出力値も返す。\n",
        "    \"\"\"\n",
        "\n",
        "    cached_sums = []  # 記録した全ノードの線形和（Σ）の値\n",
        "    cached_outs = []  # 記録した全ノードの活性化関数の出力値\n",
        "\n",
        "    # まずは、入力層を順伝播する\n",
        "    # print(f'■第1層（入力層）-全て（{len(x)}個）の特徴量：')\n",
        "    # print(f'　●入力データ： ', end='')\n",
        "    cached_outs.append(x)  # 何も処理せずに出力値を記録\n",
        "    # print(f'何もしない＝out({x})')\n",
        "    next_x = x  # 現在の層の出力（x）＝次の層への入力（next_x）\n",
        "\n",
        "    # 次に、隠れ層や出力層を順伝播する\n",
        "    SKIP_INPUT_LAYER = 1\n",
        "    for layer_i, layer in enumerate(layers):  # 各層を処理\n",
        "        if layer_i == 0:\n",
        "            continue  # 入力層は上で処理済み\n",
        "\n",
        "        # 各レイヤーのノードごとに処理を行う\n",
        "        sums = []\n",
        "        outs = []\n",
        "        for node_i in range(layer):  # 層の中の各ノードを処理\n",
        "            # print(f'■第{layer_i+1}層-第{node_i+1}ノード：')\n",
        "\n",
        "            # ノードごとの重みとバイアスを取得\n",
        "            w = weights[layer_i-SKIP_INPUT_LAYER][node_i]\n",
        "            b = biases[layer_i-SKIP_INPUT_LAYER][node_i]\n",
        "\n",
        "            # 1つのノードの処理（1）： 重み付き線形和\n",
        "            # print(f'　●重み付き線形和： ', end='')\n",
        "            node_sum = summation(next_x, w, b)\n",
        "            # print(f'＝sum({node_sum})')\n",
        "\n",
        "            # 1つのノードの処理（2）： 活性化関数\n",
        "            if layer_i < len(layers)-1:  # -1は出力層以外の意味\n",
        "                # 隠れ層（シグモイド関数）\n",
        "                # print(f'　●活性化関数（隠れ層はシグモイド関数）： ', end='')\n",
        "                node_out = sigmoid(node_sum)\n",
        "                # print(f'sigmoid({node_sum})＝out({node_out})')\n",
        "            else:\n",
        "                # 出力層（恒等関数）\n",
        "                # print(f'　●活性化関数（出力層は恒等関数）： ', end='')\n",
        "                node_out = identity(node_sum)\n",
        "                # print(f'identity({node_sum})＝out({node_out})')\n",
        "\n",
        "            # 各ノードの線形和と（活性化関数の）出力をリストにまとめていく\n",
        "            sums.append(node_sum)\n",
        "            outs.append(node_out)\n",
        "\n",
        "        # 各層内の全ノードの線形和と出力を記録\n",
        "        cached_sums.append(sums)\n",
        "        cached_outs.append(outs)\n",
        "        next_x = outs  # 現在の層の出力（outs）＝次の層への入力（next_x）\n",
        "\n",
        "    if cache_mode:\n",
        "        return (cached_outs[-1], cached_outs, cached_sums)\n",
        "\n",
        "    return cached_outs[-1]\n",
        "\n",
        "\n",
        "# 訓練時の（1）順伝播の実行例\n",
        "y_pred, cached_outs, cached_sums = forward_prop(*model, x, cache_mode=True)\n",
        "# ※先ほど作成したモデルと訓練データを引数で受け取るよう改変した\n",
        "\n",
        "print(f'cached_outs={cached_outs}')\n",
        "print(f'cached_sums={cached_sums}')\n",
        "# 出力例：\n",
        "# cached_outs=[[0.05, 0.1], [0.5, 0.5, 0.5], [0.0]]  # 入力層／隠れ層1／出力層\n",
        "# cached_sums=[[0.0, 0.0, 0.0], [0.0]]  # 隠れ層1／出力層（※入力層はない）"
      ],
      "metadata": {
        "id": "OlfyIL3VkGXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "数値が**0.0**ばかりなので、別の計算パターンのコードも入れておきました。"
      ],
      "metadata": {
        "id": "sEwU0TN8U7ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 記事にはないが、別の計算パターンでもチェックしてみよう\n",
        "x3 = [0.05, 0.1]\n",
        "layers3 = [2, 2, 2]\n",
        "weights3 = [\n",
        "    [[0.15, 0.2], [0.25, 0.3]],\n",
        "    [[0.4, 0.45], [0.5,0.55]]\n",
        "]\n",
        "biases3 = [[0.35, 0.35], [0.6, 0.6]]\n",
        "model3 = (layers3, weights3, biases3)\n",
        "\n",
        "y_pred3, cached_outs3, cached_sums3 = forward_prop(*model3, x3, cache_mode=True)\n",
        "print(f'y_pred={y_pred3}')\n",
        "print(f'cached_outs={cached_outs3}')\n",
        "print(f'cached_sums={cached_sums3}')\n",
        "# y_pred=[1.10590596705977, 1.2249214040964653]\n",
        "# cached_outs=[[0.05, 0.1], [0.5932699921071872, 0.596884378259767], [1.10590596705977, 1.2249214040964653]]\n",
        "# cached_sums=[[0.3775, 0.39249999999999996], [1.10590596705977, 1.2249214040964653]]"
      ],
      "metadata": {
        "id": "7f2urrsRWazm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここまでのコード中に仕込んでいる`print()`関数（※全てコメントアウトしています）のコメントを解除すると、以下のように途中の計算内容が順番にテキスト出力されます。計算内容の検証用の機能です。"
      ],
      "metadata": {
        "id": "9wOPPW0YVcH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![別の計算パターンの出力例](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/01-forward-prop/images/notebook-01.png)"
      ],
      "metadata": {
        "id": "aRfWOdtGPm41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●順伝播による予測の実行例"
      ],
      "metadata": {
        "id": "8Y2y79vNGoUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "非常にシンプルで原始的な実装ですが、このように任意の層数とノード数の全結合のDNN（Deep Neural Network）のアーキテクチャーを定義して、DNNモデルによる予測が行えます。"
      ],
      "metadata": {
        "id": "ym7sZcmzWW7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 異なるDNNアーキテクチャーを定義してみる\n",
        "layers2 = [\n",
        "    2,  # 入力層の入力（特徴量）の数\n",
        "    3,  # 隠れ層1のノード（ニューロン）の数\n",
        "    2,  # 隠れ層2のノード（ニューロン）の数\n",
        "    1]  # 出力層のノードの数\n",
        "\n",
        "# 重みとバイアスの初期値\n",
        "weights2 = [\n",
        "    [[-0.2, 0.4], [-0.4, -0.5], [-0.4, -0.5]], # 入力層→隠れ層1\n",
        "    [[-0.2, 0.4, 0.9], [-0.4, -0.5, -0.2]], # 隠れ層1→隠れ層2\n",
        "    [[-0.5, 1.0]] # 隠れ層2→出力層\n",
        "]\n",
        "biases2 = [\n",
        "    [0.1, -0.1, 0.1],  # 隠れ層1\n",
        "    [0.2, -0.2],  # 隠れ層2\n",
        "    [0.3]  # 出力層\n",
        "]\n",
        "\n",
        "# モデルを定義\n",
        "model2 = (layers2, weights2, biases2)\n",
        "\n",
        "# 仮の訓練データ（1件分）を準備\n",
        "x2 = [2.3, 1.5]  # x_1とx_2の2つの特徴量\n",
        "\n",
        "# 予測時の（1）順伝播の実行例\n",
        "y_pred = forward_prop(*model2, x2)\n",
        "print(y_pred)  # 予測値\n",
        "# 出力例：\n",
        "# [0.3828840428423274]"
      ],
      "metadata": {
        "id": "U0sgGfN2kOa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●今後のステップの準備：関数への仮引数の追加"
      ],
      "metadata": {
        "id": "EbO0sBxLVxjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(layers, weights, biases, y_true, cached_outs, cached_sums):\n",
        "    \" 逆伝播を行う関数。\"\n",
        "    return None, None\n",
        "\n",
        "def update_params(layers, weights, biases, grads_w, grads_b, lr=0.1):\n",
        "    \" パラメーター（重みとバイアス）を更新する関数。\"\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "ap_S235RV36g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■ステップ2. 逆伝播の実装"
      ],
      "metadata": {
        "id": "rgju9tQge_eS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●逆伝播の目的と全体像"
      ],
      "metadata": {
        "id": "yERpCp1COZhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "逆伝播の目的は、誤差（厳密には予測値に関する損失関数の偏微分係数）などの数値（本ノートブックでは**誤差情報**と呼ぶ）をニューラルネットに逆方向で流すこと（＝逆伝播）によって「**重みとバイアスの勾配を計算すること**」です（下の図）。"
      ],
      "metadata": {
        "id": "aKNJrZmiOsb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図5　「逆伝播の流れ」のイメージ（左：ネットワーク図、右：対応する処理／数学計算）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/05.png)"
      ],
      "metadata": {
        "id": "nAs9qpq4OiOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "順伝播（`forward_prop()`関数）では、計算途中に出た計算結果である「予測値（`y_pred`）」や「各ノードでの活性化関数の出力値（`cached_outs`）」と「線形和の値（`cached_sums`）」を返すだけでした。\n",
        "\n",
        "逆伝播（`back_prop()関数`）では、計算途中に出た計算結果である「各ノードへの入力の勾配（＝逆伝播していく誤差情報）」だけでなく、「各重みの勾配（`grads_w`）」「各バイアスの勾配（`grads_b`）」の計算も必要です（下の図）。"
      ],
      "metadata": {
        "id": "5muXMKUnPHhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図6　逆伝播では各ノードへの入力／各重み／各バイアスの勾配を計算する](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/06.png)"
      ],
      "metadata": {
        "id": "x_CvxtXoO8Q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "逆伝播では、$x_1$／$w_1$／$x_2$／$w_2$／……／$x_n$／$w_n$／$b$という大量の変数に関して、損失関数の偏微分係数（＝勾配）を計算する必要があります。"
      ],
      "metadata": {
        "id": "ywEzTQaDQx5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●損失関数：二乗和誤差"
      ],
      "metadata": {
        "id": "CU4U3HUOgKNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "損失関数として、最も基礎的な二乗和誤差（SSE：Sum of Squared Error）を使うことにします。導関数も実装しておきます。"
      ],
      "metadata": {
        "id": "Mh5SWGHqG7db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sseloss(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    二乗和誤差（Sum of Squared Error）の関数。\n",
        "    - 引数：\n",
        "    y_pred： モデルの最終出力値（output）＝予測値（prediction）。\n",
        "    y_true： 目的となる値（target）＝正解値（truth、label）。\n",
        "    - 戻り値：\n",
        "    二乗和誤差の計算結果をfloat値で返す。\n",
        "    \"\"\"\n",
        "    return 0.5 * (y_pred - y_true) ** 2"
      ],
      "metadata": {
        "id": "EQ7Uo6wpgKqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sseloss_der(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    二乗和誤差（Sum of Squared Error）の偏導関数。予測値（y_pred）に関して二乗和誤差関数（sseloss()）を偏微分する。\n",
        "    - 引数：\n",
        "    y_pred： モデルの最終出力値（output）＝予測値（predicted value）。\n",
        "    y_true： 目的となる値（target）＝正解値（true/actual value、label）。\n",
        "    - 戻り値：\n",
        "    二乗和誤差の偏微分の計算結果（偏微分係数）をfloat値で返す。\n",
        "    \"\"\"\n",
        "\n",
        "    return y_pred - y_true"
      ],
      "metadata": {
        "id": "-P4gEPUXG-p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "偏導関数の式`y_pred - y_true`は、予測値と正解値の「誤差（Error、ズレ）」となっています。\n",
        "\n",
        "**誤差逆伝播法**（error backpropagation）とは、この「誤差」の数値（厳密には、予測値に関しての損失関数の偏微分係数）が誤差情報としてニューラルネットワークを「逆」向きに「伝播」していく過程で、本来の目的である各重みと各バイアスの勾配を求める方法です。\n"
      ],
      "metadata": {
        "id": "b0AihXRHYECg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図7　各ノードでの逆伝播の処理はワンパターン](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/07.png)"
      ],
      "metadata": {
        "id": "36DaQKcEX1xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●1つのノードにおける逆伝播の処理"
      ],
      "metadata": {
        "id": "yW0fBcdUX5qY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`損失関数( 活性化関数( 線形和関数( 入力、重み、バイアス ) ) )`という入れ子の関数は数学で**合成関数**と呼ばれます。\n",
        "\n",
        "合成関数を微分するときの公式が**連鎖律**です。連鎖律を使うと、まるでマジックのように各関数の偏微分係数の掛け算だけの式に変化します（下の図）。"
      ],
      "metadata": {
        "id": "fR4sjl1AZILm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図8　連鎖律を使うと各関数の偏微分の掛け算になる（各重みに関して損失関数を偏微分する例）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/08.png)"
      ],
      "metadata": {
        "id": "1LmiDrcAXmvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上の図は各重みに関して損失関数を偏微分する例ですが、各バイアスや各入力に関して損失関数を偏微分する際も連鎖律の形はほぼ同じです（下の図）。ただし入力については、前の層のノードごとに、今の層からの全てのエッジから来る各誤差情報（偏微分係数）を合計する必要があるので注意してください。"
      ],
      "metadata": {
        "id": "R46Y9A8VDatf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図9　各重み／バイアス／入力に関して損失関数を偏微分する場合の連鎖律の形）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/09.png)"
      ],
      "metadata": {
        "id": "B7ScKvwnCU3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "各層における各ノードの計算は、\n",
        "\n",
        "　　「逆伝播していく誤差情報」×「活性化関数の偏微分」×「線形和関数の偏微分」\n",
        "\n",
        "という掛け算に共通化できます（下の図）。"
      ],
      "metadata": {
        "id": "WfZyaK7PDJTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図10　各層の各ノードでの計算パターンは共通化できる（出力層や隠れ層で入力の勾配を計算する例）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/10.png)"
      ],
      "metadata": {
        "id": "RDyy6F8iCa0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "出力層から隠れ層まで全て、以下の4工程のワンパターンで実装できます（下の図）。\n",
        "\n",
        "1. 逆伝播していく誤差情報\n",
        "2. 活性化関数を偏微分\n",
        "3. 線形和を重み／バイアス／入力で偏微分\n",
        "4. 各重み／バイアス／各入力の勾配を計算"
      ],
      "metadata": {
        "id": "GN0XAc9qCqKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図11　「逆伝播の流れ」の実装内容](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/11.png)"
      ],
      "metadata": {
        "id": "CkZkdGGrCgVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●（1）逆伝播していく誤差情報"
      ],
      "metadata": {
        "id": "MIUf0sjEBtK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "layer_i = 2  # 2：出力層、1：隠れ層1、0：入力層\n",
        "layer_max_i = 2  # 最後の層（＝出力層）のインデックス\n",
        "is_output_layer = (layer_i == layer_max_i)  # 出力層か（True）、隠れ層か（False）\n",
        "\n",
        "# 入力層／隠れ層1／出力層にある各ノードの（活性化関数の）出力値\n",
        "cached_outs = [[0.05, 0.1], [0.5, 0.5, 0.5], [0.0]]\n",
        "y_true = [1.0]  # 正解値\n",
        "grads_x = []  # 入力の勾配\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "if is_output_layer:\n",
        "    # 出力層（損失関数の偏微分係数）\n",
        "    back_error = []  # 逆伝播していく誤差情報\n",
        "    y_pred = cached_outs[layer_i]\n",
        "    for output, target in zip(y_pred, y_true):\n",
        "        loss_der = sseloss_der(output, target)  # 誤差情報\n",
        "        back_error.append(loss_der)\n",
        "else:\n",
        "    # 隠れ層（次の層への入力の偏微分係数）\n",
        "    back_error = grads_x[-1]  # 最後に追加された入力の勾配"
      ],
      "metadata": {
        "id": "otsQU8OEBpe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "※（1）は層ごとにまとめての処理です。以下からの（2）～（4）はノードごとの処理になります。"
      ],
      "metadata": {
        "id": "nFV7n9JaZmo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●（2）活性化関数を偏微分"
      ],
      "metadata": {
        "id": "ADqZzyY4B3Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "SKIP_INPUT_LAYER = 1  # 入力層を飛ばす\n",
        "cached_sums = [[0.0, 0.0, 0.0], [0.0]]  # 隠れ層1／出力層（※入力層はない）\n",
        "node_sum = cached_sums[layer_max_i - SKIP_INPUT_LAYER]  # 出力層\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "if is_output_layer:\n",
        "    # 出力層（恒等関数の微分）\n",
        "    active_der = identity_der(node_sum)\n",
        "else:\n",
        "    # 隠れ層（シグモイド関数の微分）\n",
        "    active_der = sigmoid_der(node_sum)"
      ],
      "metadata": {
        "id": "z4iP49xCB4bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●（3）線形和を重み／バイアス／入力で偏微分"
      ],
      "metadata": {
        "id": "C412ZRuXCAr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "PREV_LAYER = 1  # 前の層を指定するため\n",
        "node_i = 0  # ノード番号\n",
        "\n",
        "# 重みとバイアスの初期値\n",
        "weights = [\n",
        "    [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]], # 入力層→隠れ層1\n",
        "    [[0.0, 0.0, 0.0]] # 隠れ層1→出力層\n",
        "]\n",
        "biases = [\n",
        "    [0.0, 0.0, 0.0],  # 隠れ層1\n",
        "    [0.0]  # 出力層\n",
        "]\n",
        "# 入力層／隠れ層1／出力層にある各ノードの（活性化関数の）出力値\n",
        "cached_outs = [[0.05, 0.1], [0.5, 0.5, 0.5], [0.0]]\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "w = weights[layer_i - SKIP_INPUT_LAYER][node_i]\n",
        "b = biases[layer_i - SKIP_INPUT_LAYER]\n",
        "x = cached_outs[layer_i - PREV_LAYER]  # 前の層の出力（out）＝今の層への入力（x）\n",
        "sum_der_w = sum_der(x, w, b, with_respect_to='w')\n",
        "sum_der_b = sum_der(x, w, b, with_respect_to='b')\n",
        "sum_der_x = sum_der(x, w, b, with_respect_to='x')"
      ],
      "metadata": {
        "id": "VpMIf3ggCCYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図12　「逆伝播していく誤差情報」「活性化関数を偏微分」「線形和を偏微分」まで実装完了](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/12.png)"
      ],
      "metadata": {
        "id": "rJl8f75mXfmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●（4）各重み／バイアス／各入力の勾配を計算"
      ],
      "metadata": {
        "id": "v1JXGgZuCCw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まずは共通の計算部分であるデルタ（`delta`変数）を計算します。"
      ],
      "metadata": {
        "id": "F3pN4JsGYJbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta = back_error[node_i] * active_der"
      ],
      "metadata": {
        "id": "6f4r7jVICGVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図13　デルタ（delta）のイメージ](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/13.png)"
      ],
      "metadata": {
        "id": "TVJNd_B2SfZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次にバイアスの勾配（`grad_b`変数）を計算します。"
      ],
      "metadata": {
        "id": "HbjHhA3oYb_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "layer_grads_b = []  # 層ごとの、バイアス勾配のリスト\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "# バイアスは1つだけ\n",
        "grad_b = delta * sum_der_b\n",
        "layer_grads_b.append(grad_b)"
      ],
      "metadata": {
        "id": "JCWkuz3zdPJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図9（再掲）　各重み／バイアス／入力に関して損失関数を偏微分する場合の連鎖律の形）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/09.png)"
      ],
      "metadata": {
        "id": "5r2nJ3wlissE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後に各重みの勾配（`grad_w`変数）と各入力の勾配（`grad_x`変数）を計算します。"
      ],
      "metadata": {
        "id": "6DK6QGyWic7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "layer_grads_w = []  # 層ごとの、重み勾配のリスト\n",
        "layer_grads_x = []  # 層ごとの、入力勾配のリスト\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "# 重みと入力は前の層のノードの数だけある\n",
        "node_grads_w = []\n",
        "for x_i, (each_dw, each_dx) in enumerate(zip(sum_der_w, sum_der_x)):\n",
        "    # 重みは個別に取得する\n",
        "    grad_w = delta * each_dw\n",
        "    node_grads_w.append(grad_w)\n",
        "\n",
        "    # 入力は各ノードから前のノードに接続する全ての入力を合計する\n",
        "    # （※重み視点と入力視点ではエッジの並び方が違うので注意）\n",
        "    grad_x = delta * each_dx\n",
        "    if node_i == 0:\n",
        "        # 最初に、入力の勾配を作成\n",
        "        layer_grads_x.append(grad_x)\n",
        "    else:\n",
        "        # その後は、その入力の勾配に合計していく\n",
        "        layer_grads_x[x_i] += grad_x\n",
        "layer_grads_w.append(node_grads_w)"
      ],
      "metadata": {
        "id": "R-jAS9PaiNT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●逆伝播の処理全体の実装"
      ],
      "metadata": {
        "id": "yJNU9W3qCGrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットには、層があり、その中に複数のノードが存在するという構造です。  従って、\n",
        "\n",
        "- **逆順に**各層を1つずつ処理する`for`ループと、  \n",
        "  - 層の中のノードを1つずつ処理する`for`ループの2段階構造が必要で、\n",
        "    - その中に「1つのノードにおける逆伝播の処理」\n",
        "\n",
        "を記述すればよいわけです。"
      ],
      "metadata": {
        "id": "iCp5GqVfSHKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(layers, weights, biases, y_true, cached_outs, cached_sums):\n",
        "    \"\"\"\n",
        "    逆伝播を行う関数。\n",
        "    - 引数：\n",
        "    (layers, weights, biases)： モデルを指定する。\n",
        "    y_true： 正解値（出力層のノードが複数ある場合もあるのでリスト値）。\n",
        "    cached_outs： 順伝播で記録した活性化関数の出力値。予測値を含む。\n",
        "    cached_sums： 順伝播で記録した線形和（Σ）値。\n",
        "    - 戻り値：\n",
        "    重みの勾配とバイアスの勾配を返す。\n",
        "    \"\"\"\n",
        "\n",
        "    # ネットワーク全体で勾配を保持するためのリスト\n",
        "    grads_w =[]  # 重みの勾配\n",
        "    grads_b = []  # バイアスの勾配\n",
        "    grads_x = []  # 入力の勾配\n",
        "\n",
        "    layer_count = len(layers)\n",
        "    layer_max_i = layer_count-1\n",
        "    SKIP_INPUT_LAYER = 1\n",
        "    PREV_LAYER = 1\n",
        "    rng = range(SKIP_INPUT_LAYER, layer_count)  # 入力層以外の層インデックス\n",
        "    for layer_i in reversed(rng):  # 各層を逆順に処理\n",
        "\n",
        "        is_output_layer = (layer_i == layer_max_i)\n",
        "        # 層ごとで勾配を保持するためのリスト\n",
        "        layer_grads_w = []\n",
        "        layer_grads_b = []\n",
        "        layer_grads_x = []\n",
        "\n",
        "        # （1）逆伝播していく誤差情報\n",
        "        if is_output_layer:\n",
        "            # 出力層（損失関数の偏微分係数）\n",
        "            back_error = []  # 逆伝播していく誤差情報\n",
        "            y_pred = cached_outs[layer_i]\n",
        "            for output, target in zip(y_pred, y_true):\n",
        "                loss_der = sseloss_der(output, target)  # 誤差情報\n",
        "                back_error.append(loss_der)\n",
        "        else:\n",
        "            # 隠れ層（次の層への入力の偏微分係数）\n",
        "            back_error = grads_x[-1]  # 最後に追加された入力の勾配\n",
        "\n",
        "        node_sums = cached_sums[layer_i - SKIP_INPUT_LAYER]\n",
        "        for node_i, node_sum in enumerate(node_sums):  # 各ノードを処理\n",
        "\n",
        "            # （2）活性化関数を偏微分\n",
        "            if is_output_layer:\n",
        "                # 出力層（恒等関数の微分）\n",
        "                active_der = identity_der(node_sum)\n",
        "            else:\n",
        "                # 隠れ層（シグモイド関数の微分）\n",
        "                active_der = sigmoid_der(node_sum)\n",
        "\n",
        "            # （3）線形和を重み／バイアス／入力で偏微分\n",
        "            w = weights[layer_i - SKIP_INPUT_LAYER][node_i]\n",
        "            b = biases[layer_i - SKIP_INPUT_LAYER]\n",
        "            x = cached_outs[layer_i - PREV_LAYER]  # 前の層の出力＝今の層への入力\n",
        "            sum_der_w = sum_der(x, w, b, with_respect_to='w')\n",
        "            sum_der_b = sum_der(x, w, b, with_respect_to='b')\n",
        "            sum_der_x = sum_der(x, w, b, with_respect_to='x')\n",
        "\n",
        "            # （4）各重み／バイアス／各入力の勾配を計算\n",
        "            delta = back_error[node_i] * active_der\n",
        "\n",
        "            # バイアスは1つだけ\n",
        "            grad_b = delta * sum_der_b\n",
        "            layer_grads_b.append(grad_b)\n",
        "\n",
        "            # 重みと入力は前の層のノードの数だけある\n",
        "            node_grads_w = []\n",
        "            for x_i, (each_dw, each_dx) in enumerate(zip(sum_der_w, sum_der_x)):\n",
        "                # 重みは個別に取得する\n",
        "                grad_w = delta * each_dw\n",
        "                node_grads_w.append(grad_w)\n",
        "\n",
        "                # 入力は各ノードから前のノードに接続する全ての入力を合計する\n",
        "                # （※重み視点と入力視点ではエッジの並び方が違うので注意）\n",
        "                grad_x = delta * each_dx\n",
        "                if node_i == 0:\n",
        "                    # 最初に、入力の勾配を作成\n",
        "                    layer_grads_x.append(grad_x)\n",
        "                else:\n",
        "                    # その後は、その入力の勾配に合計していく\n",
        "                    layer_grads_x[x_i] += grad_x\n",
        "            layer_grads_w.append(node_grads_w)\n",
        "\n",
        "        # 層ごとの勾配を、ネットワーク全体用のリストに格納\n",
        "        grads_w.append(layer_grads_w)\n",
        "        grads_b.append(layer_grads_b)\n",
        "        grads_x.append(layer_grads_x)\n",
        "\n",
        "    # 保持しておいた各勾配（※逆順で追加したので反転が必要）を戻り値で返す\n",
        "    grads_w.reverse()\n",
        "    grads_b.reverse()\n",
        "    return (grads_w, grads_b)  # grads_xは最適化で不要なので返していない"
      ],
      "metadata": {
        "id": "l0g89nnACHxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●逆伝播の実行例"
      ],
      "metadata": {
        "id": "IbhBtq6vHFkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [0.05, 0.1]\n",
        "layers = [2, 2, 2]\n",
        "weights = [\n",
        "    [[0.15, 0.2], [0.25, 0.3]],\n",
        "    [[0.4, 0.45], [0.5,0.55]]\n",
        "]\n",
        "biases = [[0.35, 0.35], [0.6, 0.6]]\n",
        "model = (layers, weights, biases)\n",
        "y_true = [0.01, 0.99]\n",
        "\n",
        "# （1）順伝播の実行例\n",
        "y_pred, cached_outs, cached_sums = forward_prop(*model, x, cache_mode=True)\n",
        "print(f'y_pred={y_pred}')\n",
        "print(f'cached_outs={cached_outs}')\n",
        "print(f'cached_sums={cached_sums}')\n",
        "# 出力例：\n",
        "# y_pred=[1.10590596705977, 1.2249214040964653]\n",
        "# cached_outs=[[0.05, 0.1], [0.5932699921071872, 0.596884378259767], [1.10590596705977, 1.2249214040964653]]\n",
        "# cached_sums=[[0.3775, 0.39249999999999996], [1.10590596705977, 1.2249214040964653]]\n",
        "\n",
        "# （2）逆伝播の実行例\n",
        "grads_w, grads_b = back_prop(*model, y_true, cached_outs, cached_sums)\n",
        "print(f'grads_w={grads_w}')\n",
        "print(f'grads_b={grads_b}')\n",
        "# 出力例：\n",
        "# grads_w=[[[0.006706025259285303, 0.013412050518570607], [0.007487461943833829, 0.014974923887667657]], [[0.6501681244277691, 0.6541291517796395], [0.13937181955411934, 0.1402209162240302]]]\n",
        "# grads_b=[[0.13412050518570606, 0.14974923887667657], [1.09590596705977, 0.23492140409646534]]"
      ],
      "metadata": {
        "id": "o5yo0KezBkfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# お疲れさまでした。第1回～第2回は以上です。\n",
        "第3回もこのノートブックをアップデートする予定です。"
      ],
      "metadata": {
        "id": "gY5L18RrOzHb"
      }
    }
  ]
}