{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_from_scratch_without_numpy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isshiki/neural-network-by-code/blob/main/01-forward-prop/nn_from_scratch_without_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2021-2022 Digital Advantage - Deep Insider."
      ],
      "metadata": {
        "id": "I8yy30L04inZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_QLRIOK4fVd"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "連載『ニューラルネットワーク入門』\n",
        "# 「コードで必ず分かるニューラルネットワーク（DNN）の逆伝播」\n"
      ],
      "metadata": {
        "id": "6Dza91D34tmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table valign=\"middle\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://atmarkit.itmedia.co.jp/ait/articles/2202/09/news027.html\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/manabu.svg\"/>Deep Insiderで記事を読む</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/isshiki/neural-network-by-code/blob/main/01-forward-prop/nn_from_scratch_without_numpy.ipynb\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/gcolab.svg\" />Google Colabで実行する</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://studiolab.sagemaker.aws/import/github/isshiki/neural-network-by-code/tree/main/01-forward-prop/nn_from_scratch_without_numpy.ipynb\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/astudiolab.svg\" />AWS  SageMaker Studio Labで実行する</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/isshiki/neural-network-by-code/blob/main/01-forward-prop/nn_from_scratch_without_numpy.ipynb\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/github.svg\" />GitHubでソースコードを見る</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "XyoG00sPzIZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "※上から順に実行してください。上のコードで実行したものを再利用しているところがあるため、すべて実行しないとエラーになるコードがあります。  \n",
        "　すべてのコードを一括実行したい場合は、Colabであればメニューバーから［ランタイム］－［すべてのセルを実行］をクリックしてください。"
      ],
      "metadata": {
        "id": "QWomeA__rv5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■本ノートブックの目的"
      ],
      "metadata": {
        "id": "_V_STJSgw46b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットワーク（以下、ニューラルネット）の仕組みを、数学理論からではなく**Pythonコードから学ぶ**ことを狙っています。「難しい高校以降の数学は苦手だけど、コードなら読めるぜ！」という方にピッタリです。"
      ],
      "metadata": {
        "id": "ZAlfbhv1w6mI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●本ノートブックの特徴"
      ],
      "metadata": {
        "id": "jQlmkwbNxUle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 線形代数（linear algebra、行列演算）を使いません。つまり、NumPyを使いません。\n",
        "- 基本的に掛け算や足し算などの中学までの数学のみで、ニューラルネットのロジックをコーディングしていきます。\n",
        "- ※微分の導関数は、そのままコードとして記載することで、微分の計算は取り上げません。\n",
        "\n",
        "![図1　線形代数を使わないことによるメリット](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/01-forward-prop/images/01.png)"
      ],
      "metadata": {
        "id": "8X8ZSGoFxciU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■ニューラルネットワークの図"
      ],
      "metadata": {
        "id": "rjtsPfbhwyOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "基本的なニューラルネット（この例では、入力層：2、隠れ層：3、出力層：1）の図を確認しておきます。\n",
        "\n",
        "![図2　ニューラルネットワークの図（左：横描き、右：縦描き）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/01-forward-prop/images/02.png)"
      ],
      "metadata": {
        "id": "0dKydpNZydl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■訓練（学習）処理全体の実装"
      ],
      "metadata": {
        "id": "sT1m3Bseyytq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "深層「学習」のメインは、ニューラルネットの「訓練」処理ですよね。ここから書いていきます。"
      ],
      "metadata": {
        "id": "32U5YXJyQU08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず仮で、空の関数を定義して、コードが実行できるようにしておく\n",
        "def forward_prop(cache_mode=False):\n",
        "    \" 順伝播を行う関数。\"\n",
        "    return None, None, None\n",
        "\n",
        "y_true = [1.0]  # 正解値\n",
        "def back_prop(y_true, cached_outs, cached_sums):\n",
        "    \" 逆伝播を行う関数。\"\n",
        "    return None, None\n",
        "\n",
        "LEARNING_RATE = 0.1 # 学習率（lr）\n",
        "def update_params(grads_w, grads_b, lr=0.1):\n",
        "    \" パラメーター（重みとバイアス）を更新する関数。\"\n",
        "    return None, None\n",
        "\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "# 訓練処理\n",
        "y_pred, cached_outs, cached_sums = forward_prop(cache_mode=True)  # （1）\n",
        "grads_w, grads_b = back_prop(y_true, cached_outs, cached_sums)  # （2）\n",
        "weights, biases = update_params(grads_w, grads_b, LEARNING_RATE)  # （3）\n",
        "\n",
        "print(f'予測値：{y_pred}')  # 予測値： None\n",
        "print(f'正解値：{y_true}')  # 正解値： [1.0]"
      ],
      "metadata": {
        "id": "2GJPJd3EjnLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットの訓練に必要なことは、以下の3つだけ。\n",
        "\n",
        "1. **順伝播：** `forward_prop()◎`数として実装\n",
        "2. **逆伝播：** `back_prop()`関数として実装。損失（予測と正解の誤差）の計算はここで行う\n",
        "3. **パラメーター（重みとバイアス）の更新：** `update_params()`関数として実装。これによりモデルが**最適化**される\n"
      ],
      "metadata": {
        "id": "mtBTnQCjQuhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図3　訓練（学習）処理を示したニューラルネットワーク図](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/01-forward-prop/images/03.png)"
      ],
      "metadata": {
        "id": "yv5BZKpHPXIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##■モデルの定義と、仮の訓練データ"
      ],
      "metadata": {
        "id": "uL1t6_R_5vX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "入力層のノードが2個、隠れ層のノードが3個、出力層のノードが1個のモデル（`model`変数）を定義しましょう。"
      ],
      "metadata": {
        "id": "tZ76R4MbRKmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ニューラルネットワークは3層構成\n",
        "layers = [\n",
        "    2,  # 入力層の入力（特徴量）の数\n",
        "    3,  # 隠れ層1のノード（ニューロン）の数\n",
        "    1]  # 出力層のノードの数\n",
        "\n",
        "# 重みとバイアスの初期値\n",
        "weights = [\n",
        "    [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]], # 入力層→隠れ層1\n",
        "    [[0.0, 0.0, 0.0]] # 隠れ層1→出力層\n",
        "]\n",
        "biases = [\n",
        "    [0.0, 0.0, 0.0],  # 隠れ層1\n",
        "    [0.0]  # 出力層\n",
        "]\n",
        "\n",
        "# モデルを定義\n",
        "model = (layers, weights, biases)\n",
        "\n",
        "# 仮の訓練データ（1件分）を準備\n",
        "x = [0.05, 0.1]  # x_1とx_2の2つの特徴量"
      ],
      "metadata": {
        "id": "7sW0f2uH50eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■ステップ1. 順伝播の実装"
      ],
      "metadata": {
        "id": "Xm1tlcIiQOcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●1つのノードにおける順伝播の処理"
      ],
      "metadata": {
        "id": "1nxXuQ5nHW58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットの最小単位である「1つのノード」における順伝播の処理をコーディングしましょう。"
      ],
      "metadata": {
        "id": "e5qxZXE2RVQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず仮で、空の関数を定義して、コードが実行できるようにしておく\n",
        "def summation(x,weights, bias):\n",
        "    \" 重み付き線形和の関数。\"\n",
        "    return 0.0\n",
        "\n",
        "def sigmoid(x):\n",
        "    \" シグモイド関数。\"\n",
        "    return 0.0\n",
        "\n",
        "def identity(x):\n",
        "    \" 恒等関数。\"\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "w = [0.0, 0.0]  # 重み（仮の値）\n",
        "b = 0.0  # バイアス（仮の値）\n",
        "\n",
        "next_x = x  # 訓練データをノードへの入力に使う\n",
        "\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "# 1つのノードの処理（1）： 重み付き線形和\n",
        "node_sum = summation(next_x, w, b)\n",
        "\n",
        "# 1つのノードの処理（2）： 活性化関数\n",
        "is_hidden_layer = True\n",
        "if is_hidden_layer:\n",
        "    # 隠れ層（シグモイド関数）\n",
        "    node_out = sigmoid(node_sum)\n",
        "else:\n",
        "    # 出力層（恒等関数）\n",
        "    node_out = identity(node_sum)"
      ],
      "metadata": {
        "id": "I4-XMugzbUGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1つのノードの順伝播処理に必要なことは、以下の2つの数学関数だけ。\n",
        "\n",
        "1. **重み付き線形和の関数：** `summation()`関数として実装\n",
        "2. **活性化関数：** ここでは`sigmoid()`関数や`identity()`関数として実装"
      ],
      "metadata": {
        "id": "tPBzLn3hW0zU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図4　1つのニューロンにおける順伝播の処理を示した図](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/01-forward-prop/images/04.png)"
      ],
      "metadata": {
        "id": "V7DKfcYyPf1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●重み付き線形和"
      ],
      "metadata": {
        "id": "kCQTEdpGcv7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "重み付き線形和（weighted linear summation、以下では「線形和」と表記）とは、とは、あるノードへの複数の入力（$x_1$、$x_2$など）に、それぞれの重み（$w_1$、$w_2$など）を掛けて足し合わせて、最後にバイアス（$b$）を足した値です（上の図の左）。"
      ],
      "metadata": {
        "id": "VX8bRXh2Riek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summation(x, weights, bias):\n",
        "    \"\"\"\n",
        "    重み付き線形和の関数。\n",
        "    ※1データ分、つまりxとweightsは「一次元リスト」という前提。\n",
        "    - 引数：\n",
        "    x： 入力データをリスト値（各要素はfloat値）で指定する。\n",
        "    weights： 重みをリスト値（各要素はfloat値）で指定する。\n",
        "    bias： バイアスをfloat値で指定する。\n",
        "    - 戻り値：\n",
        "    線形和の計算結果をfloat値で返す。\n",
        "    \"\"\"\n",
        "    linear_sum = 0.0\n",
        "    for x_i, w_i in zip(x, weights):\n",
        "        linear_sum += x_i * w_i  # iは「番号」（数学は基本的に1スタート）\n",
        "        # print(f'x_i({x_i})×w_i({w_i})＋', end='')\n",
        "    linear_sum += bias\n",
        "    # print(f'b({bias})', end='')\n",
        "    return linear_sum\n",
        "\n",
        "# 線形代数を使う場合のコード例：\n",
        "# linear_sum = np.dot(x, weights) + bias"
      ],
      "metadata": {
        "id": "DuXPPnolcHHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ついで、次回の逆伝播（の中で使う偏微分）で必要となる線形和（linear **sum**mation）の偏導関数（partial **der**ivative function）を実装しておきます。"
      ],
      "metadata": {
        "id": "VxKE5sNZSFq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_der(x, weights, bias, with_respect_to='w'):\n",
        "    \"\"\"\n",
        "    重み付き線形和の関数の偏導関数。\n",
        "    ※1データ分、つまりxとweightsは「一次元リスト」という前提。\n",
        "    - 引数：\n",
        "    x： 入力データをリスト値で指定する。\n",
        "    weights：  重みをリスト値で指定する。\n",
        "    bias: バイアスをfloat値で指定する。\n",
        "    with_respect_to: 何に関して偏微分するかを指定する。\n",
        "        'w'＝ 重み、'b'＝ バイアス、'x'＝ 入力。\n",
        "    - 戻り値：\n",
        "    with_respect_toが'w'や'x'の場合はリスト値で、'b'の場合はfloat値で\n",
        "        線形和の偏微分の計算結果（偏微分係数）を返す。\n",
        "    \"\"\"    \n",
        "    if with_respect_to == 'w':\n",
        "        return x  # 線形和uを各重みw_iで偏微分するとx_iになる（iはノード番号）\n",
        "    elif with_respect_to == 'b':\n",
        "        return 1.0  # 線形和uをバイアスbで偏微分すると1になる\n",
        "    elif with_respect_to == 'x':\n",
        "        return weights  # 線形和uを各入力x_iで偏微分するとw_iになる"
      ],
      "metadata": {
        "id": "Ob-1saVLdtre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●活性化関数：シグモイド関数"
      ],
      "metadata": {
        "id": "09TqjNnXidxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "隠れ層では、最も基礎的なシグモイド関数（Sigmoid function）を固定的に使うことにします。導関数も実装しておきます。"
      ],
      "metadata": {
        "id": "HeCjZU9STJOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    シグモイド関数。\n",
        "    - 引数：\n",
        "    x： 入力データをfloat値で指定する。\n",
        "    - 戻り値：\n",
        "    シグモイド関数の計算結果をfloat値で返す。\n",
        "    \"\"\"\n",
        "    return 1.0 / (1.0 + math.exp(-x))\n",
        "\n",
        "# 線形代数の場合はmathをnpに変える（事前にimport numpy as np）"
      ],
      "metadata": {
        "id": "RAXhKRDyif96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_der(x):\n",
        "    \"\"\"\n",
        "    シグモイド関数の（偏）導関数。\n",
        "    - 引数：\n",
        "    x： 入力データをfloat値で指定する。\n",
        "    - 戻り値：\n",
        "    シグモイド関数の（偏）微分の計算結果（微分係数）をfloat値で返す。\n",
        "    \"\"\"\n",
        "    output = sigmoid(x)\n",
        "    return output * (1.0 - output)"
      ],
      "metadata": {
        "id": "dPghF0t5iiMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●活性化関数：恒等関数"
      ],
      "metadata": {
        "id": "aNH6Mq72jsTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "出力層では、回帰問題をイメージして、そのままの値を出力する活性化関数である恒等関数（Identity function）を使用します。導関数も実装しておきます。"
      ],
      "metadata": {
        "id": "rPevusfVTaz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identity(x):\n",
        "    \"\"\"\n",
        "    恒等関数の関数。\n",
        "    - 引数：\n",
        "    x： 入力データをfloat値で指定する。\n",
        "    - 戻り値：\n",
        "    恒等関数の計算結果（そのまま）をfloat値で返す。\n",
        "    \"\"\"\n",
        "    return x"
      ],
      "metadata": {
        "id": "8oVoxKncjrzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_der(x):\n",
        "    \"\"\"\n",
        "    恒等関数の（偏）導関数。\n",
        "    - 引数：\n",
        "    x： 入力データをfloat値で指定する。\n",
        "    - 戻り値：\n",
        "    恒等関数の（偏）微分の計算結果（微分係数）をfloat値で返す。\n",
        "    \"\"\"\n",
        "    return 1.0"
      ],
      "metadata": {
        "id": "B_4avMTJjvog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●順伝播の処理全体の実装"
      ],
      "metadata": {
        "id": "j2nmNTn0OmPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットには、層があり、その中に複数のノードが存在するという構造です。  従って、\n",
        "\n",
        "- 各層を1つずつ処理する`for`ループと、  \n",
        "  - 層の中のノードを1つずつ処理する`for`ループの2段階構造が必要で、\n",
        "    - その中に「1つのノードにおける順伝播の処理」\n",
        "\n",
        "を記述すればよいわけです。"
      ],
      "metadata": {
        "id": "_jmw6snqTsJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(layers, weights, biases, x, cache_mode=False):\n",
        "    \"\"\"\n",
        "    順伝播を行う関数。\n",
        "    - 引数：\n",
        "    (layers, weights, biases)： モデルを指定する。\n",
        "    x： 入力データを指定する。\n",
        "    cache_mode： 予測時はFalse、訓練時はTrueにする。これにより戻り値が変わる。\n",
        "    - 戻り値：\n",
        "    cache_modeがFalse時は予測値のみを返す。True時は、予測値だけでなく、\n",
        "        キャッシュに記録済みの線形和（Σ）値と、活性化関数の出力値も返す。\n",
        "    \"\"\"\n",
        "\n",
        "    cached_sums = []  # 記録した全ノードの線形和（Σ）の値\n",
        "    cached_outs = []  # 記録した全ノードの活性化関数の出力値\n",
        "\n",
        "    # まずは、入力層を順伝播する\n",
        "    # print(f'■第1層（入力層）-全て（{len(x)}個）の特徴量：')\n",
        "    # print(f'　●入力データ： ', end='')\n",
        "    cached_outs.append(x)  # 何も処理せずに出力値を記録\n",
        "    # print(f'何もしない＝out({x})')\n",
        "    next_x = x  # 現在の層の出力（x）＝次の層への入力（next_x）\n",
        "\n",
        "    # 次に、隠れ層や出力層を順伝播する\n",
        "    SKIP_INPUT_LAYER = 1\n",
        "    for layer_i, layer in enumerate(layers):  # 各層を処理\n",
        "        if layer_i == 0:\n",
        "            continue  # 入力層は上で処理済み\n",
        "\n",
        "        # 各層のノードごとに処理を行う\n",
        "        sums = []\n",
        "        outs = []\n",
        "        for node_i in range(layer):  # 層の中の各ノードを処理\n",
        "            # print(f'■第{layer_i+1}層-第{node_i+1}ノード：')\n",
        "\n",
        "            # ノードごとの重みとバイアスを取得\n",
        "            w = weights[layer_i - SKIP_INPUT_LAYER][node_i]\n",
        "            b = biases[layer_i - SKIP_INPUT_LAYER][node_i]\n",
        "\n",
        "            # 1つのノードの処理（1）： 重み付き線形和\n",
        "            # print(f'　●重み付き線形和： ', end='')\n",
        "            node_sum = summation(next_x, w, b)\n",
        "            # print(f'＝sum({node_sum})')\n",
        "\n",
        "            # 1つのノードの処理（2）： 活性化関数\n",
        "            if layer_i < len(layers)-1:  # -1は出力層以外の意味\n",
        "                # 隠れ層（シグモイド関数）\n",
        "                # print(f'　●活性化関数（隠れ層はシグモイド関数）： ', end='')\n",
        "                node_out = sigmoid(node_sum)\n",
        "                # print(f'sigmoid({node_sum})＝out({node_out})')\n",
        "            else:\n",
        "                # 出力層（恒等関数）\n",
        "                # print(f'　●活性化関数（出力層は恒等関数）： ', end='')\n",
        "                node_out = identity(node_sum)\n",
        "                # print(f'identity({node_sum})＝out({node_out})')\n",
        "\n",
        "            # 各ノードの線形和と（活性化関数の）出力をリストにまとめていく\n",
        "            sums.append(node_sum)\n",
        "            outs.append(node_out)\n",
        "\n",
        "        # 各層内の全ノードの線形和と出力を記録\n",
        "        cached_sums.append(sums)\n",
        "        cached_outs.append(outs)\n",
        "        next_x = outs  # 現在の層の出力（outs）＝次の層への入力（next_x）\n",
        "\n",
        "    if cache_mode:\n",
        "        return (cached_outs[-1], cached_outs, cached_sums)\n",
        "\n",
        "    return cached_outs[-1]\n",
        "\n",
        "\n",
        "# 訓練時の（1）順伝播の実行例\n",
        "y_pred, cached_outs, cached_sums = forward_prop(*model, x, cache_mode=True)\n",
        "# ※先ほど作成したモデルと訓練データを引数で受け取るよう改変した\n",
        "\n",
        "print(f'cached_outs={cached_outs}')\n",
        "print(f'cached_sums={cached_sums}')\n",
        "# 出力例：\n",
        "# cached_outs=[[0.05, 0.1], [0.5, 0.5, 0.5], [0.0]]  # 入力層／隠れ層1／出力層\n",
        "# cached_sums=[[0.0, 0.0, 0.0], [0.0]]  # 隠れ層1／出力層（※入力層はない）"
      ],
      "metadata": {
        "id": "OlfyIL3VkGXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "数値が**0.0**ばかりなので、別の計算パターンのコードも入れておきました。"
      ],
      "metadata": {
        "id": "sEwU0TN8U7ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 記事にはないが、別の計算パターンでもチェックしてみよう\n",
        "x3 = [0.05, 0.1]\n",
        "layers3 = [2, 2, 2]\n",
        "weights3 = [\n",
        "    [[0.15, 0.2], [0.25, 0.3]],\n",
        "    [[0.4, 0.45], [0.5,0.55]]\n",
        "]\n",
        "biases3 = [[0.35, 0.35], [0.6, 0.6]]\n",
        "model3 = (layers3, weights3, biases3)\n",
        "\n",
        "y_pred3, cached_outs3, cached_sums3 = forward_prop(*model3, x3, cache_mode=True)\n",
        "print(f'y_pred={y_pred3}')\n",
        "print(f'cached_outs={cached_outs3}')\n",
        "print(f'cached_sums={cached_sums3}')\n",
        "# y_pred=[1.10590596705977, 1.2249214040964653]\n",
        "# cached_outs=[[0.05, 0.1], [0.5932699921071872, 0.596884378259767], [1.10590596705977, 1.2249214040964653]]\n",
        "# cached_sums=[[0.3775, 0.39249999999999996], [1.10590596705977, 1.2249214040964653]]"
      ],
      "metadata": {
        "id": "7f2urrsRWazm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここまでのコード中に仕込んでいる`print()`関数（※全てコメントアウトしています）のコメントを解除すると、以下のように途中の計算内容が順番にテキスト出力されます。計算内容の検証用の機能です。"
      ],
      "metadata": {
        "id": "9wOPPW0YVcH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![別の計算パターンの出力例](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/01-forward-prop/images/notebook-01.png)"
      ],
      "metadata": {
        "id": "aRfWOdtGPm41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●順伝播による予測の実行例"
      ],
      "metadata": {
        "id": "8Y2y79vNGoUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "非常にシンプルで原始的な実装ですが、このように任意の層数とノード数の全結合のDNN（Deep Neural Network）のアーキテクチャーを定義して、DNNモデルによる予測が行えます。"
      ],
      "metadata": {
        "id": "ym7sZcmzWW7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 異なるDNNアーキテクチャーを定義してみる\n",
        "layers2 = [\n",
        "    2,  # 入力層の入力（特徴量）の数\n",
        "    3,  # 隠れ層1のノード（ニューロン）の数\n",
        "    2,  # 隠れ層2のノード（ニューロン）の数\n",
        "    1]  # 出力層のノードの数\n",
        "\n",
        "# 重みとバイアスの初期値\n",
        "weights2 = [\n",
        "    [[-0.2, 0.4], [-0.4, -0.5], [-0.4, -0.5]], # 入力層→隠れ層1\n",
        "    [[-0.2, 0.4, 0.9], [-0.4, -0.5, -0.2]], # 隠れ層1→隠れ層2\n",
        "    [[-0.5, 1.0]] # 隠れ層2→出力層\n",
        "]\n",
        "biases2 = [\n",
        "    [0.1, -0.1, 0.1],  # 隠れ層1\n",
        "    [0.2, -0.2],  # 隠れ層2\n",
        "    [0.3]  # 出力層\n",
        "]\n",
        "\n",
        "# モデルを定義\n",
        "model2 = (layers2, weights2, biases2)\n",
        "\n",
        "# 仮の訓練データ（1件分）を準備\n",
        "x2 = [2.3, 1.5]  # x_1とx_2の2つの特徴量\n",
        "\n",
        "# 予測時の（1）順伝播の実行例\n",
        "y_pred = forward_prop(*model2, x2)\n",
        "print(y_pred)  # 予測値\n",
        "# 出力例：\n",
        "# [0.3828840428423274]"
      ],
      "metadata": {
        "id": "U0sgGfN2kOa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●今後のステップの準備：関数への仮引数の追加"
      ],
      "metadata": {
        "id": "EbO0sBxLVxjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(layers, weights, biases, y_true, cached_outs, cached_sums):\n",
        "    \" 逆伝播を行う関数。\"\n",
        "    return None, None\n",
        "\n",
        "def update_params(layers, weights, biases, grads_w, grads_b, lr=0.1):\n",
        "    \" パラメーター（重みとバイアス）を更新する関数。\"\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "ap_S235RV36g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■ステップ2. 逆伝播の実装"
      ],
      "metadata": {
        "id": "rgju9tQge_eS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●逆伝播の目的と全体像"
      ],
      "metadata": {
        "id": "yERpCp1COZhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "逆伝播の目的は、誤差（厳密には予測値に関する損失関数の偏微分係数）などの数値（本ノートブックでは**誤差情報**と呼ぶ）をニューラルネットに逆方向で流すこと（＝逆伝播）によって「**重みとバイアスの勾配を計算すること**」です（下の図）。"
      ],
      "metadata": {
        "id": "aKNJrZmiOsb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図5　「逆伝播の流れ」のイメージ（左：ネットワーク図、右：対応する処理／数学計算）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/05.png)"
      ],
      "metadata": {
        "id": "nAs9qpq4OiOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "順伝播（`forward_prop()`関数）では、計算途中に出た計算結果である「予測値（`y_pred`）」や「各ノードでの活性化関数の出力値（`cached_outs`）」と「線形和の値（`cached_sums`）」を返すだけでした。\n",
        "\n",
        "逆伝播（`back_prop()関数`）では、計算途中に出た計算結果である「各ノードへの入力の勾配（＝逆伝播していく誤差情報）」だけでなく、「各重みの勾配（`grads_w`）」「各バイアスの勾配（`grads_b`）」の計算も必要です（下の図）。"
      ],
      "metadata": {
        "id": "5muXMKUnPHhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図6　逆伝播では各ノードへの入力／各重み／各バイアスの勾配を計算する](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/06.png)"
      ],
      "metadata": {
        "id": "x_CvxtXoO8Q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "逆伝播では、$x_1$／$w_1$／$x_2$／$w_2$／……／$x_n$／$w_n$／$b$という大量の変数に関して、損失関数の偏微分係数（＝勾配）を計算する必要があります。"
      ],
      "metadata": {
        "id": "ywEzTQaDQx5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●損失関数：二乗和誤差"
      ],
      "metadata": {
        "id": "CU4U3HUOgKNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "損失関数として、最も基礎的な二乗和誤差（SSE：Sum of Squared Error）を使うことにします。導関数も実装しておきます。"
      ],
      "metadata": {
        "id": "Mh5SWGHqG7db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sseloss(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    二乗和誤差（Sum of Squared Error）の関数。\n",
        "    - 引数：\n",
        "    y_pred： モデルの最終出力値（output）＝予測値（prediction）。\n",
        "    y_true： 目的となる値（target）＝正解値（truth、label）。\n",
        "    - 戻り値：\n",
        "    二乗和誤差の計算結果をfloat値で返す。\n",
        "    \"\"\"\n",
        "    return 0.5 * (y_pred - y_true) ** 2"
      ],
      "metadata": {
        "id": "EQ7Uo6wpgKqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sseloss_der(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    二乗和誤差（Sum of Squared Error）の偏導関数。予測値（y_pred）に関して二乗和誤差関数（sseloss()）を偏微分する。\n",
        "    - 引数：\n",
        "    y_pred： モデルの最終出力値（output）＝予測値（predicted value）。\n",
        "    y_true： 目的となる値（target）＝正解値（true/actual value、label）。\n",
        "    - 戻り値：\n",
        "    二乗和誤差の偏微分の計算結果（偏微分係数）をfloat値で返す。\n",
        "    \"\"\"\n",
        "    return y_pred - y_true"
      ],
      "metadata": {
        "id": "-P4gEPUXG-p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "偏導関数の式`y_pred - y_true`は、予測値と正解値の「誤差（Error、ズレ）」となっています。\n",
        "\n",
        "**誤差逆伝播法**（error backpropagation）とは、この「誤差」の数値（厳密には、予測値に関しての損失関数の偏微分係数）が誤差情報としてニューラルネットを「逆」向きに「伝播」していく過程で、本来の目的である各重みと各バイアスの勾配を求める方法です。\n"
      ],
      "metadata": {
        "id": "b0AihXRHYECg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図7　各ノードでの逆伝播の処理はワンパターン](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/07.png)"
      ],
      "metadata": {
        "id": "36DaQKcEX1xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●1つのノードにおける逆伝播の処理"
      ],
      "metadata": {
        "id": "yW0fBcdUX5qY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`損失関数( 活性化関数( 線形和関数( 入力、重み、バイアス ) ) )`という入れ子の関数は数学で**合成関数**と呼ばれます。\n",
        "\n",
        "合成関数を微分するときの公式が**連鎖律**です。連鎖律を使うと、まるでマジックのように各関数の偏微分係数の掛け算だけの式に変化します（下の図）。"
      ],
      "metadata": {
        "id": "fR4sjl1AZILm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図8　連鎖律を使うと各関数の偏微分の掛け算になる（各重みに関して損失関数を偏微分する例）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/08.png)"
      ],
      "metadata": {
        "id": "1LmiDrcAXmvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上の図は各重みに関して損失関数を偏微分する例ですが、各バイアスや各入力に関して損失関数を偏微分する際も連鎖律の形はほぼ同じです（下の図）。ただし入力については、前の層のノードごとに、今の層からの全てのエッジから来る各誤差情報（偏微分係数）を合計する必要があるので注意してください。"
      ],
      "metadata": {
        "id": "R46Y9A8VDatf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図9　各重み／バイアス／入力に関して損失関数を偏微分する場合の連鎖律の形](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/09.png)"
      ],
      "metadata": {
        "id": "B7ScKvwnCU3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "各層における各ノードの計算は、\n",
        "\n",
        "　　「逆伝播していく誤差情報」×「活性化関数の偏微分」×「線形和関数の偏微分」\n",
        "\n",
        "という掛け算に共通化できます（下の図）。"
      ],
      "metadata": {
        "id": "WfZyaK7PDJTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図10　各層の各ノードでの計算パターンは共通化できる（出力層や隠れ層で入力の勾配を計算する例）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/10.png)"
      ],
      "metadata": {
        "id": "RDyy6F8iCa0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "出力層から隠れ層まで全て、以下の4工程のワンパターンで実装できます（下の図）。\n",
        "\n",
        "1. 逆伝播していく誤差情報\n",
        "2. 活性化関数を偏微分\n",
        "3. 線形和を重み／バイアス／入力で偏微分\n",
        "4. 各重み／バイアス／各入力の勾配を計算"
      ],
      "metadata": {
        "id": "GN0XAc9qCqKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図11　「逆伝播の流れ」の実装内容](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/11.png)"
      ],
      "metadata": {
        "id": "CkZkdGGrCgVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●（1）逆伝播していく誤差情報"
      ],
      "metadata": {
        "id": "MIUf0sjEBtK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "layer_i = 2  # 2：出力層、1：隠れ層1、0：入力層\n",
        "layer_max_i = 2  # 最後の層（＝出力層）のインデックス\n",
        "is_output_layer = (layer_i == layer_max_i)  # 出力層か（True）、隠れ層か（False）\n",
        "\n",
        "# 入力層／隠れ層1／出力層にある各ノードの（活性化関数の）出力値\n",
        "cached_outs = [[0.05, 0.1], [0.5, 0.5, 0.5], [0.0]]\n",
        "y_true = [1.0]  # 正解値\n",
        "grads_x = []  # 入力の勾配\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "if is_output_layer:\n",
        "    # 出力層（損失関数の偏微分係数）\n",
        "    back_error = []  # 逆伝播していく誤差情報\n",
        "    y_pred = cached_outs[layer_i]\n",
        "    for output, target in zip(y_pred, y_true):\n",
        "        loss_der = sseloss_der(output, target)  # 誤差情報\n",
        "        back_error.append(loss_der)\n",
        "else:\n",
        "    # 隠れ層（次の層への入力の偏微分係数）\n",
        "    back_error = grads_x[-1]  # 最後に追加された入力の勾配"
      ],
      "metadata": {
        "id": "otsQU8OEBpe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "※（1）は層ごとにまとめての処理です。以下からの（2）～（4）はノードごとの処理になります。"
      ],
      "metadata": {
        "id": "nFV7n9JaZmo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●（2）活性化関数を偏微分"
      ],
      "metadata": {
        "id": "ADqZzyY4B3Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "SKIP_INPUT_LAYER = 1  # 入力層を飛ばす\n",
        "cached_sums = [[0.0, 0.0, 0.0], [0.0]]  # 隠れ層1／出力層（※入力層はない）\n",
        "node_sum = cached_sums[layer_max_i - SKIP_INPUT_LAYER]  # 出力層\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "if is_output_layer:\n",
        "    # 出力層（恒等関数の微分）\n",
        "    active_der = identity_der(node_sum)\n",
        "else:\n",
        "    # 隠れ層（シグモイド関数の微分）\n",
        "    active_der = sigmoid_der(node_sum)"
      ],
      "metadata": {
        "id": "z4iP49xCB4bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●（3）線形和を重み／バイアス／入力で偏微分"
      ],
      "metadata": {
        "id": "C412ZRuXCAr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "PREV_LAYER = 1  # 前の層を指定するため\n",
        "node_i = 0  # ノード番号\n",
        "\n",
        "# 重みとバイアスの初期値\n",
        "weights = [\n",
        "    [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]], # 入力層→隠れ層1\n",
        "    [[0.0, 0.0, 0.0]] # 隠れ層1→出力層\n",
        "]\n",
        "biases = [\n",
        "    [0.0, 0.0, 0.0],  # 隠れ層1\n",
        "    [0.0]  # 出力層\n",
        "]\n",
        "# 入力層／隠れ層1／出力層にある各ノードの（活性化関数の）出力値\n",
        "cached_outs = [[0.05, 0.1], [0.5, 0.5, 0.5], [0.0]]\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "w = weights[layer_i - SKIP_INPUT_LAYER][node_i]\n",
        "b = biases[layer_i - SKIP_INPUT_LAYER]\n",
        "x = cached_outs[layer_i - PREV_LAYER]  # 前の層の出力（out）＝今の層への入力（x）\n",
        "sum_der_w = sum_der(x, w, b, with_respect_to='w')\n",
        "sum_der_b = sum_der(x, w, b, with_respect_to='b')\n",
        "sum_der_x = sum_der(x, w, b, with_respect_to='x')"
      ],
      "metadata": {
        "id": "VpMIf3ggCCYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図12　「逆伝播していく誤差情報」「活性化関数を偏微分」「線形和を偏微分」まで実装完了](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/12.png)"
      ],
      "metadata": {
        "id": "rJl8f75mXfmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●（4）各重み／バイアス／各入力の勾配を計算"
      ],
      "metadata": {
        "id": "v1JXGgZuCCw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まずは共通の計算部分であるデルタ（`delta`変数）を計算します。"
      ],
      "metadata": {
        "id": "F3pN4JsGYJbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta = back_error[node_i] * active_der"
      ],
      "metadata": {
        "id": "6f4r7jVICGVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図13　デルタ（delta）のイメージ](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/13.png)"
      ],
      "metadata": {
        "id": "TVJNd_B2SfZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次にバイアスの勾配（`grad_b`変数）を計算します。"
      ],
      "metadata": {
        "id": "HbjHhA3oYb_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "layer_grads_b = []  # 層ごとの、バイアス勾配のリスト\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "# バイアスは1つだけ\n",
        "grad_b = delta * sum_der_b\n",
        "layer_grads_b.append(grad_b)"
      ],
      "metadata": {
        "id": "JCWkuz3zdPJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図9（再掲）　各重み／バイアス／入力に関して損失関数を偏微分する場合の連鎖律の形](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/09.png)"
      ],
      "metadata": {
        "id": "5r2nJ3wlissE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後に各重みの勾配（`grad_w`変数）と各入力の勾配（`grad_x`変数）を計算します。"
      ],
      "metadata": {
        "id": "6DK6QGyWic7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "layer_grads_w = []  # 層ごとの、重み勾配のリスト\n",
        "layer_grads_x = []  # 層ごとの、入力勾配のリスト\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "# 重みと入力は前の層のノードの数だけある\n",
        "node_grads_w = []\n",
        "for x_i, (each_dw, each_dx) in enumerate(zip(sum_der_w, sum_der_x)):\n",
        "    # 重みは個別に取得する\n",
        "    grad_w = delta * each_dw\n",
        "    node_grads_w.append(grad_w)\n",
        "\n",
        "    # 入力は各ノードから前のノードに接続する全ての入力を合計する\n",
        "    # （※重み視点と入力視点ではエッジの並び方が違うので注意）\n",
        "    grad_x = delta * each_dx\n",
        "    if node_i == 0:\n",
        "        # 最初に、入力の勾配を作成\n",
        "        layer_grads_x.append(grad_x)\n",
        "    else:\n",
        "        # その後は、その入力の勾配に合計していく\n",
        "        layer_grads_x[x_i] += grad_x\n",
        "layer_grads_w.append(node_grads_w)"
      ],
      "metadata": {
        "id": "R-jAS9PaiNT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●逆伝播の処理全体の実装"
      ],
      "metadata": {
        "id": "yJNU9W3qCGrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットには、層があり、その中に複数のノードが存在するという構造です。  従って、\n",
        "\n",
        "- **逆順に**各層を1つずつ処理する`for`ループと、  \n",
        "  - 層の中のノードを1つずつ処理する`for`ループの2段階構造が必要で、\n",
        "    - その中に「1つのノードにおける逆伝播の処理」\n",
        "\n",
        "を記述すればよいわけです。"
      ],
      "metadata": {
        "id": "iCp5GqVfSHKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(layers, weights, biases, y_true, cached_outs, cached_sums):\n",
        "    \"\"\"\n",
        "    逆伝播を行う関数。\n",
        "    - 引数：\n",
        "    (layers, weights, biases)： モデルを指定する。\n",
        "    y_true： 正解値（出力層のノードが複数ある場合もあるのでリスト値）。\n",
        "    cached_outs： 順伝播で記録した活性化関数の出力値。予測値を含む。\n",
        "    cached_sums： 順伝播で記録した線形和（Σ）値。\n",
        "    - 戻り値：\n",
        "    重みの勾配とバイアスの勾配を返す。\n",
        "    \"\"\"\n",
        "\n",
        "    # ネットワーク全体で勾配を保持するためのリスト\n",
        "    grads_w =[]  # 重みの勾配\n",
        "    grads_b = []  # バイアスの勾配\n",
        "    grads_x = []  # 入力の勾配\n",
        "\n",
        "    layer_count = len(layers)\n",
        "    layer_max_i = layer_count-1\n",
        "    SKIP_INPUT_LAYER = 1\n",
        "    PREV_LAYER = 1\n",
        "    rng = range(SKIP_INPUT_LAYER, layer_count)  # 入力層以外の層インデックス\n",
        "    for layer_i in reversed(rng):  # 各層を逆順に処理\n",
        "\n",
        "        is_output_layer = (layer_i == layer_max_i)\n",
        "        # 層ごとで勾配を保持するためのリスト\n",
        "        layer_grads_w = []\n",
        "        layer_grads_b = []\n",
        "        layer_grads_x = []\n",
        "\n",
        "        # （1）逆伝播していく誤差情報\n",
        "        if is_output_layer:\n",
        "            # 出力層（損失関数の偏微分係数）\n",
        "            back_error = []  # 逆伝播していく誤差情報\n",
        "            y_pred = cached_outs[layer_i]\n",
        "            for output, target in zip(y_pred, y_true):\n",
        "                loss_der = sseloss_der(output, target)  # 誤差情報\n",
        "                back_error.append(loss_der)\n",
        "        else:\n",
        "            # 隠れ層（次の層への入力の偏微分係数）\n",
        "            back_error = grads_x[-1]  # 最後に追加された入力の勾配\n",
        "\n",
        "        node_sums = cached_sums[layer_i - SKIP_INPUT_LAYER]\n",
        "        for node_i, node_sum in enumerate(node_sums):  # 各ノードを処理\n",
        "\n",
        "            # （2）活性化関数を偏微分\n",
        "            if is_output_layer:\n",
        "                # 出力層（恒等関数の微分）\n",
        "                active_der = identity_der(node_sum)\n",
        "            else:\n",
        "                # 隠れ層（シグモイド関数の微分）\n",
        "                active_der = sigmoid_der(node_sum)\n",
        "\n",
        "            # （3）線形和を重み／バイアス／入力で偏微分\n",
        "            w = weights[layer_i - SKIP_INPUT_LAYER][node_i]\n",
        "            b = biases[layer_i - SKIP_INPUT_LAYER]\n",
        "            x = cached_outs[layer_i - PREV_LAYER]  # 前の層の出力＝今の層への入力\n",
        "            sum_der_w = sum_der(x, w, b, with_respect_to='w')\n",
        "            sum_der_b = sum_der(x, w, b, with_respect_to='b')\n",
        "            sum_der_x = sum_der(x, w, b, with_respect_to='x')\n",
        "\n",
        "            # （4）各重み／バイアス／各入力の勾配を計算\n",
        "            delta = back_error[node_i] * active_der\n",
        "\n",
        "            # バイアスは1つだけ\n",
        "            grad_b = delta * sum_der_b\n",
        "            layer_grads_b.append(grad_b)\n",
        "\n",
        "            # 重みと入力は前の層のノードの数だけある\n",
        "            node_grads_w = []\n",
        "            for x_i, (each_dw, each_dx) in enumerate(zip(sum_der_w, sum_der_x)):\n",
        "                # 重みは個別に取得する\n",
        "                grad_w = delta * each_dw\n",
        "                node_grads_w.append(grad_w)\n",
        "\n",
        "                # 入力は各ノードから前のノードに接続する全ての入力を合計する\n",
        "                # （※重み視点と入力視点ではエッジの並び方が違うので注意）\n",
        "                grad_x = delta * each_dx\n",
        "                if node_i == 0:\n",
        "                    # 最初に、入力の勾配を作成\n",
        "                    layer_grads_x.append(grad_x)\n",
        "                else:\n",
        "                    # その後は、その入力の勾配に合計していく\n",
        "                    layer_grads_x[x_i] += grad_x\n",
        "            layer_grads_w.append(node_grads_w)\n",
        "\n",
        "        # 層ごとの勾配を、ネットワーク全体用のリストに格納\n",
        "        grads_w.append(layer_grads_w)\n",
        "        grads_b.append(layer_grads_b)\n",
        "        grads_x.append(layer_grads_x)\n",
        "\n",
        "    # 保持しておいた各勾配（※逆順で追加したので反転が必要）を戻り値で返す\n",
        "    grads_w.reverse()\n",
        "    grads_b.reverse()\n",
        "    return (grads_w, grads_b)  # grads_xは最適化で不要なので返していない"
      ],
      "metadata": {
        "id": "l0g89nnACHxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●逆伝播の実行例"
      ],
      "metadata": {
        "id": "IbhBtq6vHFkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のようなコードを書けば、順伝播から逆伝播までを続けて実行できます。"
      ],
      "metadata": {
        "id": "xBuUkZYrBrJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [0.05, 0.1]\n",
        "layers = [2, 2, 2]\n",
        "weights = [\n",
        "    [[0.15, 0.2], [0.25, 0.3]],\n",
        "    [[0.4, 0.45], [0.5,0.55]]\n",
        "]\n",
        "biases = [[0.35, 0.35], [0.6, 0.6]]\n",
        "model = (layers, weights, biases)\n",
        "y_true = [0.01, 0.99]\n",
        "\n",
        "# （1）順伝播の実行例\n",
        "y_pred, cached_outs, cached_sums = forward_prop(*model, x, cache_mode=True)\n",
        "print(f'y_pred={y_pred}')\n",
        "print(f'cached_outs={cached_outs}')\n",
        "print(f'cached_sums={cached_sums}')\n",
        "# 出力例：\n",
        "# y_pred=[1.10590596705977, 1.2249214040964653]\n",
        "# cached_outs=[[0.05, 0.1], [0.5932699921071872, 0.596884378259767], [1.10590596705977, 1.2249214040964653]]\n",
        "# cached_sums=[[0.3775, 0.39249999999999996], [1.10590596705977, 1.2249214040964653]]\n",
        "\n",
        "# （2）逆伝播の実行例\n",
        "grads_w, grads_b = back_prop(*model, y_true, cached_outs, cached_sums)\n",
        "print(f'grads_w={grads_w}')\n",
        "print(f'grads_b={grads_b}')\n",
        "# 出力例：\n",
        "# grads_w=[[[0.006706025259285303, 0.013412050518570607], [0.007487461943833829, 0.014974923887667657]], [[0.6501681244277691, 0.6541291517796395], [0.13937181955411934, 0.1402209162240302]]]\n",
        "# grads_b=[[0.13412050518570606, 0.14974923887667657], [1.09590596705977, 0.23492140409646534]]"
      ],
      "metadata": {
        "id": "o5yo0KezBkfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■ステップ3. パラメーター（重みとバイアス）更新の実装"
      ],
      "metadata": {
        "id": "4a4Bw6fGuLBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "パラメーター（各重みと各バイアス）を更新する目的は、「**ニューラルネットのモデルを最適化すること**」です。下の図は「最適化の**参考イメージ**」です。\n",
        "\n",
        "※なお、本稿で説明するのは最も基礎的な**勾配降下法**（**Gradient Descent**）です。後述するSGD（確率的勾配降下法）もその一種で、他にはRMSPropやAdamなどより応用的な手法があります。SGD以外の場合は、重みパラメーターの更新方法も少し変わってきます。"
      ],
      "metadata": {
        "id": "DAERKD0XKjwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図14　最適化の参考イメージ](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/03-optimizer/images/14.png)"
      ],
      "metadata": {
        "id": "dvBQaEMgJtdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●1つのパラメーターの更新"
      ],
      "metadata": {
        "id": "SSNQtRXxAX9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1つの重み／バイアスのパラメーター更新をPythonコードで書くと以下のようになります。"
      ],
      "metadata": {
        "id": "AqGWeJuFAr0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "w_ij = 0.0  # 各重み\n",
        "b_j = 0.0  # バイアス\n",
        "grad_w_ij = 0.2  # 各重みの勾配\n",
        "grad_b_j = 0.2  # バイアスの勾配\n",
        "LEARNING_RATE = 0.1  # 学習率（lr）\n",
        "lr = LEARNING_RATE\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "w_ij = w_ij - lr * grad_w_ij  # 重みパラメーターの更新\n",
        "\n",
        "b_j = b_j - lr * grad_b_j  # バイアスパラメーターの更新"
      ],
      "metadata": {
        "id": "aEuYVALGFz5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![重みパラメーター更新の計算式](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/03-optimizer/images/15.png)"
      ],
      "metadata": {
        "id": "CzjUVAkPuvgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●パラメーター更新の処理全体の実装"
      ],
      "metadata": {
        "id": "TIs2wLVD6Ufe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットは、層があり、その中に複数のノードが存在するという構造ですので、\n",
        "\n",
        "- 各層を1つずつ処理するforループと\n",
        "  - 層の中のノードを1つずつ処理するforループの2段階構造が必要で\n",
        "    - その中に「1つのパラメーターの更新」\n",
        "\n",
        "を記述すればよいわけです。"
      ],
      "metadata": {
        "id": "aqIVwOxHBAAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_params(layers, weights, biases, grads_w, grads_b, lr=0.1):\n",
        "    \"\"\"\n",
        "    パラメーター（重みとバイアス）を更新する関数\n",
        "    - 引数：\n",
        "    (layers, weights, biases)： モデルを指定する。\n",
        "    grads_w： 重みの勾配。\n",
        "    grads_b： バイアスの勾配。\n",
        "    lr： 学習率（learning rate）。最適化を進める量を調整する。\n",
        "    - 戻り値：\n",
        "    新しい重みとバイアスを返す。\n",
        "    \"\"\"\n",
        "\n",
        "    # ネットワーク全体で勾配を保持するためのリスト\n",
        "    new_weights = [] # 重み\n",
        "    new_biases = [] # バイアス\n",
        "\n",
        "    SKIP_INPUT_LAYER = 1\n",
        "    for layer_i, layer in enumerate(layers):  # 各層を処理\n",
        "        if layer_i == 0:\n",
        "            continue  # 入力層はスキップ\n",
        "\n",
        "        # 層ごとで勾配を保持するためのリスト\n",
        "        layer_w = []\n",
        "        layer_b = []\n",
        "\n",
        "        for node_i in range(layer):  # 層の中の各ノードを処理\n",
        "            b = biases[layer_i - SKIP_INPUT_LAYER][node_i]\n",
        "            grad_b = grads_b[layer_i - SKIP_INPUT_LAYER][node_i]\n",
        "            b = b - lr * grad_b  # バイアスパラメーターの更新\n",
        "            layer_b.append(b)\n",
        "\n",
        "            node_weights = weights[layer_i - SKIP_INPUT_LAYER][node_i]\n",
        "            node_w = []\n",
        "            for each_w_i, w in enumerate(node_weights):\n",
        "                grad_w = grads_w[layer_i - SKIP_INPUT_LAYER][node_i][each_w_i]\n",
        "                w = w - lr * grad_w  # 重みパラメーターの更新\n",
        "                node_w.append(w)\n",
        "            layer_w.append(node_w)\n",
        "\n",
        "        new_weights.append(layer_w)\n",
        "        new_biases.append(layer_b)\n",
        "    \n",
        "    return (new_weights, new_biases)"
      ],
      "metadata": {
        "id": "9bCBA7JwvE2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●パラメーター更新の実行例"
      ],
      "metadata": {
        "id": "te0ar682pzvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のようなコードを書けば、順伝播から逆伝播、パラメーター更新までを続けて実行できます。"
      ],
      "metadata": {
        "id": "MiWOhdY7Blbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [2, 2, 2]\n",
        "weights = [\n",
        "    [[0.15, 0.2], [0.25, 0.3]],\n",
        "    [[0.4, 0.45], [0.5,0.55]]\n",
        "]\n",
        "biases = [[0.35, 0.35], [0.6, 0.6]]\n",
        "model = (layers, weights, biases)\n",
        "\n",
        "# 元の重み\n",
        "print(f'old-weights={weights}')\n",
        "print(f'old-biases={biases}' )\n",
        "# old-weights=[[[0.15, 0.2], [0.25, 0.3]], [[0.4, 0.45], [0.5, 0.55]]]\n",
        "# old-biases=[[0.35, 0.35], [0.6, 0.6]]\n",
        "\n",
        "# （1）順伝播の実行例\n",
        "x = [0.05, 0.1]\n",
        "y_pred, cached_outs, cached_sums = forward_prop(*model, x, cache_mode=True)\n",
        "\n",
        "# （2）逆伝播の実行例\n",
        "y_true = [0.01, 0.99]\n",
        "grads_w, grads_b = back_prop(*model, y_true, cached_outs, cached_sums)\n",
        "print(f'grads_w={grads_w}')\n",
        "print(f'grads_b={grads_b}')\n",
        "# grads_w=[[[0.006706025259285303, 0.013412050518570607], [0.007487461943833829, 0.014974923887667657]], [[0.6501681244277691, 0.6541291517796395], [0.13937181955411934, 0.1402209162240302]]]\n",
        "# grads_b=[[0.13412050518570606, 0.14974923887667657], [1.09590596705977, 0.23492140409646534]]\n",
        "\n",
        "# （3）パラメーター更新の実行例\n",
        "LEARNING_RATE = 0.1 # 学習率（lr）\n",
        "weights, biases = update_params(*model, grads_w, grads_b, lr=LEARNING_RATE)\n",
        "\n",
        "# 更新後の新しい重み\n",
        "print(f'new-weights={weights}')\n",
        "print(f'new-biases={biases}')\n",
        "# new-weights=[[[0.14932939747407145, 0.19865879494814295], [0.2492512538056166, 0.2985025076112332]], [[0.3349831875572231, 0.3845870848220361], [0.48606281804458806, 0.5359779083775971]]]\n",
        "# new-biases=[[0.3365879494814294, 0.33502507611233234], [0.490409403294023, 0.5765078595903534]]"
      ],
      "metadata": {
        "id": "ck4l6GqGp2n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■3つのステップを呼び出す最適化処理の実装"
      ],
      "metadata": {
        "id": "YuRVFxSewyh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●最適化処理：学習方法と勾配降下法"
      ],
      "metadata": {
        "id": "jLanYX6WCcAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "代表的な学習方法を簡単にまとめておきます。\n",
        "\n",
        "- **オンライン学習**（**Online training**）： データ1件ずつ訓練していくこと\n",
        "- **ミニバッチ学習**（**Mini-batch training**）： 小さなまとまりのデータごとに訓練していくこと\n",
        "- **バッチ学習**（**Batch training**）： データ全件で訓練していくこと"
      ],
      "metadata": {
        "id": "IQ2ZpLzMC148"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習方法ごとに、勾配降下法をまとめると以下のようになります。\n",
        "\n",
        "- **SGD**（**Stochastic Gradient Descent**）： オンライン学習\n",
        "- **ミニバッチSGD**（**Mini-batch SGD**）： ミニバッチ学習。単に**ミニバッチ勾配降下法**（**Mini-batch Gradient Descent**）とも呼ぶ\n",
        "- **最急降下法**（**Steepest Descent**）： バッチ学習。**バッチ勾配降下法**（**Batch Gradient Descent**）とも呼ぶ\n"
      ],
      "metadata": {
        "id": "XomOMl8sDC0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "コード内容も簡単なので少し難易度を上げて、あえて全ての学習方法に対応できる実装コードにしてみます。"
      ],
      "metadata": {
        "id": "MCAtNGe8DkG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●最適化の処理全体の実装"
      ],
      "metadata": {
        "id": "zvS7FT0DtiPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "訓練処理では、エポック（＝全データ分で1回の訓練）があり、その中にイテレーション（＝バッチサイズごとでのパラメーターの更新）が存在するという構造ですので、\n",
        "\n",
        "- エポックを1回ずつ処理するforループと\n",
        "  - その中にデータを1件ずつ処理するforループの2段階構造を用意し\n",
        "    - その中に「ステップ①順伝播」「ステップ②逆伝播」と\n",
        "    - イテレーションごとに「ステップ③パラメーターの更新」\n",
        "\n",
        "を記述するようにします（※あくまで筆者による実装方針の例です）。\n"
      ],
      "metadata": {
        "id": "2SM0Ss5dDxsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "階層が深くなる上にコードの行数が少し長いので、説明の都合上、上の箇条書きの前半2行を`train()`親関数、後半2行を`optimize()`子関数、という親子関係の2つの関数に分けて記述します。※1つの関数として実装した方がシンプルになって見通しもよくなるので、本来であればそうした方がよいと思います。\n"
      ],
      "metadata": {
        "id": "3Rya4h8IEE6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# 取りあえず仮で、空の関数を定義して、コードが実行できるようにしておく\n",
        "def optimize(model, x, y, data_i, last_i, batch_i, batch_size, acm_g, lr=0.1):\n",
        "    \" モデルを最適化する関数（子関数）。\"\n",
        "    loss = 0.1\n",
        "    return model, loss, batch_i, acm_g\n",
        "\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "def train(model, x, y, batch_size=32, epochs=10, lr=0.1, verbose=10):\n",
        "    \"\"\"\n",
        "    モデルの訓練を行う関数（親関数）。\n",
        "    - 引数：\n",
        "    model： モデルをタプル「(layers, weights, biases)」で指定する。\n",
        "    x： 訓練データ（各データが行、各特徴量が列の、2次元リスト値）。\n",
        "    y： 訓練ラベル（各データが行、各正解値が列の、2次元リスト値）。\n",
        "    batch_size： バッチサイズ。何件のデータをまとめて処理するか。\n",
        "    epochs： エポック数。全データ分で何回、訓練するか。\n",
        "    lr： 学習率（learning rate）。最適化を進める量を調整する。\n",
        "    verbose： 訓練状況を何エポックおきに出力するか。\n",
        "    - 戻り値：\n",
        "    損失値の履歴を返す。これを使って損失値の推移グラフが描ける。\n",
        "    \"\"\"\n",
        "    loss_history = []  # 損失値の履歴\n",
        "\n",
        "    data_size = len(y)  # 訓練データ数\n",
        "    data_indexes = range(data_size)  # 訓練データのインデックス\n",
        "\n",
        "    # 各エポックを処理\n",
        "    for epoch_i in range(1, epochs + 1):  # 経過表示用に1スタート\n",
        "\n",
        "        acm_loss = 0  # 損失値を蓄積（accumulate）していく\n",
        "\n",
        "        # 訓練データのインデックスをシャッフル（ランダムサンプリング）\n",
        "        random_indexes = random.sample(data_indexes, data_size)\n",
        "        last_i = random_indexes[-1]  # 最後の訓練データのインデックス\n",
        "\n",
        "        # 親関数で管理すべき変数\n",
        "        acm_g = (None, None)  # 重み／バイアスの勾配を蓄積していくため\n",
        "        batch_i = 0  # バッチ番号をインクリメントしていくため\n",
        "\n",
        "        # 訓練データを1件1件処理していく\n",
        "        for data_i in random_indexes:\n",
        "\n",
        "            # 親子に分割したうちの子関数を呼び出す\n",
        "            model, loss, batch_i, acm_g = optimize(\n",
        "                model, x, y, data_i, last_i, batch_i, batch_size, acm_g, lr)\n",
        "\n",
        "            acm_loss += loss  # 損失値を蓄積\n",
        "\n",
        "        # エポックごとに損失値を計算。今回の実装では「平均」する\n",
        "        layers = model[0]  # レイヤー構造\n",
        "        out_count = layers[-1]  # 出力層のノード数\n",
        "        # 「訓練データ数（イテレーション数×バッチサイズ）×出力ノード数」で平均\n",
        "        epoch_loss = acm_loss / (data_size * out_count)\n",
        "\n",
        "        # 訓練状況を出力\n",
        "        if verbose != 0 and \\\n",
        "            (epoch_i % verbose == 0 or epoch_i == 1 or epoch_i == EPOCHS):\n",
        "            print(f'[Epoch {epoch_i}/{EPOCHS}] train_loss: {epoch_loss}')\n",
        "\n",
        "        loss_history.append(epoch_loss)  # 損失値の履歴として保存\n",
        "\n",
        "    return model, loss_history\n",
        "\n",
        "\n",
        "# サンプル実行用の仮のモデルとデータ\n",
        "layers = [2, 2, 2]\n",
        "weights = [\n",
        "    [[0.15, 0.2], [0.25, 0.3]],\n",
        "    [[0.4, 0.45], [0.5,0.55]]\n",
        "]\n",
        "biases = [[0.35, 0.35], [0.6, 0.6]]\n",
        "model = (layers, weights, biases)\n",
        "x = [[0.05, 0.1]]\n",
        "y = [[0.01, 0.99]]\n",
        "\n",
        "# モデルを訓練する\n",
        "BATCH_SIZE = 2  # バッチサイズ\n",
        "EPOCHS = 1  # エポック数\n",
        "LEARNING_RATE = 0.02 # 学習率（lr）\n",
        "model, loss_history = train(model, x, y, BATCH_SIZE, EPOCHS, LEARNING_RATE)\n",
        "# 出力例：\n",
        "# [Epoch 1/1] train_loss: 0.05"
      ],
      "metadata": {
        "id": "D_01bjboj7ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accumulate(list1, list2):\n",
        "    \"2つのリストの値を足し算する関数。\"\n",
        "    new_list = []\n",
        "    for item1, item2 in zip(list1, list2):\n",
        "        if isinstance(item1, list):\n",
        "            child_list = accumulate(item1, item2)\n",
        "            new_list.append(child_list)\n",
        "        else:\n",
        "            new_list.append(item1 + item2)\n",
        "    return new_list\n",
        "\n",
        "# （NumPy利用バージョン）\n",
        "# import numpy as np\n",
        "# def accumulate(list1, list2):\n",
        "#     \"2つのリストの値を足し算する関数。\"\n",
        "#     new_list = []\n",
        "#     for item1, item2 in zip(list1, list2):\n",
        "#         # ※全体の重み勾配は行数と列数が同じではないので層ごとに処理する必要がある。\n",
        "#         np_sum = np.array(item1) + np.array(item2)  # NumPyなら行列データをまとめて処理できる\n",
        "#         new_list.append(np_sum.tolist())\n",
        "#     return new_list"
      ],
      "metadata": {
        "id": "Fj3iui6Oy0X7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_element(list1, data_count):\n",
        "    \"1つのリストの値をデータ数で平均する関数。\"\n",
        "    new_list = []\n",
        "    for item1 in list1:\n",
        "        if isinstance(item1, list):\n",
        "            child_list = mean_element(item1, data_count)\n",
        "            new_list.append(child_list)\n",
        "        else:\n",
        "            new_list.append(item1 / data_count)\n",
        "    return new_list\n",
        "\n",
        "# （NumPy利用バージョン）\n",
        "# import numpy as np\n",
        "# def mean_element(list1, data_count):\n",
        "#     \"1つのリストの値をデータ数で平均する関数。\"\n",
        "#     new_list = []\n",
        "#     for item1 in list1:\n",
        "#         # ※全体の重み勾配は行数と列数が同じではないので層ごとに処理する必要がある。\n",
        "#         np_mean = np.array(item1) / data_count  # NumPyなら行列データをまとめて処理できる\n",
        "#         new_list.append(np_mean.tolist())\n",
        "#     return new_list"
      ],
      "metadata": {
        "id": "9QUcfFfxzq00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize(model, x, y, data_i, last_i, batch_i, batch_size, acm_g, lr=0.1):\n",
        "    \"train()親関数から呼ばれる、最適化のための子関数。\"\n",
        "\n",
        "    layers = model[0]  # レイヤー構造\n",
        "    each_x = x[data_i]  # 1件分の訓練データ\n",
        "    y_true = y[data_i]  # 1件分の正解値\n",
        "\n",
        "    # ステップ（1）順伝播\n",
        "    y_pred, outs, sums = forward_prop(*model, each_x, cache_mode=True)\n",
        "\n",
        "    # ステップ（2）逆伝播\n",
        "    gw, gb = back_prop(*model, y_true, outs, sums)\n",
        "\n",
        "    # 各勾配を蓄積（accumulate）していく\n",
        "    if batch_i == 0:\n",
        "        acm_gw = gw\n",
        "        acm_gb = gb\n",
        "    else:\n",
        "        acm_gw = accumulate(acm_g[0], gw)\n",
        "        acm_gb = accumulate(acm_g[1], gb)\n",
        "    batch_i += 1  # バッチ番号をカウントアップ＝現在のバッチ数\n",
        "\n",
        "    # 訓練状況を評価するために、損失値を取得\n",
        "    loss = 0.0\n",
        "    for output, target in zip(y_pred, y_true):\n",
        "        loss += sseloss(output, target)\n",
        "\n",
        "    # バッチサイズごとで後続の処理に進む\n",
        "    if batch_i % BATCH_SIZE != 0 and data_i != last_i:\n",
        "        return model, loss, batch_i, (acm_gw, acm_gb)  # バッチ内のデータごと\n",
        "\n",
        "    layers = model[0]  # レイヤー構造\n",
        "    out_count = layers[-1]  # 出力層のノード数\n",
        "\n",
        "    # 平均二乗誤差なら平均する（損失関数によって異なる）\n",
        "    grads_w = mean_element(acm_gw, batch_i * out_count)  # 「バッチサイズ ×\n",
        "    grads_b = mean_element(acm_gb, batch_i * out_count)  #   出力ノード数」で平均\n",
        "    batch_i = 0  # バッチ番号を初期化して次のイテレーションに備える\n",
        "\n",
        "    # ステップ（3）パラメーター（重みとバイアス）の更新\n",
        "    weights, biases = update_params(*model, grads_w, grads_b, lr)\n",
        "\n",
        "    # モデルをアップデート（＝最適化）\n",
        "    model = (layers, weights, biases)\n",
        "\n",
        "    return model, loss, batch_i, (acm_gw, acm_gb)  # イテレーションごと\n",
        "\n",
        "\n",
        "# サンプル実行\n",
        "model, loss_history = train(model, x, y, BATCH_SIZE, EPOCHS, LEARNING_RATE)\n",
        "# 出力例：\n",
        "# [Epoch 1/1] train_loss: 0.31404948868496607"
      ],
      "metadata": {
        "id": "aLnd_nyxuVnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■回帰問題を解くデモ"
      ],
      "metadata": {
        "id": "sYJQqNtsPlim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "特徴量（入力データ）は$x_1$（X軸）と$x_2$（Y軸）の座標です。その座標点における色、具体的にはオレンジ色（**-1**）～灰色（**0**）～青色（**1**）を予測する回帰問題となります。"
      ],
      "metadata": {
        "id": "dAP3OvdiGT0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まず訓練データを用意します。このデモの訓練データは、「[回帰問題をディープラーニング（基本のDNN）で解こう](https://atmarkit.itmedia.co.jp/ait/articles/2005/25/news011.html)」でも使っているライブラリー「playground-data」の平面（Plain）データセットです。\n"
      ],
      "metadata": {
        "id": "5DFBc6foGjxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playground-data"
      ],
      "metadata": {
        "id": "DSDpoPptvLyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# playground-dataライブラリのplygdataパッケージを「pg」という別名でインポート\n",
        "import plygdata as pg\n",
        "\n",
        "# 問題種別で「分類（Classification）」を選択し、\n",
        "# データ種別で「2つのガウシアンデータ（TwoGaussData）」を選択する場合の、\n",
        "# 設定値を定数として定義\n",
        "PROBLEM_DATA_TYPE = pg.DatasetType.RegressPlane\n",
        "\n",
        "# 各種設定を定数として定義\n",
        "TRAINING_DATA_RATIO = 0.5  # データの何％を訓練【Training】用に？ (残りは精度検証【Validation】用) ： 50％\n",
        "DATA_NOISE = 0.0           # ノイズ： 0％\n",
        "\n",
        "# 定義済みの定数を引数に指定して、データを生成する\n",
        "data_list = pg.generate_data(PROBLEM_DATA_TYPE, DATA_NOISE)\n",
        "\n",
        "# データを「訓練用」と「精度検証用」を指定の比率で分割し、さらにそれぞれを「データ（X）」と「教師ラベル（y）」に分ける\n",
        "X_train, y_train, _, _ = pg.split_data(data_list, training_size=TRAINING_DATA_RATIO)\n",
        "\n",
        "# それぞれ5件ずつ出力\n",
        "print('X_train:'); print(X_train[:5])\n",
        "print('y_train:'); print(y_train[:5])"
      ],
      "metadata": {
        "id": "Rkn4rwV-vNtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に、モデルを定義します。"
      ],
      "metadata": {
        "id": "ODXzS-VOG4KX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [2, 3, 1]\n",
        "weights = [\n",
        "    [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]],\n",
        "    [[0.0, 0.0, 0.0]]\n",
        "]\n",
        "biases = [\n",
        "    [0.0, 0.0, 0.0],  # hidden1\n",
        "    [0.0]  # output\n",
        "]\n",
        "model = (layers, weights, biases)"
      ],
      "metadata": {
        "id": "gyGB3RpcxT79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "訓練「前」のモデルによる予測状態を図示します。"
      ],
      "metadata": {
        "id": "I08F5zRgHJ1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `draw_decision_boundary()`関数がクラス内の`predict()`メソッドを呼び出す仕様のため\n",
        "class MyModel:\n",
        "    def __init__(self, l, w, b):\n",
        "        self.layers = l\n",
        "        self.weights = w\n",
        "        self.biases = b\n",
        "\n",
        "    def predict(self, x, batch_size=1, verbose=False):\n",
        "        probability = []\n",
        "        for each_x in x:\n",
        "            y = forward_prop(self.layers, self.weights, self.biases, each_x)\n",
        "            probability.append(y[0])\n",
        "        return probability\n",
        "\n",
        "# 出力のグラフ表示\n",
        "trained_model = MyModel(*model)\n",
        "fig, ax = pg.plot_points_with_playground_style(X_train, y_train, None, None, figsize = (6, 6), dpi = 100)\n",
        "pg.draw_decision_boundary(fig, ax, trained_model=trained_model, discretize=False)"
      ],
      "metadata": {
        "id": "jOnl4C1svRCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "それぞれの丸い点の座標は、訓練データ1件1件の特徴量を表します。その点の色が正解ラベルです。例えば左下の座標点でれば、色はオレンジ色、つまり**-1.0**に近い値が正解となります。右上が青色、つまり**1.0**に近い値が正解です。\n",
        "\n",
        "モデルによる予測値は、背景色として描画されています。上の図は全面が灰色です。これは、どの座標を入力しても、**0.0**が予測されることを意味します。これをニューラルネットで学習することで、オレンジ色の座標点の背景色はオレンジ色に、青色の座標点の背景色は青色に描画されるようにします。\n"
      ],
      "metadata": {
        "id": "WRzM-KZdHPIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後に、訓練処理の`train()`関数を呼び出すだけです。"
      ],
      "metadata": {
        "id": "h9TyZr0_HBSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "BATCH_SIZE = 4   # バッチサイズ\n",
        "EPOCHS = 100     # エポック数\n",
        "LERNING_RATE = 0.02  # 学習係数\n",
        "\n",
        "model, loss_history = train(model, X_train, y_train, BATCH_SIZE, EPOCHS, LEARNING_RATE)\n",
        "\n",
        "# 学習結果（損失）のグラフを描画\n",
        "epochs = len(loss_history)\n",
        "plt.plot(range(1, epochs + 1), loss_history, marker='.', label='loss (Training data)')\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sBqxFJ_2izHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "重みやバイアスはモデル（タプル型オブジェクト）の中に格納されています。"
      ],
      "metadata": {
        "id": "1PaQCZIlHn8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'weights={model[1]}')\n",
        "print(f'biases={model[2]}')"
      ],
      "metadata": {
        "id": "067jKgb1vasf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "訓練「後」のモデルによる予測状態を図示します。"
      ],
      "metadata": {
        "id": "gHMVBGWgH0l6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 出力のグラフ表示\n",
        "trained_model = MyModel(*model)\n",
        "fig, ax = pg.plot_points_with_playground_style(X_train, y_train, None, None, figsize = (6, 6), dpi = 100)\n",
        "pg.draw_decision_boundary(fig, ax, trained_model=trained_model, discretize=False)"
      ],
      "metadata": {
        "id": "Iy3fXcIZvc3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "線形代数（NumPy）なしで作ってきた自作のニューラルネットで、確かに回帰問題を解けることが確認できました。"
      ],
      "metadata": {
        "id": "rJqY8cPIH-ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# お疲れさまでした。基礎編（第1回～第3回）は修了です。"
      ],
      "metadata": {
        "id": "gY5L18RrOzHb"
      }
    }
  ]
}