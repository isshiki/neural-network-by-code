{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_from_scratch_without_numpy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "private_outputs": true,
      "authorship_tag": "ABX9TyOxudl+ZEu8qf24gkEjEWEB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isshiki/neural-network-by-code/blob/main/01-forward-prop/nn_from_scratch_without_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2021-2022 Digital Advantage - Deep Insider."
      ],
      "metadata": {
        "id": "I8yy30L04inZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_QLRIOK4fVd"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "連載『ニューラルネットワーク入門』\n",
        "# 「コードで必ず分かるニューラルネットワーク（DNN）の逆伝播」\n"
      ],
      "metadata": {
        "id": "6Dza91D34tmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table valign=\"middle\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://atmarkit.itmedia.co.jp/ait/subtop/features/di/neuralnetwork_index.html\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/manabu.svg\"/>Deep Insiderで記事を読む</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/isshiki/neural-network-by-code/tree/main/01-forward-prop/nn_from_scratch_without_numpy.ipynb\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/gcolab.svg\" />Google Colabで実行する</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://studiolab.sagemaker.aws/import/github/isshiki/neural-network-by-code/tree/main/01-forward-prop/nn_from_scratch_without_numpy.ipynb\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/astudiolab.svg\" />AWS  SageMaker Studio Labで実行する</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/isshiki/neural-network-by-code/tree/main/01-forward-prop/nn_from_scratch_without_numpy.ipynb\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/github.svg\" />GitHubでソースコードを見る</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "XyoG00sPzIZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■本ノートブックの目的"
      ],
      "metadata": {
        "id": "_V_STJSgw46b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットワーク（以下、ニューラルネット）の仕組みを、数学理論からではなく**Pythonコードから学ぶ**ことを狙っています。「難しい高校以降の数学は苦手だけど、コードなら読めるぜ！」という方にピッタリです。"
      ],
      "metadata": {
        "id": "ZAlfbhv1w6mI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●本ノートブックの特徴"
      ],
      "metadata": {
        "id": "jQlmkwbNxUle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 線形代数（linear algebra、行列演算）を使いません。つまり、NumPyを使いません。\n",
        "- 基本的に掛け算や足し算などの中学までの数学のみで、ニューラルネットのロジックをコーディングしていきます。\n",
        "- ※微分の導関数は、そのままコードとして記載することで、微分の計算は取り上げません。\n",
        "\n",
        "![図1　線形代数は使わないことによるメリット](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/01-forward-prop/images/01.png)  \n",
        "図1　線形代数は使わないことによるメリット"
      ],
      "metadata": {
        "id": "8X8ZSGoFxciU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■ニューラルネットワークの図"
      ],
      "metadata": {
        "id": "rjtsPfbhwyOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "基本的なニューラルネット（この例では、入力層：2、隠れ層：3、出力層：1）の図を確認しておきます。\n",
        "\n",
        "![図2　ニューラルネットワークの図（左：通常、右：本ノートブック）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/01-forward-prop/images/02.png)"
      ],
      "metadata": {
        "id": "0dKydpNZydl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■訓練（学習）処理全体の実装"
      ],
      "metadata": {
        "id": "sT1m3Bseyytq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "深層「学習」のメインは、ニューラルネットの「訓練」処理ですよね。ここから書いていきます。"
      ],
      "metadata": {
        "id": "32U5YXJyQU08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず空の関数を定義して、コードが実行できるようにしておく\n",
        "def forward_prop(cache_mode=False):\n",
        "    \" 順伝播を行う関数。\"\n",
        "    return None, None, None\n",
        "\n",
        "y_true = 1.0  # 正解値\n",
        "def back_prop(y_true, cached_outs, cached_sums):\n",
        "    \" 逆伝播を行う関数。\"\n",
        "    return None, None\n",
        "\n",
        "LEARNING_RATE = 0.1 # 学習率\n",
        "def update_params(grads_w, grads_b, lr=0.1):\n",
        "    \" パラメーター（重みとバイアス）を更新する関数。\"\n",
        "    return None, None\n",
        "\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "# 訓練処理\n",
        "y_pred, cached_outs, cached_sums = forward_prop(cache_mode=True)  # （1）\n",
        "grads_w, grads_b = back_prop(y_true, cached_outs, cached_sums)  # （2）\n",
        "weights, biases = update_params(grads_w, grads_b, LEARNING_RATE)  # （3）\n",
        "\n",
        "print(f'予測値：{y_pred}')  # 予測値： None\n",
        "print(f'正解値：{y_true}')  # 正解値： 1.0"
      ],
      "metadata": {
        "id": "2GJPJd3EjnLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットの訓練に必要なことは、以下の3つだけ。\n",
        "\n",
        "1. **順伝播：** `forward_prop()◎`数として実装\n",
        "2. **逆伝播：** `back_prop()`関数として実装\n",
        "3. **パラメーター（重みとバイアス）の更新：** `update_params()`関数として実装。これによりモデルが**最適化**される\n"
      ],
      "metadata": {
        "id": "mtBTnQCjQuhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図3　訓練（学習）処理を示したニューラルネットワーク図](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/01-forward-prop/images/03.png)"
      ],
      "metadata": {
        "id": "yv5BZKpHPXIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##■モデルの定義と、仮の訓練データ"
      ],
      "metadata": {
        "id": "uL1t6_R_5vX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "入力層のノードが2個、隠れ層のノードが3個、出力層のノードが1個のモデル（`model`変数）を定義しましょう。"
      ],
      "metadata": {
        "id": "tZ76R4MbRKmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ニューラルネットワークは3層構成\n",
        "layers = [\n",
        "    2,  # 入力層の入力（特徴量）の数\n",
        "    3,  # 隠れ層1のノード（ニューロン）の数\n",
        "    1]  # 出力層のノードの数\n",
        "\n",
        "# 重みとバイアスの初期値\n",
        "weights = [\n",
        "    [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]], # 入力層→隠れ層1\n",
        "    [[0.0, 0.0, 0.0]] # 隠れ層1→出力層\n",
        "]\n",
        "biases = [\n",
        "    [0.0, 0.0, 0.0],  # 隠れ層1\n",
        "    [0.0]  # 出力層\n",
        "]\n",
        "\n",
        "# モデルを定義\n",
        "model = (layers, weights, biases)\n",
        "\n",
        "# 仮の訓練データ（1行分）を準備\n",
        "x = [0.05, 0.1]  # x_1とx_2の2つの特徴量"
      ],
      "metadata": {
        "id": "7sW0f2uH50eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■ステップ1. 順伝播の実装"
      ],
      "metadata": {
        "id": "Xm1tlcIiQOcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットの最小単位である「1つのノード」における順伝播の処理をコーディングしましょう。"
      ],
      "metadata": {
        "id": "e5qxZXE2RVQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 取りあえず空の関数を定義して、コードが実行できるようにしておく\n",
        "def summation(x,weights, bias):\n",
        "    \" 重み付き線形和の関数。\"\n",
        "    return 0.0\n",
        "\n",
        "def sigmoid(x):\n",
        "    \" シグモイド関数。\"\n",
        "    return 0.0\n",
        "\n",
        "def identity(x):\n",
        "    \" 恒等関数。\"\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "w = [0.0, 0.0]  # 重み（仮の値）\n",
        "b = 0.0  # バイアス（仮の値）\n",
        "\n",
        "next_x = x  # 訓練データをノードへの入力に使う\n",
        "\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "# 1つのノードの処理（1）： 重み付き線形和\n",
        "node_sum = summation(next_x, w, b)\n",
        "\n",
        "# 1つのノードの処理（2）： 活性化関数\n",
        "is_hidden_layer = True\n",
        "if is_hidden_layer:\n",
        "    # 隠れ層（シグモイド関数）\n",
        "    node_out = sigmoid(node_sum)\n",
        "else:\n",
        "    # 出力層（恒等関数）\n",
        "    node_out = identity(node_sum)"
      ],
      "metadata": {
        "id": "I4-XMugzbUGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1つのノードの順伝播処理に必要なことは、以下の2つの数学関数だけ。\n",
        "\n",
        "1. **重み付き線形和の関数：** `summation()`関数として実装\n",
        "2. **活性化関数：** ここでは`sigmoid()`関数や`identity()`関数として実装"
      ],
      "metadata": {
        "id": "tPBzLn3hW0zU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![図4　訓練（学習）処理を示したニューラルネットワーク図](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/01-forward-prop/images/04.png)"
      ],
      "metadata": {
        "id": "V7DKfcYyPf1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●重み付き線形和"
      ],
      "metadata": {
        "id": "kCQTEdpGcv7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "重み付き線形和（weighted linear summation、以下では「線形和」と表記）とは、とは、あるノードへの複数の入力（$x_1$、$x_2$など）に、それぞれの重み（$w_1$、$w_2$など）を掛けて足し合わせて、最後にバイアス（$b$）を足した値です（上の図の左）。"
      ],
      "metadata": {
        "id": "VX8bRXh2Riek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summation(x, weights, bias):\n",
        "    \"\"\"\n",
        "    重み付き線形和の関数。\n",
        "    ※1データ分、つまりxとweightsは「一次元リスト」という前提。\n",
        "    - 引数：\n",
        "    x： 入力データをリスト値（各要素はfloat値）で指定する。\n",
        "    weights： 重みをリスト値（各要素はfloat値）で指定する。\n",
        "    bias： バイアスをfloat値で指定する。\n",
        "    - 戻り値：\n",
        "    線形和の計算結果をfloat値で返す。。\n",
        "    \"\"\"\n",
        "    linear_sum = 0.0\n",
        "    for x_i, w_i in zip(x, weights):\n",
        "        linear_sum += x_i * w_i  # iは番号の意味（数学は基本的に1スタート）\n",
        "        # print(f'x_i({x_i})×w_i({w_i})＋', end='')\n",
        "    linear_sum += bias\n",
        "    # print(f'b({bias})', end='')\n",
        "    return linear_sum\n",
        "\n",
        "# 線形代数を使う場合のコード例：\n",
        "# linear_sum = np.dot(x, weights) + bias"
      ],
      "metadata": {
        "id": "DuXPPnolcHHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ついで、次回の逆伝播（の中で使う偏微分）で必要となる線形和（linear **sum**mation）の偏導関数（partial **der**ivative function）を実装しておきます。"
      ],
      "metadata": {
        "id": "VxKE5sNZSFq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_der(x, weights, bias, with_respect_to='w'):\n",
        "    \"\"\"\n",
        "    重み付き線形和の関数の偏導関数。\n",
        "    ※1データ分、つまりxとweightsは「一次元リスト」という前提。\n",
        "    - 引数：\n",
        "    x： 入力データをリスト値で指定する。\n",
        "    weights：  重みをリスト値で指定する。\n",
        "    bias: バイアスをfloat値で指定する。\n",
        "    with_respect_to: 何に関して偏微分するかを指定する。\n",
        "        'w'＝ 重み、'b'＝ バイアス、'x'＝ 入力。\n",
        "    - 戻り値：\n",
        "    with_respect_toが'w'や'x'の場合はリスト値で、'b'の場合はfloat値で\n",
        "        線形和の偏微分の計算結果（微分係数）を返す。\n",
        "    \"\"\"    \n",
        "    if with_respect_to == 'w':\n",
        "        return x  # 線形和uを各重みw_iで偏微分するとx_iになる（iはノード番号）。\n",
        "    if with_respect_to == 'b':\n",
        "        return 1.0  # 線形和uをバイアスbで偏微分すると1になる。\n",
        "    elif with_respect_to == 'x':\n",
        "        return weights  # 線形和uを各入力x_iで偏微分するとw_iになる。"
      ],
      "metadata": {
        "id": "Ob-1saVLdtre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●活性化関数：シグモイド関数"
      ],
      "metadata": {
        "id": "09TqjNnXidxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "隠れ層では、最も基礎的なシグモイド関数（Sigmoid function）を固定的に使うことにします。導関数も実装しておきます。"
      ],
      "metadata": {
        "id": "HeCjZU9STJOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    シグモイド関数。\n",
        "    - 引数：\n",
        "    x： 入力データをfloat値で指定する。\n",
        "    - 戻り値：\n",
        "    シグモイド関数の計算結果をfloat値で返す。\n",
        "    \"\"\"\n",
        "    return 1.0 / (1.0 + math.exp(-x))\n",
        "\n",
        "# 線形代数の場合はmathをnpに変える（事前にimport numpy as np）"
      ],
      "metadata": {
        "id": "RAXhKRDyif96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_der(x):\n",
        "    \"\"\"\n",
        "    シグモイド関数の（偏）導関数。\n",
        "    - 引数：\n",
        "    x： 入力データをfloat値で指定する。\n",
        "    - 戻り値：\n",
        "    シグモイド関数の（偏）微分の計算結果（微分係数）をfloat値で返す。\n",
        "    \"\"\"\n",
        "    output = sigmoid(x)\n",
        "    return output * (1.0 - output)"
      ],
      "metadata": {
        "id": "dPghF0t5iiMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●活性化関数：恒等関数"
      ],
      "metadata": {
        "id": "aNH6Mq72jsTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "出力層では、回帰問題をイメージして、そのままの値を出力する活性化関数である恒等関数（Identity function）を使用します。導関数も実装しておきます。"
      ],
      "metadata": {
        "id": "rPevusfVTaz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identity(x):\n",
        "    \"\"\"\n",
        "    恒等関数の関数。\n",
        "    - 引数：\n",
        "    x： 入力データをfloat値で指定する。\n",
        "    - 戻り値：\n",
        "    恒等関数の計算結果（そのまま）をfloat値で返す。\n",
        "    \"\"\"\n",
        "    return x"
      ],
      "metadata": {
        "id": "8oVoxKncjrzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_der(x):\n",
        "    \"\"\"\n",
        "    恒等関数の（偏）導関数。\n",
        "    - 引数：\n",
        "    x： 入力データをfloat値で指定する。\n",
        "    - 戻り値：\n",
        "    恒等関数の（偏）微分の計算結果（微分係数）をfloat値で返す。\n",
        "    \"\"\"\n",
        "    return 1.0"
      ],
      "metadata": {
        "id": "B_4avMTJjvog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●順伝播の処理全体の実装"
      ],
      "metadata": {
        "id": "j2nmNTn0OmPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットには、層があり、その中に複数のノードが存在するという構造です。  従って、\n",
        "\n",
        "- 各層を1つずつ処理する`for`ループと、  \n",
        "  - 層の中のノードを1つずつ処理する`for`ループの2段階構造が必要で、\n",
        "    - その中に「1つのノードにおける処理」\n",
        "\n",
        "を記述すればよいわけです。"
      ],
      "metadata": {
        "id": "_jmw6snqTsJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(layers, weights, biases, x, cache_mode=False):\n",
        "    \"\"\"\n",
        "    順伝播を行う関数。\n",
        "    - 引数：\n",
        "    (layers, weights, biases)： モデルを指定する。\n",
        "    x： 入力データを指定する。\n",
        "    cache_mode： 予測時はFalse、訓練時はTrueにする。これにより戻り値が変わる。\n",
        "    - 戻り値：\n",
        "    cache_modeがFalse時は予測値のみを返す。True時は、予測値だけでなく、\n",
        "        キャッシュに記録済みの線形和（Σ）値と、活性化関数の出力値も返す。\n",
        "    \"\"\"\n",
        "\n",
        "    cached_sums = []  # 記録した全ノードの線形和（Σ）の値\n",
        "    cached_outs = []  # 記録した全ノードの活性化関数の出力値\n",
        "\n",
        "    # まずは、入力層を順伝播する\n",
        "    # print(f'■第1層（入力層）-全て（{len(x)}個）の特徴量：')\n",
        "    outs = x  # \n",
        "    # print(f'　●入力データ： ', end='')\n",
        "    cached_outs.append(x)  # 何も処理せずに出力値を記録\n",
        "    # print(f'何もしない＝out({x})')\n",
        "    next_x = x  # 現在の層の出力（x）＝次の層への入力（next_x）\n",
        "\n",
        "    # 次に、隠れ層や出力層を順伝播する\n",
        "    SKIP_INPUT_LAYER = 1\n",
        "    for layer_i, layer in enumerate(layers):  # 各層を処理\n",
        "        if layer_i == 0:\n",
        "            continue  # 上で処理済み\n",
        "\n",
        "        # 各レイヤーのノードごとに処理を行う\n",
        "        sums = []\n",
        "        outs = []\n",
        "        for node_i in range(layer):  # 層の中のノードを処理\n",
        "            # print(f'■第{layer_i+1}層-第{node_i+1}ノード：')\n",
        "\n",
        "            # ノードごとの重みとバイアスを取得\n",
        "            w = weights[layer_i-SKIP_INPUT_LAYER][node_i]\n",
        "            b = biases[layer_i-SKIP_INPUT_LAYER][node_i]\n",
        "\n",
        "            # 1つのノードの処理（1）： 重み付き線形和\n",
        "            # print(f'　●重み付き線形和： ', end='')\n",
        "            node_sum = summation(next_x, w, b)\n",
        "            # print(f'＝sum({node_sum})')\n",
        "\n",
        "            # 1つのノードの処理（2）： 活性化関数\n",
        "            if layer_i < len(layers)-SKIP_INPUT_LAYER:\n",
        "                # 隠れ層（シグモイド関数）\n",
        "                # print(f'　●活性化関数（隠れ層はシグモイド関数）： ', end='')\n",
        "                node_out = sigmoid(node_sum)\n",
        "                # print(f'sigmoid({node_sum})＝out({node_out})')\n",
        "            else:\n",
        "                # 出力層（恒等関数）\n",
        "                # print(f'　●活性化関数（出力層は恒等関数）： ', end='')\n",
        "                node_out = identity(node_sum)\n",
        "                # print(f'identity({node_sum})＝out({node_out})')\n",
        "\n",
        "            # 各ノードの線形和と（活性化関数の）出力をリストにまとめていく\n",
        "            sums.append(node_sum)\n",
        "            outs.append(node_out)\n",
        "\n",
        "        # 各層内の全ノードの線形和と出力を記録\n",
        "        cached_sums.append(sums)\n",
        "        cached_outs.append(outs)\n",
        "        next_x = outs  # 現在の層の出力（outs）＝次の層への入力（next_x）\n",
        "\n",
        "    if cache_mode:\n",
        "        return (cached_outs[-1], cached_outs, cached_sums)\n",
        "\n",
        "    return cached_outs[-1]\n",
        "\n",
        "\n",
        "# 訓練時の（1）順伝播の実行例\n",
        "y_pred, cached_outs, cached_sums = forward_prop(*model, x, cache_mode=True)\n",
        "# ※先ほど作成したモデルと訓練データを引数で受け取るよう改変した\n",
        "\n",
        "print(f'cached_outs={cached_outs}')\n",
        "print(f'cached_sums={cached_sums}')\n",
        "# 出力例：\n",
        "# cached_outs=[[0.05, 0.1], [0.5, 0.5, 0.5], [0.0]]  # 入力層／隠れ層1／出力層\n",
        "# cached_sums=[[0.0, 0.0, 0.0], [0.0]]  # 隠れ層1／出力層（※入力層はない）"
      ],
      "metadata": {
        "id": "OlfyIL3VkGXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "数値が**0.0**ばかりなので、別の計算パターンのコードも入れておきました。"
      ],
      "metadata": {
        "id": "sEwU0TN8U7ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 記事にはないが。別の計算パターンでもチェックしてみよう\n",
        "x3 = [0.05, 0.1]\n",
        "layers3 = [2, 2, 2]\n",
        "weights3 = [\n",
        "    [[0.15, 0.2], [0.25, 0.3]],\n",
        "    [[0.4, 0.45], [0.5,0.55]]\n",
        "]\n",
        "biases3 = [[0.35, 0.35], [0.6, 0.6]]\n",
        "model3 = (layers3, weights3, biases3)\n",
        "\n",
        "y_pred3, cached_outs3, cached_sums3 = forward_prop(*model3, x3, cache_mode=True)\n",
        "print(f'y_pred={y_pred3}')\n",
        "print(f'cached_outs={cached_outs3}')\n",
        "print(f'cached_sums={cached_sums3}')\n",
        "# y_pred=[1.10590596705977, 1.2249214040964653]\n",
        "# cached_outs=[[0.05, 0.1], [0.5932699921071872, 0.596884378259767], [1.10590596705977, 1.2249214040964653]]\n",
        "# cached_sums=[[0.3775, 0.39249999999999996], [1.10590596705977, 1.2249214040964653]]"
      ],
      "metadata": {
        "id": "7f2urrsRWazm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここまでのコード中に仕込んでいる`print()`関数を（※全てコメントアウトしています）のコメントを解除すると、以下のように途中の計算内容が順番にテキスト出力されます。計算内容の検証用の機能です。"
      ],
      "metadata": {
        "id": "9wOPPW0YVcH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![別の計算パターンの出力例](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/01-forward-prop/images/notebook-01.png)"
      ],
      "metadata": {
        "id": "aRfWOdtGPm41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●順伝播による予測の実行例"
      ],
      "metadata": {
        "id": "8Y2y79vNGoUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "非常にシンプルで原始的な実装ですが、このように任意の層数とノード数の全結合のDNN（Deep Neural Network）のアーキテクチャーを定義して、DNNモデルによる予測が行えます。"
      ],
      "metadata": {
        "id": "ym7sZcmzWW7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 異なるDNNアーキテクチャーを定義してみる\n",
        "layers2 = [\n",
        "    2,  # 入力層の入力（特徴量）の数\n",
        "    3,  # 隠れ層1のノード（ニューロン）の数\n",
        "    2,  # 隠れ層2のノード（ニューロン）の数\n",
        "    1]  # 出力層のノードの数\n",
        "\n",
        "# 重みとバイアスの初期値\n",
        "weights2 = [\n",
        "    [[-0.2, 0.4], [-0.4, -0.5], [-0.4, -0.5]], # 入力層→隠れ層1\n",
        "    [[-0.2, 0.4, 0.9], [-0.4, -0.5, -0.2]], # 入力層→隠れ層1\n",
        "    [[-0.5, 1.0]] # 隠れ層1→出力層\n",
        "]\n",
        "biases2 = [\n",
        "    [0.1, -0.1, 0.1],  # 隠れ層1\n",
        "    [0.2, -0.2],  # 隠れ層1\n",
        "    [0.3]  # 出力層\n",
        "]\n",
        "\n",
        "# モデルを定義\n",
        "model2 = (layers2, weights2, biases2)\n",
        "\n",
        "# 仮の訓練データ（1行分）を準備\n",
        "x2 = [2.3, 1.5]  # x_1とx_2の2つの特徴量\n",
        "\n",
        "# 予測時の（1）順伝播の実行例\n",
        "y_pred = forward_prop(*model2, x2)\n",
        "print(y_pred)  # 予測値\n",
        "# 出力例：\n",
        "# [0.3828840428423274]"
      ],
      "metadata": {
        "id": "U0sgGfN2kOa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●今後のステップの準備：関数への仮引数の追加"
      ],
      "metadata": {
        "id": "EbO0sBxLVxjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(layers, weights, biases, y_true, cached_outs, cached_sums):\n",
        "    \" 逆伝播を行う関数。\"\n",
        "    return None, None\n",
        "\n",
        "def update_params(layers, weights, biases, grads_w, grads_b, lr=0.1):\n",
        "    \" パラメーター（重みとバイアス）を更新する関数。\"\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "ap_S235RV36g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# お疲れさまでした。第1回は以上です。\n",
        "第2回と第3回はこのノートブックをアップデートする予定です。"
      ],
      "metadata": {
        "id": "gY5L18RrOzHb"
      }
    }
  ]
}