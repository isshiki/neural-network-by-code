{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isshiki/neural-network-by-code/blob/main/04-linear-algebra/nn_from_scratch_with_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8yy30L04inZ"
      },
      "source": [
        "##### Copyright 2021-2022 Digital Advantage - Deep Insider."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_QLRIOK4fVd"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Dza91D34tmx"
      },
      "source": [
        "連載『ニューラルネットワーク入門』\n",
        "# 「NumPyでニューラルネットワークをフルスクラッチ実装してみよう」\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyoG00sPzIZY"
      },
      "source": [
        "<table valign=\"middle\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://atmarkit.itmedia.co.jp/ait/articles/2202/09/news027.html\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/manabu.svg\"/>Deep Insiderで記事を読む</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/isshiki/neural-network-by-code/blob/main/04-linear-algebra/nn_from_scratch_with_numpy.ipynb\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/gcolab.svg\" />Google Colabで実行する</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://studiolab.sagemaker.aws/import/github/isshiki/neural-network-by-code/tree/main/04-linear-algebra/nn_from_scratch_with_numpy.ipynb\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/astudiolab.svg\" />AWS  SageMaker Studio Labで実行する</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/isshiki/neural-network-by-code/blob/main/04-linear-algebra/nn_from_scratch_with_numpy.ipynb\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/github.svg\" />GitHubでソースコードを見る</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWomeA__rv5-"
      },
      "source": [
        "※上から順に実行してください。上のコードで実行したものを再利用しているところがあるため、すべて実行しないとエラーになるコードがあります。  \n",
        "　すべてのコードを一括実行したい場合は、Colabであればメニューバーから［ランタイム］－［すべてのセルを実行］をクリックしてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V_STJSgw46b"
      },
      "source": [
        "## ■本ノートブックの目的"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAlfbhv1w6mI"
      },
      "source": [
        "ニューラルネットワーク（以下、ニューラルネット）の仕組みを、数学理論と**Python＋NumPyのコードから学ぶ**ことを狙っています。「線形代数を使ったニューラルネットワークの基礎を押さえたい！」という方にピッタリです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQlmkwbNxUle"
      },
      "source": [
        "### ●本ノートブックのポイント"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X8ZSGoFxciU"
      },
      "source": [
        "- 線形代数（linear algebra、行列演算）、つまりNumPyを使用します。\n",
        "- 「[Pythonでニューラルネットワークを書いてみよう](https://atmarkit.itmedia.co.jp/ait/articles/2202/09/news027.html)」（基礎編）で`for`ループを使っていた繰り返し処理部分のコードを、NumPyを使うコードに置き換えるだけにしています。\n",
        "- 基礎編と同様に、入力データはバッチサイズの行列（二次元配列）では扱わず、1件ずつのベクトル（一次元配列）で扱います（その分、よりシンプルな線形代数の計算式となっています）。\n",
        "- ※微分の導関数は、そのままコードとして記載することで、微分の計算は取り上げません。\n",
        "\n",
        "※以下の3つの図に記載された数式の意味は後述します。\n",
        "\n",
        "![図1　順伝播に置ける重み付き線形和の処理コードをNumPy化（線形代数化）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/04-linear-algebra/images/01.png)\n",
        "![図2　逆伝播における勾配計算のコードをNumPy化（線形代数化）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/04-linear-algebra/images/02.png)\n",
        "![図3　最適化におけるパラメーター更新のコードをNumPy化（線形代数化）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/04-linear-algebra/images/03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjtsPfbhwyOY"
      },
      "source": [
        "## ■NumPyのインポート"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "今回はNumPyを利用するため、`numpy`モジュールをインポートします。"
      ],
      "metadata": {
        "id": "7xdka8_jxWgI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez7B-8kGNvgI"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT1m3Bseyytq"
      },
      "source": [
        "## ■訓練（学習）処理全体の実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32U5YXJyQU08"
      },
      "source": [
        "深層「学習」のメインは、ニューラルネットの「訓練」処理ですよね。ここから書いていきます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GJPJd3EjnLc"
      },
      "outputs": [],
      "source": [
        "# 取りあえず仮で、空の関数を定義して、コードが実行できるようにしておく\n",
        "def forward_prop(cache_mode=False):\n",
        "    \" 順伝播を行う関数。\"\n",
        "    return None, None, None\n",
        "\n",
        "y_true = np.array([1.0])  # 正解値\n",
        "def back_prop(y_true, cached_outs, cached_sums):\n",
        "    \" 逆伝播を行う関数。\"\n",
        "    return None, None\n",
        "\n",
        "LEARNING_RATE = 0.1 # 学習率（lr）\n",
        "def update_params(grads_w, grads_b, lr=0.1):\n",
        "    \" パラメーター（重みとバイアス）を更新する関数。\"\n",
        "    return None, None\n",
        "\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "# 訓練処理\n",
        "y_pred, cached_outs, cached_sums = forward_prop(cache_mode=True)  # （1）\n",
        "grads_w, grads_b = back_prop(y_true, cached_outs, cached_sums)  # （2）\n",
        "weights, biases = update_params(grads_w, grads_b, LEARNING_RATE)  # （3）\n",
        "\n",
        "print(f'予測値：{y_pred}')  # 予測値： None\n",
        "print(f'正解値：{y_true}')  # 正解値：[1.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtBTnQCjQuhj"
      },
      "source": [
        "ニューラルネットの訓練に必要なことは、以下の3つだけ。\n",
        "\n",
        "1. **順伝播：** `forward_prop()◎`数として実装\n",
        "2. **逆伝播：** `back_prop()`関数として実装。損失（予測と正解の誤差）の計算はここで行う\n",
        "3. **パラメーター（重みとバイアス）の更新：** `update_params()`関数として実装。これによりモデルが**最適化**される\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv5BZKpHPXIk"
      },
      "source": [
        "![図3　訓練（学習）処理を示したニューラルネットワーク図](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/01-forward-prop/images/03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL1t6_R_5vX1"
      },
      "source": [
        "##■モデルの定義と、仮の訓練データ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ76R4MbRKmc"
      },
      "source": [
        "入力層のノードが2個、隠れ層のノードが3個、出力層のノードが1個のモデル（`model`変数）を定義してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sW0f2uH50eJ"
      },
      "outputs": [],
      "source": [
        "# ニューラルネットワークは3層構成\n",
        "layers = [\n",
        "    2,  # 入力層の入力（特徴量）の数\n",
        "    3,  # 隠れ層1のノード（ニューロン）の数\n",
        "    1]  # 出力層のノードの数\n",
        "\n",
        "# 重みとバイアスの初期値\n",
        "weights = [\n",
        "    np.array([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]), # 入力層→隠れ層1\n",
        "    np.array([[0.0, 0.0, 0.0]]) # 隠れ層1→出力層\n",
        "]\n",
        "biases = [\n",
        "    np.array([0.0, 0.0, 0.0]),  # 隠れ層1\n",
        "    np.array([0.0])  # 出力層\n",
        "]\n",
        "\n",
        "# モデルを定義\n",
        "model = (layers, weights, biases)\n",
        "\n",
        "# 仮の訓練データ（1件分）を準備\n",
        "x = np.array([0.05, 0.1])  # x_1とx_2の2つの特徴量"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■おまけ：重みやバイアスの初期値をNumPyで自動生成する方法"
      ],
      "metadata": {
        "id": "P7ed3gC24wYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ニューラルネットワークは3層構成\n",
        "layers = [\n",
        "    2,  # 入力層の入力（特徴量）の数\n",
        "    3,  # 隠れ層1のノード（ニューロン）の数\n",
        "    1]  # 出力層のノードの数\n",
        "\n",
        "def init_parameters(layers):\n",
        "    weights = []\n",
        "    biases = []\n",
        "\n",
        "    n_prev_nodes = 1\n",
        "    for layer_i, n_curr_nodes in enumerate(layers):\n",
        "        if layer_i == 0:\n",
        "            n_prev_nodes = n_curr_nodes\n",
        "            continue\n",
        "\n",
        "        w = np.zeros((n_curr_nodes, n_prev_nodes))\n",
        "        b = np.zeros(n_curr_nodes)\n",
        "\n",
        "        weights.append(w)\n",
        "        biases.append(b)\n",
        "\n",
        "        n_prev_nodes = n_curr_nodes\n",
        "\n",
        "    return (weights, biases)\n",
        "\n",
        "weights, biases = init_parameters(layers)\n",
        "print(f'weights={weights}'.replace('\\n      ', ''))\n",
        "print(f'biases={biases}')\n",
        "# weights=[array([[0., 0.], [0., 0.], [0., 0.]]), array([[0., 0., 0.]])]\n",
        "# biases=[array([0., 0., 0.]), array([0.])]"
      ],
      "metadata": {
        "id": "o2eHdd9S5Fil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm1tlcIiQOcB"
      },
      "source": [
        "## ■ステップ1. 順伝播の実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nxXuQ5nHW58"
      },
      "source": [
        "### ●1つの層における順伝播の処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5qxZXE2RVQs"
      },
      "source": [
        "「1つの層」内にある「全ノード」における順伝播の処理をまとめてコーディングします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4-XMugzbUGM"
      },
      "outputs": [],
      "source": [
        "# 取りあえず仮で、空の関数を定義して、コードが実行できるようにしておく\n",
        "def summation(x, W, b):\n",
        "    \" 重み付き線形和の関数。\"\n",
        "    return np.array([0.0])\n",
        "\n",
        "def sigmoid(x):\n",
        "    \" シグモイド関数。\"\n",
        "    return x\n",
        "\n",
        "def identity(x):\n",
        "    \" 恒等関数。\"\n",
        "    return x\n",
        "\n",
        "\n",
        "W = np.array([[0.0, 0.0]])  # 重み（仮の値）\n",
        "b = np.array([0.0])  # バイアス（仮の値）\n",
        "\n",
        "next_x = x  # 訓練データをノードへの入力に使う\n",
        "\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "# 1つの層内にある全ノードの処理（1）： 重み付き線形和 u=Σx_i*w_i+b\n",
        "sums = summation(next_x, W, b)\n",
        "\n",
        "# 1つの層内にある全ノードの処理（2）： 活性化関数 z=f(u)\n",
        "is_hidden_layer = True\n",
        "if is_hidden_layer:\n",
        "    # 隠れ層（シグモイド関数）\n",
        "    outs = sigmoid(sums)\n",
        "else:\n",
        "    # 出力層（恒等関数）\n",
        "    outs = identity(sums)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPBzLn3hW0zU"
      },
      "source": [
        "1つのノードの順伝播処理に必要なことは、以下の2つの数学関数だけ。\n",
        "\n",
        "1. **重み付き線形和の関数：** `summation()`関数として実装。$u=\\sum_{i=1}^{n} x_i w_i + b$\n",
        "2. **活性化関数：** ここでは`sigmoid()`関数や`identity()`関数として実装。$z=f(u)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7DKfcYyPf1U"
      },
      "source": [
        "![図4　1つのニューロンにおける順伝播の処理を示した図](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/04-linear-algebra/images/04.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "行列を使うことで、1つの層内にある全てのノード分の値を並列的にまとめて計算し、その結果を`sums`と`outs`という変数にまとめて格納しています。"
      ],
      "metadata": {
        "id": "vubk6ODv5UCk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCQTEdpGcv7T"
      },
      "source": [
        "### ●重み付き線形和"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX8bRXh2Riek"
      },
      "source": [
        "重み付き線形和（weighted linear summation、以下では「線形和」と表記）とは、あるノードへの複数の入力（$x_1$、$x_2$など）に、それぞれの重み（$w_11$、$w_21$など）を掛けて足し合わせて、最後にバイアス（$b_1$）を足した値です（上の図の左）。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "今回の実装における、入力／重み／バイアスの行列内容について確認しておきます。\n",
        "\n",
        "- __入力：__ $\\boldsymbol{x}=\\left(\n",
        "\\begin{array}{c}\n",
        "x_1 \\\\\n",
        "x_2 \\\\\n",
        "\\vdots \\\\\n",
        "x_m\n",
        "\\end{array}\n",
        "\\right)$ のようなベクトル\n",
        "\n",
        "- __重み：__ $\\boldsymbol{W}=\\left(\n",
        "\\begin{array}{cccc}\n",
        "w_{11} & w_{21} & \\ldots & w_{m1} \\\\\n",
        "w_{12} & w_{22} & \\ldots & w_{m2} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "w_{1n} & w_{2n} & \\ldots & w_{mn}\n",
        "\\end{array}\n",
        "\\right)$ のような行列\n",
        "\n",
        "- __バイアス：__ $\\boldsymbol{b}=\\left(\n",
        "\\begin{array}{c}\n",
        "b_1  \\\\\n",
        "b_2  \\\\\n",
        "\\vdots  \\\\\n",
        "b_n\n",
        "\\end{array}\n",
        "\\right)$ のようなベクトル\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CX4eqePBIsPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">※</font>一般的な行列の定義は、以下のようになりますが、上の$\\boldsymbol{W}$はこれを転置した形になっている点に注意してください。前の層を基準に重みを並べると下のようになりますが（＝前の層のノード$1$～$m$の$m$行が縦に並ぶ）、今の層を基準に重みを並べると上のようになります（＝今の層のノード$1$～$n$の$n$行が縦に並ぶ）。\n",
        "$$\n",
        "\\left(\n",
        "\\begin{array}{cccc}\n",
        "w_{11} & w_{12} & \\ldots & w_{1n} \\\\\n",
        "w_{21} & w_{22} & \\ldots & w_{2n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "w_{m1} & w_{m2} & \\ldots & w_{mn}\n",
        "\\end{array}\n",
        "\\right)=\\boldsymbol{W}^{\\top}\n",
        "$$"
      ],
      "metadata": {
        "id": "1r7MmAfjLT71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ベクトルである$\\boldsymbol{x}$（入力）と$\\boldsymbol{b}$（バイアス）、行列である$\\boldsymbol{W}$（重み）の3つの変数を使って上の図のような重み付き線形和の計算式を成立させるには次のような計算式を組み立てればよいです。これが冒頭の図1に掲載した数式です。\n",
        "$$\n",
        "\\begin{align}\n",
        "\\boldsymbol{u}&=\\boldsymbol{W}\\boldsymbol{x}+\\boldsymbol{b} \\\\\n",
        "&=\\left(\n",
        "\\begin{array}{cccc}\n",
        "w_{11} & w_{21} & \\ldots & w_{m1} \\\\\n",
        "w_{12} & w_{22} & \\ldots & w_{m2} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "w_{1n} & w_{2n} & \\ldots & w_{mn}\n",
        "\\end{array}\n",
        "\\right)\\left(\n",
        "\\begin{array}{c}\n",
        "x_{1} \\\\\n",
        "x_{2} \\\\\n",
        "\\vdots \\\\\n",
        "x_{m}\n",
        "\\end{array}\n",
        "\\right)+\\left(\n",
        "\\begin{array}{c}\n",
        "b_{1} \\\\\n",
        "b_{2} \\\\\n",
        "\\vdots \\\\\n",
        "b_{n}\n",
        "\\end{array}\n",
        "\\right) \\\\\n",
        "&=\\left(\n",
        "\\begin{array}{cccc}\n",
        "x_1 w_{11}+x_2 w_{21}+\\ldots +x_m w_{m1}+b_1 \\\\\n",
        "x_1 w_{12}+x_2 w_{22} + \\ldots +x_m w_{m2}+b_2 \\\\\n",
        "\\vdots\\\\\n",
        "x_1 w_{1n}+x_2 w_{2n} + \\ldots +x_m w_{mn}+b_n\n",
        "\\end{array}\n",
        "\\right)\n",
        "\\end{align}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "aISZzPHKQI-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuXPPnolcHHW"
      },
      "outputs": [],
      "source": [
        "def summation(x, W, b):\n",
        "    \"\"\"\n",
        "    重み付き線形和の関数。\n",
        "    ※1データ分×全ノード数分を処理する前提。\n",
        "    - 引数：\n",
        "    x： 入力データを一次元配列値（各要素はfloat値）で指定する。\n",
        "    W： 重みを二次元配列値（各要素はfloat値）で指定する。\n",
        "    b： バイアスを一次元配列値（各要素はfloat値）で指定する。\n",
        "    - 戻り値：\n",
        "    線形和の計算結果を一次元配列値（各要素はfloat値）で返す。\n",
        "    \"\"\"\n",
        "    linear_sums = np.dot(W, x) + b\n",
        "    # linear_sums = np.dot(x, W.T) + b  # こう書いてもOK\n",
        "    return linear_sums"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "コメントアウトした行にあるように`np.dot(x, W.T) + b`と書いた場合は、以下の計算式になり、結果は同じです。\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\boldsymbol{u}&=\\boldsymbol{x}\\boldsymbol{W}^{\\top}+\\boldsymbol{b} \\\\\n",
        "&=\\left(\n",
        "\\begin{array}{cccc}\n",
        "x_{1}, & x_{2}, & \\cdots, & x_{m}\n",
        "\\end{array}\n",
        "\\right)\\left(\n",
        "\\begin{array}{cccc}\n",
        "w_{11} & w_{12} & \\ldots & w_{1n} \\\\\n",
        "w_{21} & w_{22} & \\ldots & w_{2n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "w_{m1} & w_{m2} & \\ldots & w_{mn}\n",
        "\\end{array}\n",
        "\\right)+\\left(\n",
        "\\begin{array}{c}\n",
        "b_{1} \\\\\n",
        "b_{2} \\\\\n",
        "\\vdots \\\\\n",
        "b_{n}\n",
        "\\end{array}\n",
        "\\right) \\\\\n",
        "&=\\left(\n",
        "\\begin{array}{cccc}\n",
        "x_1 w_{11}+x_2 w_{21}+\\ldots +x_m w_{m1}+b_1 \\\\\n",
        "x_1 w_{12}+x_2 w_{22} + \\ldots +x_m w_{m2}+b_2 \\\\\n",
        "\\vdots\\\\\n",
        "x_1 w_{1n}+x_2 w_{2n} + \\ldots +x_m w_{mn}+b_n\n",
        "\\end{array}\n",
        "\\right)\n",
        "\\end{align}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "OIh7AbO_jxVL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxKE5sNZSFq0"
      },
      "source": [
        "同様の要領で、他に定義する関数も行列／ベクトル対応にしていきましょう。次に、次回の逆伝播（の中で使う偏微分）で必要となる線形和（linear **sum**mation）の偏導関数（partial **der**ivative function）を実装しておきます。重み付き線形和の偏導関数にもベクトル（$\\boldsymbol{x}$／$\\boldsymbol{b}$）や行列（$\\boldsymbol{W}$）を指定できるようにします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob-1saVLdtre"
      },
      "outputs": [],
      "source": [
        "def sum_der(x, W, b, with_respect_to='W'):\n",
        "    \"\"\"\n",
        "    重み付き線形和の関数の偏導関数。\n",
        "    ※1データ分×全ノード数分を処理する前提。\n",
        "    - 引数：\n",
        "    x： 入力データを一次元配列値で指定する。\n",
        "    W： 重みを二次元配列値で指定する。\n",
        "    b： バイアスを一次元配列値で指定する。\n",
        "    with_respect_to: 何に関して偏微分するかを指定する。\n",
        "       'W'＝ 重み、'b'＝ バイアス、'x'＝ 入力。\n",
        "    - 戻り値：\n",
        "    with_respect_toが、\n",
        "        'W'の場合は二次元配列値（行ベクトル）で、\n",
        "        'b'の場合は一次元配列値（ベクトル）で、\n",
        "        'x'の場合は二次元配列値（行列）で、\n",
        "        線形和の偏微分の計算結果（偏微分係数）を返す。\n",
        "    \"\"\"    \n",
        "    if with_respect_to == 'W':\n",
        "        return x.reshape(1, len(x))  # 線形和uを各重みw_ijで偏微分するとx_iになる（iはノード番号）\n",
        "    elif with_respect_to == 'b':\n",
        "        return np.ones(len(b))  # 線形和uをバイアスb_jで偏微分すると1になる\n",
        "    elif with_respect_to == 'x':\n",
        "        return W  # 線形和uを各入力x_iで偏微分するとw_ijになる\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "理論的にバイアス（$\\boldsymbol{b}$）は、前の層には関係がなく「今の層のノード数（$j=1,2,\\cdots,n$の$n$個）」だけ計算すればよいので、計算結果は$n$個の要素を持つベクトル（一次元配列）になります。\n",
        "\n",
        "一方で、重み（$\\boldsymbol{W}$）や入力（$\\boldsymbol{x}$）は、「今の層のノード数（$n$個）」×「前の層のノード数（$i=1,2,\\cdots,m$の$m$個）」を計算するので、計算結果は$n$行$m$列の行列（二次元配列）になります。\n",
        "\n",
        "しかし上のコードでは、重み（$\\boldsymbol{W}$）が$n$行$m$列の行列ではなく、$1$行$m$列の行ベクトルになっている点に注意してください。これは実際には$j$行$m$列を意図しており、今の層の何ノード目（$j=1,2,\\cdots,n$）であっても、計算結果が同じ値となるので、しかも行ベクトルの方が逆伝播のNumPyによる計算がしやすかったので、$1$行に要約しました。"
      ],
      "metadata": {
        "id": "HT0y18UqxYRk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09TqjNnXidxP"
      },
      "source": [
        "### ●活性化関数：シグモイド関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeCjZU9STJOE"
      },
      "source": [
        "隠れ層では、最も基礎的なシグモイド関数（Sigmoid function）を固定的に使うことにします。導関数も実装しておきます。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "f(x) = \\frac{1}{1+e^{-x}}\n",
        "$$"
      ],
      "metadata": {
        "id": "HNU4vhzU528t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAXhKRDyif96"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    シグモイド関数。\n",
        "    - 引数：\n",
        "    x： 入力データを一次元配列値で指定する。\n",
        "    - 戻り値：\n",
        "    シグモイド関数の計算結果を一次元配列値で返す。\n",
        "    \"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        " f'(x)=f(x)(1-f(x))\n",
        "$$"
      ],
      "metadata": {
        "id": "Ty1dyxXn558M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPghF0t5iiMZ"
      },
      "outputs": [],
      "source": [
        "def sigmoid_der(x):\n",
        "    \"\"\"\n",
        "    シグモイド関数の（偏）導関数。\n",
        "    - 引数：\n",
        "    x： 入力データを一次元配列値で指定する。\n",
        "    - 戻り値：\n",
        "    シグモイド関数の（偏）微分の計算結果（微分係数）を一次元配列値で返す。\n",
        "    \"\"\"\n",
        "    output = sigmoid(x)\n",
        "    return output * (1.0 - output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNH6Mq72jsTB"
      },
      "source": [
        "### ●活性化関数：恒等関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPevusfVTaz7"
      },
      "source": [
        "出力層では、回帰問題をイメージして、そのままの値を出力する活性化関数である恒等関数（Identity function）を使用します。導関数も実装しておきます。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "f(x) = x\n",
        "$$"
      ],
      "metadata": {
        "id": "YEJd0TyM59Aw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oVoxKncjrzy"
      },
      "outputs": [],
      "source": [
        "def identity(x):\n",
        "    \"\"\"\n",
        "    恒等関数の関数。\n",
        "    - 引数：\n",
        "    x： 入力データを一次元配列値で指定する。\n",
        "    - 戻り値：\n",
        "    恒等関数の計算結果（そのまま）を一次元配列値で返す。\n",
        "    \"\"\"\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "f'(x) = 1\n",
        "$$"
      ],
      "metadata": {
        "id": "mMZXi7Pa5_C7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_4avMTJjvog"
      },
      "outputs": [],
      "source": [
        "def identity_der(x):\n",
        "    \"\"\"\n",
        "    恒等関数の（偏）導関数。\n",
        "    - 引数：\n",
        "    x： 入力データを一次元配列値で指定する。\n",
        "    - 戻り値：\n",
        "    恒等関数の（偏）微分の計算結果（微分係数）を一次元配列値で返す。\n",
        "    \"\"\"\n",
        "    return np.ones(len(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2nmNTn0OmPh"
      },
      "source": [
        "### ●順伝播の処理全体の実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jmw6snqTsJl"
      },
      "source": [
        "ニューラルネットには、層があり、その中に複数のノードが存在するという構造です。従って、\n",
        "\n",
        "- 各層を1つずつ処理する`for`ループと、  \n",
        "  - 層の中の全ノードをまとめて処理する行列計算、の2段階構造が必要で、\n",
        "    - 行列計算を使った「順伝播の処理」\n",
        "\n",
        "を記述すればよいわけです。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlfyIL3VkGXZ"
      },
      "outputs": [],
      "source": [
        "def forward_prop(layers, weights, biases, x, cache_mode=False):\n",
        "    \"\"\"\n",
        "    順伝播を行う関数。\n",
        "    - 引数：\n",
        "    (layers, weights, biases)： モデルを指定する。\n",
        "    x： 入力データ（一次元配列値）を指定する。\n",
        "    cache_mode： 予測時はFalse、訓練時はTrueにする。これにより戻り値が変わる。\n",
        "    - 戻り値：\n",
        "    cache_modeがFalse時は予測値のみを返す。True時は、予測値だけでなく、\n",
        "        キャッシュに記録済みの線形和（Σ）値と、活性化関数の出力値も返す。\n",
        "    \"\"\"\n",
        "\n",
        "    cached_sums = []  # 記録した全ノードの線形和（Σ）の値\n",
        "    cached_outs = []  # 記録した全ノードの活性化関数の出力値\n",
        "\n",
        "    # まずは、入力層を順伝播する\n",
        "    # print(f'■第1層（入力層）-全て（{len(x)}個）の特徴量：')\n",
        "    # print(f'　●入力データ： ', end='')\n",
        "    cached_outs.append(x)  # 何も処理せずに出力値を記録\n",
        "    # print(f'何もしない＝out({x})')\n",
        "    next_x = x  # 現在の層の出力（x）＝次の層への入力（next_x）\n",
        "\n",
        "    # 次に、隠れ層や出力層を順伝播する\n",
        "    SKIP_INPUT_LAYER = 1\n",
        "    for layer_i, layer in enumerate(layers):  # 各層を処理\n",
        "        if layer_i == 0:\n",
        "            continue  # 入力層は上で処理済み\n",
        "\n",
        "        # 層ごとに全ノードまとめて処理を行う\n",
        "        sums = []  # 現在の層の全ノードの線形和\n",
        "        outs = []  # 現在の層の全ノードの（活性化関数の）出力\n",
        "        # print(f'■第{layer_i+1}層-全ノード：')\n",
        "\n",
        "        # 層ごとに全ノードの重みとバイアスを取得\n",
        "        W = weights[layer_i - SKIP_INPUT_LAYER]\n",
        "        b = biases[layer_i - SKIP_INPUT_LAYER]\n",
        "\n",
        "        # 1つの層内にある全ノードの処理（1）： 重み付き線形和\n",
        "        # print(f'　●重み付き線形和： ', end='')\n",
        "        sums = summation(next_x, W, b)\n",
        "        # print(f'W({W})・x({next_x})＋b({b})＝sum({sums})'.replace('\\n', ''))\n",
        "\n",
        "        # 1つの層内にある全ノードの処理（2）： 活性化関数\n",
        "        if layer_i < len(layers)-1:  # -1は出力層以外の意味\n",
        "            # 隠れ層（シグモイド関数）\n",
        "            # print(f'　●活性化関数（隠れ層はシグモイド関数）： ', end='')\n",
        "            outs = sigmoid(sums)\n",
        "            # print(f'sigmoid({sums})＝out({outs})')\n",
        "        else:\n",
        "            # 出力層（恒等関数）\n",
        "            # print(f'　●活性化関数（出力層は恒等関数）： ', end='')\n",
        "            outs = identity(sums)\n",
        "            # outs = sigmoid(sums)\n",
        "            # print(f'identity({sums})＝out({outs})')\n",
        "\n",
        "        # 各層内の全ノードの線形和と出力を記録\n",
        "        cached_sums.append(sums)\n",
        "        cached_outs.append(outs)\n",
        "        next_x = outs  # 現在の層の出力（outs）＝次の層への入力（next_x）\n",
        "\n",
        "    if cache_mode:\n",
        "        return (cached_outs[-1], cached_outs, cached_sums)\n",
        "\n",
        "    return cached_outs[-1]\n",
        "\n",
        "\n",
        "# 訓練時の（1）順伝播の実行例\n",
        "y_pred, cached_outs, cached_sums = forward_prop(*model, x, cache_mode=True)\n",
        "# ※先ほど作成したモデルと訓練データを引数で受け取るよう改変した\n",
        "\n",
        "print(f'cached_outs={cached_outs}')\n",
        "print(f'cached_sums={cached_sums}')\n",
        "# 出力例：\n",
        "# cached_outs=[array([0.05, 0.1 ]), array([0.5, 0.5, 0.5]), array([0.])]  # 入力層／隠れ層1／出力層\n",
        "# cached_sums=[array([0., 0., 0.]), array([0.])]  # 隠れ層1／出力層（※入力層はない）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEwU0TN8U7ZT"
      },
      "source": [
        "数値が**0.0**ばかりなので、別の計算パターンのコードも入れておきました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f2urrsRWazm"
      },
      "outputs": [],
      "source": [
        "# 記事にはないが、別の計算パターンでもチェックしてみよう\n",
        "x3 = np.array([0.05, 0.1])\n",
        "layers3 = [2, 2, 2]\n",
        "weights3 = [\n",
        "    np.array([[0.15, 0.2], [0.25, 0.3]]),\n",
        "    np.array([[0.4, 0.45], [0.5,0.55]])\n",
        "]\n",
        "biases3 = [\n",
        "    np.array([0.35, 0.35]),\n",
        "    np.array([0.6, 0.6])\n",
        "]\n",
        "model3 = (layers3, weights3, biases3)\n",
        "\n",
        "y_pred3, cached_outs3, cached_sums3 = forward_prop(*model3, x3, cache_mode=True)\n",
        "print(f'y_pred={y_pred3}')\n",
        "print(f'cached_outs={cached_outs3}')\n",
        "print(f'cached_sums={cached_sums3}')\n",
        "# y_pred=[1.10590597 1.2249214 ]\n",
        "# cached_outs=[array([0.05, 0.1 ]), array([0.59326999, 0.59688438]), array([1.10590597, 1.2249214 ])]\n",
        "# cached_sums=[array([0.3775, 0.3925]), array([1.10590597, 1.2249214 ])]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここまでの順伝播のコード中に仕込んでいる`print()`関数（※全てコメントアウトしています）のコメントを解除すると、以下のように途中の計算内容が順番にテキスト出力されます。計算内容の検証用の機能です。\n",
        "\n",
        "![順伝播：別の計算パターンの出力例](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/04-linear-algebra/images/notebook-01.png)"
      ],
      "metadata": {
        "id": "kgzg2fNwGnNF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y2y79vNGoUD"
      },
      "source": [
        "### ●順伝播による予測の実行例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym7sZcmzWW7b"
      },
      "source": [
        "非常にシンプルで原始的な実装ですが、このように任意の層数とノード数の全結合のDNN（Deep Neural Network）のアーキテクチャーを定義して、DNNモデルによる予測が行えます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0sgGfN2kOa_"
      },
      "outputs": [],
      "source": [
        "# 異なるDNNアーキテクチャーを定義してみる\n",
        "layers2 = [\n",
        "    2,  # 入力層の入力（特徴量）の数\n",
        "    3,  # 隠れ層1のノード（ニューロン）の数\n",
        "    2,  # 隠れ層2のノード（ニューロン）の数\n",
        "    1]  # 出力層のノードの数\n",
        "\n",
        "# 重みとバイアスの初期値\n",
        "weights2 = [\n",
        "    np.array([[-0.2, 0.4], [-0.4, -0.5], [-0.4, -0.5]]), # 入力層→隠れ層1\n",
        "    np.array([[-0.2, 0.4, 0.9], [-0.4, -0.5, -0.2]]), # 隠れ層1→隠れ層2\n",
        "    np.array([[-0.5, 1.0]]) # 隠れ層2→出力層\n",
        "]\n",
        "biases2 = [\n",
        "    np.array([0.1, -0.1, 0.1]),  # 隠れ層1\n",
        "    np.array([0.2, -0.2]),  # 隠れ層2\n",
        "    np.array([0.3])  # 出力層\n",
        "]\n",
        "\n",
        "# モデルを定義\n",
        "model2 = (layers2, weights2, biases2)\n",
        "\n",
        "# 仮の訓練データ（1件分）を準備\n",
        "x2 = np.array([2.3, 1.5])  # x_1とx_2の2つの特徴量\n",
        "\n",
        "# 予測時の（1）順伝播の実行例\n",
        "y_pred = forward_prop(*model2, x2)\n",
        "print(y_pred)  # [0.38288404]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbO0sBxLVxjj"
      },
      "source": [
        "### ●今後のステップの準備：関数への仮引数の追加"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap_S235RV36g"
      },
      "outputs": [],
      "source": [
        "def back_prop(layers, weights, biases, y_true, cached_outs, cached_sums):\n",
        "    \" 逆伝播を行う関数。\"\n",
        "    return None, None\n",
        "\n",
        "def update_params(layers, weights, biases, grads_w, grads_b, lr=0.1):\n",
        "    \" パラメーター（重みとバイアス）を更新する関数。\"\n",
        "    return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgju9tQge_eS"
      },
      "source": [
        "## ■ステップ2. 逆伝播の実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yERpCp1COZhb"
      },
      "source": [
        "### ●逆伝播の目的と全体像"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKNJrZmiOsb2"
      },
      "source": [
        "逆伝播の目的は、誤差（厳密には予測値に関する損失関数の偏微分係数）などの数値（本ノートブックでは**誤差情報**と呼ぶ）をニューラルネットに逆方向で流すこと（＝逆伝播）によって「**重みとバイアスの勾配を計算すること**」です（下の図）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAs9qpq4OiOH"
      },
      "source": [
        "![図5　「逆伝播の流れ」のイメージ（左：ネットワーク図、右：対応する処理／数学計算）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5muXMKUnPHhE"
      },
      "source": [
        "順伝播（`forward_prop()`関数）では、計算途中に出た計算結果である「予測値（`y_pred`）」や「各ノードでの活性化関数の出力値（`cached_outs`）」と「線形和の値（`cached_sums`）」を返すだけでした。\n",
        "\n",
        "逆伝播（`back_prop()関数`）では、計算途中に出た計算結果である「各ノードへの入力の勾配（＝逆伝播していく誤差情報）」だけでなく、「各重みの勾配（`grads_w`）」「各バイアスの勾配（`grads_b`）」の計算も必要です（下の図）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_CvxtXoO8Q2"
      },
      "source": [
        "![図6　逆伝播では各ノードへの入力／各重み／各バイアスの勾配を計算する](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/06.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywEzTQaDQx5G"
      },
      "source": [
        "逆伝播では、$x_1$／$w_1$／$x_2$／$w_2$／……／$x_n$／$w_n$／$b$という大量の変数に関して、損失関数の偏微分係数（＝勾配）を計算する必要があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU4U3HUOgKNo"
      },
      "source": [
        "### ●損失関数：二乗和誤差"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh5SWGHqG7db"
      },
      "source": [
        "損失関数として、最も基礎的な二乗和誤差（SSE：Sum of Squared Error）を使うことにします。導関数も実装しておきます。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{align}\n",
        "L &= \\frac{1}{2}\\sum_{i=1}^{n}(\\hat{y}_{i}-y_{i})^{2} \\\\\n",
        "&\\color{gray}{= \\frac{1}{2}\\sum_{i番目=1から}^{データ数まで総和}(予測値_{i番目}-正解値_{i番目　})^{2乗}}\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "MayaSqdR6OYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "総和（$\\Sigma$）する処理は関数外で行う仕様のため、リスト13では総和していません。次の導関数（リスト14）も同様の仕様です。"
      ],
      "metadata": {
        "id": "-R4xrIjMBS3q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQ7Uo6wpgKqL"
      },
      "outputs": [],
      "source": [
        "def sseloss(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    二乗和誤差（Sum of Squared Error）の関数。\n",
        "    - 引数：\n",
        "    y_pred： モデルの最終出力値＝予測値（predicted value）を一次元配列値で指定する。\n",
        "    y_true： 目的となる値＝正解値（true/actual value）を一次元配列値で指定する。\n",
        "    - 戻り値：\n",
        "    二乗和誤差の計算結果を一次元配列値で返す。\n",
        "    \"\"\"\n",
        "    return 0.5 * (y_pred - y_true) ** 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial\\hat{y}} &= \\sum_{i=1}^{n}(\\hat{y}_{i}-y_{i}) \\\\\n",
        "&\\color{gray}{= \\sum_{i番目=1から}^{データ数まで総和}{予測値_{i番目}-正解値_{i番目　}}}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "$\\partial$は偏微分を表し、「ラウンドディー」などと呼びます。"
      ],
      "metadata": {
        "id": "w06ojAVS6VX3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-P4gEPUXG-p0"
      },
      "outputs": [],
      "source": [
        "def sseloss_der(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    二乗和誤差（Sum of Squared Error）の偏導関数。\n",
        "    予測値（y_pred）に関して二乗和誤差関数（sseloss()）を偏微分する。\n",
        "    - 引数：\n",
        "    y_pred： モデルの最終出力値＝予測値（predicted value）を一次元配列値で指定する。\n",
        "    y_true： 目的となる値＝正解値（true/actual value）を一次元配列値で指定する。\n",
        "    - 戻り値：\n",
        "    二乗和誤差の偏微分の計算結果（偏微分係数）を一次元配列値で返す。\n",
        "    \"\"\"\n",
        "    return y_pred - y_true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0AihXRHYECg"
      },
      "source": [
        "偏導関数の式`y_pred - y_true`は、予測値と正解値の「誤差（Error、ズレ）」となっています。\n",
        "\n",
        "**誤差逆伝播法**（error backpropagation）とは、この「誤差」の数値（厳密には、予測値に関しての損失関数の偏微分係数）が誤差情報としてニューラルネットを「逆」向きに「伝播」していく過程で、本来の目的である各重みと各バイアスの勾配を求める方法です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36DaQKcEX1xa"
      },
      "source": [
        "![図7　各ノードでの逆伝播の処理はワンパターン](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/07.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW0fBcdUX5qY"
      },
      "source": [
        "### ●1つのノードにおける逆伝播の処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR4sjl1AZILm"
      },
      "source": [
        "基本的なニューラルネットワークでは、線形和関数／活性化関数／損失関数の3つの関数を使います。これら3つの関数の関係をPythonコード的に表現すると、\n",
        "\n",
        "　　Loss(  # 損失関数。数式では$L$と表記  \n",
        "　　　　activation(  # 活性化関数（出力層にある$j$番目のノード）。数式では$z_j$と表記  \n",
        "　　　　　　summation(  # 線形和関数。数式では$u_j$と表記  \n",
        "　　　　　　　　next_x,  # ノードへの入力  \n",
        "　　　　　　　　w,  # 重み  \n",
        "　　　　　　　　b  # バイアス  \n",
        "　　　　　　)  \n",
        "　　　　)  \n",
        "　　)  \n",
        "\n",
        "のような入れ子構造になっています。\n",
        "\n",
        "`損失関数( 活性化関数( 線形和関数( 入力、重み、バイアス ) ) )`という入れ子の関数は数学で**合成関数**と呼ばれます。\n",
        "\n",
        "合成関数を微分するときの公式が**連鎖律**です。連鎖律を使うと、まるでマジックのように各関数の偏微分係数の掛け算だけの式に変化します（下の図）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LmiDrcAXmvf"
      },
      "source": [
        "![図8　連鎖律を使うと各関数の偏微分の掛け算になる（各重みに関して損失関数を偏微分する例）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/04-linear-algebra/images/05.png)\n",
        "\n",
        "※損失関数の数式では予測値、つまり出力層の「活性化関数」の出力値を$\\hat{y}$と表現しましたが、この図では（出力層にある$j$番目のノードにおける）「活性化関数」をコード的に$activation_j()$、数式では$z_j$と表現しています。また、（出力層にある$j$番目のノードにおける）「線形和関数」をコード的に$summation_j()$、数式では$u_j$と表現しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R46Y9A8VDatf"
      },
      "source": [
        "上の図は各重みに関して損失関数を偏微分する例ですが、各バイアスや各入力に関して損失関数を偏微分する際も連鎖律の形はほぼ同じです（下の図）。ただし入力については、前の層のノードごとに、今の層からの全てのエッジから来る各誤差情報（偏微分係数）を合計する必要があるので注意してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7ScKvwnCU3_"
      },
      "source": [
        "![図9　各重み／バイアス／入力に関して損失関数を偏微分する場合の連鎖律の形](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/04-linear-algebra/images/06.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上の図を数式で表現すると次のようになります。\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial w_{ij}} &= \\frac{\\partial L}{\\partial z_j}\\frac{\\partial z_j}{\\partial u_j}\\frac{\\partial u_j}{\\partial w_{ij}}  \\\\\n",
        "\\frac{\\partial L}{\\partial b_{j}} &= \\frac{\\partial L}{\\partial z_j}\\frac{\\partial z_j}{\\partial u_j}\\frac{\\partial u_j}{\\partial b_{j}} \\\\\n",
        "\\frac{\\partial L}{\\partial x_{i}} &=  \\sum_{j=1}^{n}(\\frac{\\partial L}{\\partial z_j}\\frac{\\partial z_j}{\\partial u_j}\\frac{\\partial u_j}{\\partial x_{i}})\n",
        "\\end{align}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "t_gOn5YAUGzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "共通する計算の部分を抽出すると、次のように（今の層にある$j$番目のノードにおける）$\\delta$（デルタ）の数式を定義できますね。\n",
        "\n",
        "$$\n",
        "\\delta_j = \\frac{\\partial L}{\\partial z_j}\\frac{\\partial z_j}{\\partial u_j}\n",
        "$$"
      ],
      "metadata": {
        "id": "7WwPf7d7ZM-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "よって最終的には、よりシンプルに次の式にまとめられます。これらが冒頭の図2に掲載した数式です。\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial w_{ij}} &= \\delta_j\\frac{\\partial u_j}{\\partial w_{ij}}  \\\\\n",
        "\\frac{\\partial L}{\\partial b_{j}} &= \\delta_j\\frac{\\partial u_j}{\\partial b_{j}} \\\\\n",
        "\\frac{\\partial L}{\\partial x_{i}} &=  \\sum_{j=1}^{n}(\\delta_j\\frac{\\partial u_j}{\\partial x_{i}})\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "Ibhg_DmuULp-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfZyaK7PDJTD"
      },
      "source": [
        "ここまでの説明は出力層におけるものですが、他の層でも同様の計算式になるので共通化することが可能です。具体的に各層における各ノードの計算は、\n",
        "\n",
        "　　「逆伝播していく誤差情報」×「活性化関数の偏微分」×「線形和関数の偏微分」\n",
        "\n",
        "という掛け算に共通化できます（下の図）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDyy6F8iCa0V"
      },
      "source": [
        "![図10　各層の各ノードでの計算パターンは共通化できる（出力層や隠れ層で入力の勾配を計算する例）](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/04-linear-algebra/images/07.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN0XAc9qCqKC"
      },
      "source": [
        "出力層から隠れ層まで全て、以下の4工程のワンパターンで実装できます（下の図）。\n",
        "\n",
        "- （1）逆伝播していく誤差情報\n",
        "- （2）活性化関数を偏微分\n",
        "- （3）線形和を重み／バイアス／入力で偏微分\n",
        "- （4）各重み／バイアス／各入力の勾配を計算"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkZkdGGrCgVG"
      },
      "source": [
        "![図11　「逆伝播の流れ」の実装内容](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/11.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "※（1）～（4）は層ごとにまとめた処理として実装していきます。"
      ],
      "metadata": {
        "id": "l7_7wELrfDCx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIUf0sjEBtK1"
      },
      "source": [
        "### ●（1）逆伝播していく誤差情報"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otsQU8OEBpe5"
      },
      "outputs": [],
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "layer_i = 2  # 2：出力層、1：隠れ層1、0：入力層\n",
        "layer_max_i = 2  # 最後の層（＝出力層）のインデックス\n",
        "is_output_layer = (layer_i == layer_max_i)  # 出力層か（True）、隠れ層か（False）\n",
        "\n",
        "# 入力層／隠れ層1／出力層にある各ノードの（活性化関数の）出力値\n",
        "cached_outs = [\n",
        "    np.array([0.05, 0.1]),\n",
        "    np.array([0.5, 0.5, 0.5]),\n",
        "    np.array([0.0])\n",
        "]\n",
        "y_true = np.array([1.0])  # 正解値\n",
        "grads_x = []  # 入力の勾配\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "if is_output_layer:\n",
        "    # 出力層（損失関数の偏微分係数）\n",
        "    y_pred = cached_outs[layer_i]\n",
        "    back_error = sseloss_der(y_pred, y_true)  # 逆伝播していく誤差情報\n",
        "else:\n",
        "    # 隠れ層（次の層への入力の偏微分係数）\n",
        "    back_error = grads_x[-1]  # 最後に追加された入力の勾配\n",
        "\n",
        "# print(f'back_error={back_error}')  # back_error=[-1.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADqZzyY4B3Iz"
      },
      "source": [
        "### ●（2）活性化関数を偏微分"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4iP49xCB4bv"
      },
      "outputs": [],
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "SKIP_INPUT_LAYER = 1  # 入力層を飛ばす\n",
        "cached_sums = [\n",
        "    np.array([0.0, 0.0, 0.0]),  # 隠れ層1\n",
        "    np.array([0.0])  # 出力層（※入力層はない）\n",
        "]\n",
        "layer_sums = cached_sums[layer_max_i - SKIP_INPUT_LAYER]  # 出力層\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "if is_output_layer:\n",
        "    # 出力層（恒等関数の微分）\n",
        "    active_der = identity_der(layer_sums)\n",
        "else:\n",
        "    # 隠れ層（シグモイド関数の微分）\n",
        "    active_der = sigmoid_der(layer_sums)\n",
        "\n",
        "# print(f'active_der={active_der}')  # active_der=[1.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C412ZRuXCAr2"
      },
      "source": [
        "### ●（3）線形和を重み／バイアス／入力で偏微分"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpMIf3ggCCYp"
      },
      "outputs": [],
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "PREV_LAYER = 1  # 前の層を指定するため\n",
        "node_i = 0  # ノード番号\n",
        "\n",
        "# 重みとバイアスの初期値\n",
        "weights = [\n",
        "    np.array([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]), # 入力層→隠れ層1\n",
        "    np.array([[0.0, 0.0, 0.0]]) # 隠れ層1→出力層\n",
        "]\n",
        "biases = [\n",
        "    np.array([0.0, 0.0, 0.0]),  # 隠れ層1\n",
        "    np.array([0.0])  # 出力層\n",
        "]\n",
        "# 入力層／隠れ層1／出力層にある各ノードの（活性化関数の）出力値\n",
        "cached_outs = [\n",
        "    np.array([0.05, 0.1]),\n",
        "    np.array([0.5, 0.5, 0.5]),\n",
        "    np.array([0.0])\n",
        "]\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "W = weights[layer_i - SKIP_INPUT_LAYER]\n",
        "b = biases[layer_i - SKIP_INPUT_LAYER]\n",
        "x = cached_outs[layer_i - PREV_LAYER]  # 前の層の出力（out）＝今の層への入力（x）\n",
        "sum_der_w = sum_der(x, W, b, with_respect_to='W')\n",
        "sum_der_b = sum_der(x, W, b, with_respect_to='b')\n",
        "sum_der_X = sum_der(x, W, b, with_respect_to='x')\n",
        "\n",
        "# print(f'W={W}')  # W=[[0. 0. 0.]]\n",
        "# print(f'b={b}')  # b=[0.]\n",
        "# print(f'x={x}')  # x=[0.5 0.5 0.5]\n",
        "# print(f'sum_der_w={sum_der_w}')  # sum_der_w=[[0.5 0.5 0.5]]\n",
        "# print(f'sum_der_b={sum_der_b}')  # sum_der_b=[1.]\n",
        "# print(f'sum_der_X={sum_der_X}')  # sum_der_X=[[0. 0. 0.]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJl8f75mXfmu"
      },
      "source": [
        "![図12　「逆伝播していく誤差情報」「活性化関数を偏微分」「線形和を偏微分」まで実装完了](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/12.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1JXGgZuCCw8"
      },
      "source": [
        "### ●（4）各重み／バイアス／各入力の勾配を計算"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3pN4JsGYJbW"
      },
      "source": [
        "まずは共通の計算部分であるデルタ（`delta`変数）を計算します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f4r7jVICGVI"
      },
      "outputs": [],
      "source": [
        "delta = back_error * active_der\n",
        "\n",
        "# print(f'back_error({back_error})×active_der({active_der})＝delta({delta})')\n",
        "# 出力例： back_error([-1.])×active_der([1.])＝delta([-1.])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "要素ごとの掛け算（アダマール積：$\\odot$）をしています。数学的に表現すると次のような計算になります。\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathrm{delta} &= \\mathrm{back\\_error} \\odot \\mathrm{active\\_der} \\\\\n",
        "&= (\\frac{\\partial L}{\\partial z_1}, \\frac{\\partial L}{\\partial z_2}, \\cdots, \\frac{\\partial L}{\\partial z_n}) \\odot(\\frac{\\partial z_1}{\\partial u_1}, \\frac{\\partial z_2}{\\partial u_2}, \\cdots, \\frac{\\partial z_n}{\\partial u_n}) \\\\\n",
        "&= (\\frac{\\partial L}{\\partial z_1} \\frac{\\partial z_1}{\\partial u_1}, \\frac{\\partial L}{\\partial z_2} \\frac{\\partial z_2}{\\partial u_2}, \\cdots, \\frac{\\partial L}{\\partial z_n} \\frac{\\partial z_n}{\\partial u_n}) \\\\\n",
        "&= (\\delta_1, \\delta_2, \\cdots, \\delta_n) \\\\\n",
        "&= \\boldsymbol{\\delta}\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "7YixYf_qCe8J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVJNd_B2SfZh"
      },
      "source": [
        "![図13　デルタ（delta）のイメージ](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/02-back-prop/images/13.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbjHhA3oYb_V"
      },
      "source": [
        "次に1つの層内にある全てのバイアスの勾配（`layer_grads_b`変数）を計算します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCWkuz3zdPJj"
      },
      "outputs": [],
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "layer_grads_b = []  # 層ごとの、バイアス勾配のリスト\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "# 1つのノードに対して、バイアスは「1つ」だけ\n",
        "layer_grads_b = delta * sum_der_b\n",
        "\n",
        "# print(f'delta({delta})×sum_der_b({sum_der_b})＝layer_grads_b({layer_grads_b})')\n",
        "# 出力例： delta([-1.])×sum_der_b([1.])＝layer_grads_b([-1.])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "これも要素ごとの掛け算（アダマール積：$\\odot$）です。数学的に表現すると次のような計算になります。\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathrm{layer\\_grads\\_b} &= \\mathrm{delta} \\odot \\mathrm{sum\\_der\\_b} \\\\\n",
        "&= (\\delta_1, \\delta_2, \\cdots, \\delta_n) \\odot (\\frac{\\partial u_1}{\\partial b_1}, \\frac{\\partial u_2}{\\partial b_2}, \\cdots, \\frac{\\partial u_n}{\\partial b_n}) \\\\\n",
        "&= (\\delta_1 \\frac{\\partial u_1}{\\partial b_1}, \\delta_2 \\frac{\\partial u_2}{\\partial b_2}, \\cdots, \\delta_n \\frac{\\partial u_n}{\\partial b_n}) \\\\\n",
        "&= \\left(\\frac{\\partial L}{\\partial b_{1}}, \\frac{\\partial L}{\\partial b_{2}}, \\cdots, \\frac{\\partial L}{\\partial b_{n}}\\right) \\\\\n",
        "&= \\frac{\\partial L}{\\partial \\boldsymbol{b}}\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "mlF2720s_FgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "バイアスの勾配の計算結果（一次元配列値）は、各要素に「今の層にあるノード数分（$n$個）の偏微分係数」がノード順に並んでいますね。この並び順は、重み付き線形和の数式で使った$\\boldsymbol{b}$と同じですね。\n",
        "\n"
      ],
      "metadata": {
        "id": "0ZoZ95bKzdb3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r2nJ3wlissE"
      },
      "source": [
        "念のため、各重み／バイアス／入力の偏微分計算の図を再掲しておきます。例えば図の左側の縦中央にある「損失関数を／バイアス $b_j$ で偏微分」の計算結果が、上記の一次元配列値にある1つの要素に対応しています。\n",
        "\n",
        "![図9（再掲）　各重み／バイアス／入力に関して損失関数を偏微分する場合の連鎖律の形](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/04-linear-algebra/images/06.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DK6QGyWic7a"
      },
      "source": [
        "さらに1つの層内にある全ての重みの勾配（`layer_grads_W`変数）と全ての入力の勾配（`layer_grads_x`変数）を計算します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-jAS9PaiNT6"
      },
      "outputs": [],
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "node_count = len(layer_sums)\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "# 1つのノードに対して、重みと入力は「前の層のノードの数」だけある\n",
        "\n",
        "# 重みは「今の層のノード」×「前の層のノード」の行列で取得する\n",
        "layer_grads_W = np.dot(delta.reshape(node_count, 1), sum_der_w)\n",
        "\n",
        "# 入力は「前の層のノード」ごとに「今の層からのエッジ」を全て合計する\n",
        "layer_grads_x = np.dot(delta, sum_der_X)\n",
        "# layer_grads_x = np.dot(sum_der_X.T, delta)  # こう書いてもOK\n",
        "\n",
        "# print(f'delta.T({delta.reshape(node_count, 1)})・sum_der_w({sum_der_w})＝layer_grads_W({layer_grads_W})')\n",
        "# print(f'delta({delta})・sum_der_X({sum_der_X})＝layer_grads_x({layer_grads_x})')\n",
        "# print(f'sum_der_X.T({sum_der_X.T})・delta({delta})＝layer_grads_x({np.dot(sum_der_X.T, delta)})'.replace('\\n', ''))\n",
        "# 出力例：\n",
        "# delta.T([[-1.]])・sum_der_w([[0.5 0.5 0.5]])＝layer_grads_W([[-0.5 -0.5 -0.5]])\n",
        "# delta([-1.])・sum_der_X([[0. 0. 0.]])＝layer_grads_x([0. 0. 0.])\n",
        "# sum_der_X.T([[0.] [0.] [0.]])・delta([-1.])＝layer_grads_x([0. 0. 0.])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "まずは「重みの勾配の計算内容」を見てみます。`reshape(node_count, 1)`は$n$（＝今の層にあるノードの数）行×$1$列の列ベクトルに変換する処理です。\n",
        "\n",
        "`sum_der_w`は$n$行$m$列の行列ではなく、$1$行$m$（＝前の層にあるノードの数）列の行ベクトルになっている点に注意してください。前述の「重み付き線形和」の偏微分のコード部分でも説明しましたが、これは実際には$j$行$m$列を意図しており、今の層の何ノード目（$j=1,2,\\cdots,n$）であっても、計算結果が同じ値となるので、しかも行ベクトルの方が逆伝播のNumPyによる計算がしやすかったので、$1$行に要約しました。\n",
        "\n",
        "重みの勾配の計算は、数学的に表現すると以下のように表現できます。\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathrm{layer\\_grads\\_w} &= \\mathrm{delta}^{\\top} \\cdot \\mathrm{sum\\_der\\_w} \\\\\n",
        "&= \\left(\\begin{array}{c}\\delta_1 \\\\ \\delta_2 \\\\ \\vdots \\\\ \\delta_n \\end{array}\\right) (\\frac{\\partial u_j}{\\partial w_{1j}}, \\frac{\\partial u_j}{\\partial w_{2j}}, \\cdots, \\frac{\\partial u_j}{\\partial w_{mj}}) \\\\\n",
        "&= \\left(\n",
        "\\begin{array}{cccc}\n",
        "\\delta_1 \\frac{\\partial u_j}{\\partial w_{1j}} & \\delta_1 \\frac{\\partial u_j}{\\partial w_{2j}} & \\ldots & \\delta_1 \\frac{\\partial u_j}{\\partial w_{mj}} \\\\\n",
        "\\delta_2 \\frac{\\partial u_j}{\\partial w_{1j}} & \\delta_2 \\frac{\\partial u_j}{\\partial w_{2j}} & \\ldots & \\delta_2 \\frac{\\partial u_j}{\\partial w_{mj}} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\delta_n \\frac{\\partial u_j}{\\partial w_{1j}} & \\delta_n \\frac{\\partial u_j}{\\partial w_{2j}} & \\ldots & \\delta_n \\frac{\\partial u_j}{\\partial w_{mj}}\n",
        "\\end{array}\n",
        "\\right) \\\\\n",
        "※jは今の層に&あるノード番号（1～n）を意味し、\\\\\n",
        "　このn個が行&列の1行目～n行目まで並んでいます。\\\\\n",
        "　このため例え&ば\\frac{\\partial u_j}{\\partial w_{1j}}=\\frac{\\partial u_1} {\\partial w_{11}}～\\frac{\\partial u_n}{\\partial w_{1n}}は全て同じ値です。 \\\\\n",
        "　よって以下の&ように書くこともできます。 \\\\\n",
        "&= \\left(\n",
        "\\begin{array}{cccc}\n",
        "\\delta_1 \\frac{\\partial u_1}{\\partial w_{11}} & \\delta_1 \\frac{\\partial u_1}{\\partial w_{21}} & \\ldots & \\delta_1 \\frac{\\partial u_1}{\\partial w_{m1}} \\\\\n",
        "\\delta_2 \\frac{\\partial u_2}{\\partial w_{12}} & \\delta_2 \\frac{\\partial u_2}{\\partial w_{22}} & \\ldots & \\delta_2 \\frac{\\partial u_2}{\\partial w_{m2}} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\delta_n \\frac{\\partial u_n}{\\partial w_{1n}} & \\delta_n \\frac{\\partial u_n}{\\partial w_{2n}} & \\ldots & \\delta_n \\frac{\\partial u_n}{\\partial w_{mn}}\n",
        "\\end{array}\n",
        "\\right) \\\\\n",
        "&= \\left(\n",
        "\\begin{array}{cccc}\n",
        "\\frac{\\partial L}{\\partial w_{11}} & \\frac{\\partial L}{\\partial w_{21}} & \\ldots & \\frac{\\partial L}{\\partial w_{m1}} \\\\\n",
        "\\frac{\\partial L}{\\partial w_{12}} & \\frac{\\partial L}{\\partial w_{22}} & \\ldots & \\frac{\\partial L}{\\partial w_{m2}} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{\\partial L}{\\partial w_{1n}} & \\frac{\\partial L}{\\partial w_{2n}} & \\ldots & \\frac{\\partial L}{\\partial w_{mn}}\n",
        "\\end{array}\n",
        "\\right) \\\\\n",
        "&= \\frac{\\partial L}{\\partial \\boldsymbol{W}}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "重みの勾配の計算結果（二次元配列値）は、各行に「今の層にあるノード数分（$n$個）の偏微分係数」がノード順に並び、各列に「前の層にあるノード数分（$m$個）の偏微分係数」がノード順に並びます。この並び順は、重み付き線形和の数式で使った$\\boldsymbol{W}$と同じですね。"
      ],
      "metadata": {
        "id": "jJuqDMskwzYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に「入力の勾配の計算」を見てみます。\n",
        "\n",
        "`sum_der_X`は$n$行$m$列の行列です。\n",
        "\n",
        "入力の勾配の計算は、数学的に表現すると以下のように表現できます。\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathrm{layer\\_grads\\_x} &= \\mathrm{delta} \\cdot \\mathrm{sum\\_der\\_X} \\\\\n",
        "&= (\\delta_1, \\delta_2, \\cdots, \\delta_n) \\left(\n",
        "\\begin{array}{cccc}\n",
        "\\frac{\\partial u_1}{\\partial x_1} & \\frac{\\partial u_1}{\\partial x_2} & \\ldots & \\frac{\\partial u_1}{\\partial x_m} \\\\\n",
        "\\frac{\\partial u_2}{\\partial x_1} & \\frac{\\partial u_2}{\\partial x_2} & \\ldots & \\frac{\\partial u_2}{\\partial x_m} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{\\partial u_n}{\\partial x_1} & \\frac{\\partial u_n}{\\partial x_2} & \\ldots & \\frac{\\partial u_n}{\\partial x_m}\n",
        "\\end{array}\n",
        "\\right) \\\\\n",
        "&= (\\sum_{j=1}^{n} \\delta_j \\frac{\\partial u_j}{\\partial x_1}, \\sum_{j=1}^{n} \\delta_j \\frac{\\partial u_j}{\\partial x_2}, \\cdots, \\sum_{j=1}^{n} \\delta_j \\frac{\\partial u_j}{\\partial x_m}) \\\\\n",
        "&= (\\frac{\\partial L}{\\partial x_1}, \\frac{\\partial L}{\\partial x_2}, \\cdots, \\frac{\\partial L}{\\partial x_m}) \\\\\n",
        "&= \\frac{\\partial L}{\\partial \\boldsymbol{x}}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "入力の勾配の計算結果（一次元配列値）は、各要素に__「前」__の層の出力（＝今の層の入力）にある$m$個の偏微分係数がノード順に並びます。この並び順は、重み付き線形和の数式で使った$\\boldsymbol{x}$と同じですね。\n",
        "\n",
        "注意点として、「前の層の出力」ごとに「今の層からのエッジ」（$n$個）の計算結果を全て合計する必要があります。（上の数式にもある）行列計算の中で総和（$\\Sigma$）する計算部分がこの合計処理に該当します。"
      ],
      "metadata": {
        "id": "hFlcg2IyDAWi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJNU9W3qCGrC"
      },
      "source": [
        "### ●逆伝播の処理全体の実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCp5GqVfSHKQ"
      },
      "source": [
        "ニューラルネットには、層があり、その中に複数のノードが存在するという構造です。  従って、\n",
        "\n",
        "- **逆順に**各層を1つずつ処理する`for`ループと、  \n",
        "  - 層の中の全ノードをまとめて処理する行列計算、の2段階構造が必要で、\n",
        "    - 行列計算を使った「逆伝播の処理」\n",
        "\n",
        "を記述すればよいわけです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0g89nnACHxS"
      },
      "outputs": [],
      "source": [
        "def back_prop(layers, weights, biases, y_true, cached_outs, cached_sums):\n",
        "    \"\"\"\n",
        "    逆伝播を行う関数。\n",
        "    - 引数：\n",
        "    (layers, weights, biases)： モデルを指定する。\n",
        "    y_true： 正解値（出力層のノードが複数ある場合もあるのでリスト値）。\n",
        "    cached_outs： 順伝播で記録した活性化関数の出力値。予測値を含む。\n",
        "    cached_sums： 順伝播で記録した線形和（Σ）値。\n",
        "    - 戻り値：\n",
        "    重みの勾配とバイアスの勾配を返す。\n",
        "    \"\"\"\n",
        "\n",
        "    # ネットワーク全体で勾配を保持するためのリスト\n",
        "    grads_w = []  # 重みの勾配\n",
        "    grads_b = []  # バイアスの勾配\n",
        "    grads_x = []  # 入力の勾配\n",
        "\n",
        "    layer_count = len(layers)\n",
        "    layer_max_i = layer_count-1\n",
        "    SKIP_INPUT_LAYER = 1\n",
        "    PREV_LAYER = 1\n",
        "    rng = range(SKIP_INPUT_LAYER, layer_count)  # 入力層以外の層インデックス\n",
        "    for layer_i in reversed(rng):  # 各層を逆順に処理\n",
        "\n",
        "        layer_sums = cached_sums[layer_i - SKIP_INPUT_LAYER]\n",
        "        node_count = len(layer_sums)\n",
        "        is_output_layer = (layer_i == layer_max_i)\n",
        "\n",
        "        # 層ごとに全ノードまとめて処理を行う\n",
        "        # print(f'■第{layer_i+1}層-全て（{node_count}個）のノード：')\n",
        "\n",
        "        # （1）逆伝播していく誤差情報\n",
        "        # print(f'　●（1）逆伝播していく誤差情報', end='')\n",
        "        if is_output_layer:\n",
        "            # 出力層（損失関数の偏微分係数）\n",
        "            y_pred = cached_outs[layer_i]\n",
        "            back_error = sseloss_der(y_pred, y_true)  # 逆伝播していく誤差情報\n",
        "            # print(f'（出力層は損失関数：二乗和誤差）の偏微分係数＝{back_error})')\n",
        "        else:\n",
        "            # 隠れ層（次の層への入力の偏微分係数）\n",
        "            back_error = grads_x[-1]  # 最後に追加された入力の勾配\n",
        "            # print(f'（隠れ層は次の層への入力）の偏微分係数＝{back_error})')\n",
        "\n",
        "        # （2）活性化関数を偏微分\n",
        "        # print(f'　●（2）活性化関数', end='')\n",
        "        if is_output_layer:\n",
        "            # 出力層（恒等関数の微分）\n",
        "            active_der = identity_der(layer_sums)\n",
        "            # print(f'（出力層は恒等関数）({layer_sums})の偏微分＝{active_der}')\n",
        "        else:\n",
        "            # 隠れ層（シグモイド関数の微分）\n",
        "            active_der = sigmoid_der(layer_sums)\n",
        "            # print(f'sigmoid_der({layer_sums})＝layer_sums({active_der})')\n",
        "            # print(f'（隠れ層はシグモイド関数）({layer_sums})の偏微分＝{active_der}')\n",
        "\n",
        "        # （3）線形和を重み／バイアス／入力で偏微分\n",
        "        # print(f'　●（3）線形和関数の偏微分：')\n",
        "        W = weights[layer_i - SKIP_INPUT_LAYER]\n",
        "        b = biases[layer_i - SKIP_INPUT_LAYER]\n",
        "        x = cached_outs[layer_i - PREV_LAYER]  # 前の層の出力＝今の層への入力\n",
        "        sum_der_w = sum_der(x, W, b, with_respect_to='W')\n",
        "        sum_der_b = sum_der(x, W, b, with_respect_to='b')\n",
        "        sum_der_X = sum_der(x, W, b, with_respect_to='x')\n",
        "        # print(f'　　○重み({W})で偏微分＝{sum_der_w}'.replace('\\n', ''))\n",
        "        # print(f'　　○バイアス({b})で偏微分＝{sum_der_b}')\n",
        "        # print(f'　　○入力({x})で偏微分＝{sum_der_X}'.replace('\\n', ''))\n",
        "        \n",
        "        # （4）各重み／バイアス／各入力の勾配を計算\n",
        "        # print(f'　●（4）各重み／バイアス／各入力の勾配を計算： ')\n",
        "        delta = back_error * active_der\n",
        "        # print(f'　　○デルタ： 逆伝播していく誤差情報({back_error})×活性化関数の偏微分({active_der})＝{delta}'.replace('\\n', ''))\n",
        "\n",
        "        # 1つのノードに対して、バイアスは「1つ」だけ\n",
        "        layer_grads_b = delta * sum_der_b\n",
        "        # print(f'　　○バイアスの勾配： デルタ({delta})×線形和関数をバイアスで偏微分({sum_der_b})＝{layer_grads_b}'.replace('\\n', ''))\n",
        "\n",
        "        # 1つのノードに対して、重みと入力は「前の層のノードの数」だけある\n",
        "\n",
        "        # 重みは「今の層のノード」×「前の層のノード」の行列で取得する\n",
        "        layer_grads_W = np.dot(delta.reshape(node_count, 1), sum_der_w)\n",
        "        # print(f'　　○重みの勾配： デルタの列ベクトル({delta.reshape(node_count, 1)})・線形和関数を重みで偏微分({sum_der_w})＝{layer_grads_W}'.replace('\\n', ''))\n",
        "\n",
        "        # 入力は「前の層のノード」ごとに「今の層からのエッジ」を全て合計する\n",
        "        layer_grads_x = np.dot(delta, sum_der_X)\n",
        "        # print(f'　　○入力の勾配： デルタ({delta})・線形和関数を入力で偏微分({sum_der_X})＝{layer_grads_x}'.replace('\\n', ''))\n",
        "\n",
        "        # 層ごとの勾配を、ネットワーク全体用のリストに格納\n",
        "        grads_w.append(layer_grads_W)\n",
        "        grads_b.append(layer_grads_b)\n",
        "        grads_x.append(layer_grads_x)\n",
        "\n",
        "    # print(f'■第1層-全て（{len(cached_sums[0])}個）の特徴量：')\n",
        "    # print(f'　●（1）逆伝播していく誤差情報： ', end='')\n",
        "    # print(f'【入力層】次の層への入力の偏微分係数＝{grads_x[-1]})')\n",
        "\n",
        "    # 保持しておいた各勾配（※逆順で追加したので反転が必要）を戻り値で返す\n",
        "    grads_w.reverse()\n",
        "    grads_b.reverse()\n",
        "    return (grads_w, grads_b)  # grads_xは最適化で不要なので返していない"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbhBtq6vHFkz"
      },
      "source": [
        "### ●逆伝播の実行例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBuUkZYrBrJD"
      },
      "source": [
        "以下のようなコードを書けば、順伝播から逆伝播までを続けて実行できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5yo0KezBkfZ"
      },
      "outputs": [],
      "source": [
        "x = np.array([0.05, 0.1])\n",
        "layers = [2, 2, 2]\n",
        "weights = [\n",
        "    np.array([[0.15, 0.2], [0.25, 0.3]]),\n",
        "    np.array([[0.4, 0.45], [0.5,0.55]])\n",
        "]\n",
        "biases = [\n",
        "    np.array([0.35, 0.35]),\n",
        "    np.array([0.6, 0.6])\n",
        "]\n",
        "model = (layers, weights, biases)\n",
        "y_true = np.array([0.01, 0.99])\n",
        "\n",
        "# （1）順伝播の実行例\n",
        "y_pred, cached_outs, cached_sums = forward_prop(*model, x, cache_mode=True)\n",
        "print(f'y_pred={y_pred}')\n",
        "print(f'cached_outs={cached_outs}')\n",
        "print(f'cached_sums={cached_sums}')\n",
        "# 出力例：\n",
        "# y_pred=[1.10590597 1.2249214 ]\n",
        "# cached_outs=[array([0.05, 0.1 ]), array([0.59326999, 0.59688438]), array([1.10590597, 1.2249214 ])]\n",
        "# cached_sums=[array([0.3775, 0.3925]), array([1.10590597, 1.2249214 ])]\n",
        "\n",
        "# （2）逆伝播の実行例\n",
        "grads_w, grads_b = back_prop(*model, y_true, cached_outs, cached_sums)\n",
        "print(f'grads_w={grads_w}'.replace('\\n      ', ''))\n",
        "print(f'grads_b={grads_b}'.replace('\\n      ', ''))\n",
        "# 出力例：\n",
        "# grads_w=[array([[0.00670603, 0.01341205], [0.00748746, 0.01497492]]), array([[0.65016812, 0.65412915], [0.13937182, 0.14022092]])]\n",
        "# grads_b=[array([0.13412051, 0.14974924]), array([1.09590597, 0.2349214 ])]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここまでの逆伝播のコード中に仕込んでいる`print()`関数（※全てコメントアウトしています）のコメントを解除すると、以下のように途中の計算内容が順番にテキスト出力されます。計算内容の検証用の機能です。\n",
        "\n",
        "![逆伝播：別の計算パターンの出力例](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/04-linear-algebra/images/notebook-02.png)"
      ],
      "metadata": {
        "id": "waoQSN9-1CX3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a4Bw6fGuLBj"
      },
      "source": [
        "## ■ステップ3. パラメーター（重みとバイアス）更新の実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAERKD0XKjwp"
      },
      "source": [
        "パラメーター（各重みと各バイアス）を更新する目的は、「**ニューラルネットのモデルを最適化すること**」です。下の図は「最適化の**参考イメージ**」です。\n",
        "\n",
        "※なお、本稿で説明するのは最も基礎的な**勾配降下法**（**Gradient Descent**）です。後述するSGD（確率的勾配降下法）もその一種で、他にはRMSPropやAdamなどより応用的な手法があります。SGD以外の場合は、重みパラメーターの更新方法も少し変わってきます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvBQaEMgJtdX"
      },
      "source": [
        "![図14　最適化の参考イメージ](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/03-optimizer/images/14.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSNQtRXxAX9E"
      },
      "source": [
        "### ●1つのパラメーターの更新"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqGWeJuFAr0L"
      },
      "source": [
        "最も基本的なSGD（確率的勾配降下法）の場合、1つの重み／バイアスのパラメーター更新は以下の計算方法で行えます。なお、$\\eta$は「イータ」と読みます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzjUVAkPuvgF"
      },
      "source": [
        "![重みパラメーター更新の計算式](https://raw.githubusercontent.com/isshiki/neural-network-by-code/main/04-linear-algebra/images/08.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "数式で表現すると、以下のようになります。これらが冒頭の図3に掲載した数式です。\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "w_{ij} &\\leftarrow w_{ij}-\\eta\\frac{\\partial L}{\\partial w_\n",
        "{ij}} \\\\\n",
        "b_j &\\leftarrow b_j-\\eta\\frac{\\partial L}{\\partial b_j} \n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "dxU-C97C9vPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ●1つの層内にある全パラメーターの更新"
      ],
      "metadata": {
        "id": "6gxdFUql-4N9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NumPyの二次元配列や一次元配列を使う場合、多次元配列内の各要素をまとめて計算可能です。その場合の数式は次のように表現できるでしょう。\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\boldsymbol{W} &\\leftarrow \\boldsymbol{W}-\\eta\\frac{\\partial L}{\\partial \\boldsymbol{W}} \\\\\n",
        "\\boldsymbol{b} &\\leftarrow \\boldsymbol{b}-\\eta\\frac{\\partial L}{\\partial \\boldsymbol{b}} \n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "ZImfDlDc9eGd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEuYVALGFz5n"
      },
      "outputs": [],
      "source": [
        "# 取りあえず仮で、変数を定義して、コードが実行できるようにしておく\n",
        "W = np.array([[0.0]])  # 重み（行列）\n",
        "b = np.array([0.0])  # バイアス（ベクトル）\n",
        "grad_W = np.array([[0.2]])  # 重みの勾配（行列）\n",
        "grad_b = np.array([0.2])  # バイアスの勾配（ベクトル）\n",
        "LEARNING_RATE = 0.1  # 学習率（lr）\n",
        "lr = LEARNING_RATE\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "W = W - lr * grad_W  # 重みパラメーターの更新\n",
        "\n",
        "b = b - lr * grad_b  # バイアスパラメーターの更新"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIs2wLVD6Ufe"
      },
      "source": [
        "### ●パラメーター更新の処理全体の実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqIVwOxHBAAB"
      },
      "source": [
        "ニューラルネットには、層があり、その中に複数のノードが存在するという構造ですので、\n",
        "\n",
        "- 各層を1つずつ処理する`for`ループと、  \n",
        "  - 層の中の全ノードをまとめて処理する行列計算、の2段階構造が必要で、\n",
        "    - 行列計算を使った「パラメーター更新の処理」\n",
        "\n",
        "を記述すればよいわけです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bCBA7JwvE2H"
      },
      "outputs": [],
      "source": [
        "def update_params(layers, weights, biases, grads_w, grads_b, lr=0.1):\n",
        "    \"\"\"\n",
        "    パラメーター（重みとバイアス）を更新する関数\n",
        "    - 引数：\n",
        "    (layers, weights, biases)： モデルを指定する。\n",
        "    grads_w： 重みの勾配。\n",
        "    grads_b： バイアスの勾配。\n",
        "    lr： 学習率（learning rate）。最適化を進める量を調整する。\n",
        "    - 戻り値：\n",
        "    新しい重みとバイアスを返す。\n",
        "    \"\"\"\n",
        "\n",
        "    # ネットワーク全体で勾配を保持するためのリスト\n",
        "    new_weights = [] # 重み\n",
        "    new_biases = [] # バイアス\n",
        "\n",
        "    SKIP_INPUT_LAYER = 1\n",
        "    for layer_i, layer in enumerate(layers):  # 各層を処理\n",
        "        if layer_i == 0:\n",
        "            continue  # 入力層はスキップ\n",
        "\n",
        "        # 層ごとに全ノードまとめて処理を行う\n",
        "\n",
        "        b = biases[layer_i - SKIP_INPUT_LAYER]\n",
        "        grad_b = grads_b[layer_i - SKIP_INPUT_LAYER]\n",
        "        layer_b = b - lr * grad_b  # バイアスパラメーターの更新\n",
        "\n",
        "        W = weights[layer_i - SKIP_INPUT_LAYER]\n",
        "        grad_W = grads_w[layer_i - SKIP_INPUT_LAYER]\n",
        "        layer_W = W - lr * grad_W  # 重みパラメーターの更新\n",
        "\n",
        "        new_weights.append(layer_W)\n",
        "        new_biases.append(layer_b)\n",
        "    \n",
        "    return (new_weights, new_biases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te0ar682pzvK"
      },
      "source": [
        "### ●パラメーター更新の実行例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiWOhdY7Blbf"
      },
      "source": [
        "以下のようなコードを書けば、順伝播から逆伝播、パラメーター更新までを続けて実行できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ck4l6GqGp2n4"
      },
      "outputs": [],
      "source": [
        "layers = [2, 2, 2]\n",
        "weights = [\n",
        "    np.array([[0.15, 0.2], [0.25, 0.3]]),\n",
        "    np.array([[0.4, 0.45], [0.5,0.55]])\n",
        "]\n",
        "biases = [\n",
        "    np.array([0.35, 0.35]),\n",
        "    np.array([0.6, 0.6])\n",
        "]\n",
        "model = (layers, weights, biases)\n",
        "\n",
        "# 元の重み\n",
        "print(f'old-weights={weights}'.replace('\\n      ', ''))\n",
        "print(f'old-biases={biases}')\n",
        "# old-weights=[array([[0.15, 0.2 ], [0.25, 0.3 ]]), array([[0.4 , 0.45], [0.5 , 0.55]])]\n",
        "# old-biases=[array([0.35, 0.35]), array([0.6, 0.6])]\n",
        "\n",
        "# （1）順伝播の実行例\n",
        "x = np.array([0.05, 0.1])\n",
        "y_pred, cached_outs, cached_sums = forward_prop(*model, x, cache_mode=True)\n",
        "\n",
        "# （2）逆伝播の実行例\n",
        "y_true = np.array([0.01, 0.99])\n",
        "grads_w, grads_b = back_prop(*model, y_true, cached_outs, cached_sums)\n",
        "print(f'grads_w={grads_w}'.replace('\\n      ', ''))\n",
        "print(f'grads_b={grads_b}')\n",
        "# grads_w=[array([[0.00670603, 0.01341205], [0.00748746, 0.01497492]]), array([[0.65016812, 0.65412915], [0.13937182, 0.14022092]])]\n",
        "# grads_b=[array([0.13412051, 0.14974924]), array([1.09590597, 0.2349214 ])]\n",
        "\n",
        "# （3）パラメーター更新の実行例\n",
        "LEARNING_RATE = 0.1 # 学習率（lr）\n",
        "weights, biases = update_params(*model, grads_w, grads_b, lr=LEARNING_RATE)\n",
        "\n",
        "# 更新後の新しい重み\n",
        "print(f'new-weights={weights}'.replace('\\n      ', ''))\n",
        "print(f'new-biases={biases}')\n",
        "# new-weights=[array([[0.1493294 , 0.19865879], [0.24925125, 0.29850251]]), array([[0.33498319, 0.38458708], [0.48606282, 0.53597791]])]\n",
        "# new-biases=[array([0.33658795, 0.33502508]), array([0.4904094 , 0.57650786])]\n",
        "\n",
        "# モデルの最適化\n",
        "model = (layers, weights, biases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuRVFxSewyh9"
      },
      "source": [
        "## ■3つのステップを呼び出す最適化処理の実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLanYX6WCcAL"
      },
      "source": [
        "### ●最適化処理：学習方法と勾配降下法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ2ZpLzMC148"
      },
      "source": [
        "代表的な学習方法を簡単にまとめておきます。\n",
        "\n",
        "- **オンライン学習**（**Online training**）： データ1件ずつ訓練していくこと\n",
        "- **ミニバッチ学習**（**Mini-batch training**）： 小さなまとまりのデータごとに訓練していくこと\n",
        "- **バッチ学習**（**Batch training**）： データ全件で訓練していくこと"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XomOMl8sDC0X"
      },
      "source": [
        "学習方法ごとに、勾配降下法をまとめると以下のようになります。\n",
        "\n",
        "- **SGD**（**Stochastic Gradient Descent**）： オンライン学習\n",
        "- **ミニバッチSGD**（**Mini-batch SGD**）： ミニバッチ学習。単に**ミニバッチ勾配降下法**（**Mini-batch Gradient Descent**）とも呼ぶ\n",
        "- **最急降下法**（**Steepest Descent**）： バッチ学習。**バッチ勾配降下法**（**Batch Gradient Descent**）とも呼ぶ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCAtNGe8DkG8"
      },
      "source": [
        "コード内容も簡単なので少し難易度を上げて、あえて全ての学習方法に対応できる実装コードにしてみます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvS7FT0DtiPE"
      },
      "source": [
        "### ●最適化の処理全体の実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SM0Ss5dDxsN"
      },
      "source": [
        "訓練処理では、エポック（＝全データ分で1回の訓練）があり、その中にイテレーション（＝バッチサイズごとでのパラメーターの更新）が存在するという構造ですので、\n",
        "\n",
        "- エポックを1回ずつ処理する`for`ループと、\n",
        "  - その中にデータを1件ずつ処理する`for`ループの2段階構造を用意し、\n",
        "    - その中に「ステップ1. 順伝播」「ステップ2. 逆伝播」と、\n",
        "    - イテレーションごとに「ステップ3. パラメーターの更新」\n",
        "\n",
        "を記述するようにします（※あくまで筆者による実装方針の例です）。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rya4h8IEE6H"
      },
      "source": [
        "階層が深くなる上にコードの行数が少し長いので、説明の都合上、上の箇条書きの前半2行を`train()`親関数、後半2行を`optimize()`子関数、という親子関係の2つの関数に分けて記述します。※1つの関数として実装した方がシンプルになって見通しもよくなるので、本来であればそうした方がよいと思います。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_01bjboj7ry"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# 取りあえず仮で、空の関数を定義して、コードが実行できるようにしておく\n",
        "def optimize(model, x, y, data_i, last_i, batch_i, batch_size, acm_g, lr=0.1):\n",
        "    \" モデルを最適化する関数（子関数）。\"\n",
        "    loss = 0.1\n",
        "    return model, loss, batch_i, acm_g\n",
        "\n",
        "# ---ここまでは仮の実装。ここからが必要な実装---\n",
        "\n",
        "def train(model, x, y, batch_size=32, epochs=10, lr=0.1, verbose=10):\n",
        "    \"\"\"\n",
        "    モデルの訓練を行う関数（親関数）。\n",
        "    - 引数：\n",
        "    model： モデルをタプル「(layers, weights, biases)」で指定する。\n",
        "    x： 訓練データ（各データが行、各特徴量が列の、2次元リスト値）。\n",
        "    y： 訓練ラベル（各データが行、各正解値が列の、2次元リスト値）。\n",
        "    batch_size： バッチサイズ。何件のデータをまとめて処理するか。\n",
        "    epochs： エポック数。全データ分で何回、訓練するか。\n",
        "    lr： 学習率（learning rate）。最適化を進める量を調整する。\n",
        "    verbose： 訓練状況を何エポックおきに出力するか。\n",
        "    - 戻り値：\n",
        "    損失値の履歴を返す。これを使って損失値の推移グラフが描ける。\n",
        "    \"\"\"\n",
        "    loss_history = []  # 損失値の履歴\n",
        "\n",
        "    data_size = len(y)  # 訓練データ数\n",
        "    data_indexes = range(data_size)  # 訓練データのインデックス\n",
        "\n",
        "    # 各エポックを処理\n",
        "    for epoch_i in range(1, epochs + 1):  # 経過表示用に1スタート\n",
        "\n",
        "        acm_loss = 0  # 損失値を蓄積（accumulate）していく\n",
        "\n",
        "        # 訓練データのインデックスをシャッフル（ランダムサンプリング）\n",
        "        random_indexes = random.sample(data_indexes, data_size)\n",
        "        last_i = random_indexes[-1]  # 最後の訓練データのインデックス\n",
        "\n",
        "        # 親関数で管理すべき変数\n",
        "        acm_g = (None, None)  # 重み／バイアスの勾配を蓄積していくため\n",
        "        batch_i = 0  # バッチ番号をインクリメントしていくため\n",
        "\n",
        "        # 訓練データを1件1件処理していく\n",
        "        for data_i in random_indexes:\n",
        "\n",
        "            # 親子に分割したうちの子関数を呼び出す\n",
        "            model, loss, batch_i, acm_g = optimize(\n",
        "                model, x, y, data_i, last_i, batch_i, batch_size, acm_g, lr)\n",
        "\n",
        "            acm_loss += loss  # 損失値を蓄積\n",
        "\n",
        "        # エポックごとに損失値を計算。今回の実装では「平均」する\n",
        "        layers = model[0]  # レイヤー構造\n",
        "        out_count = layers[-1]  # 出力層のノード数\n",
        "        # 「訓練データ数（イテレーション数×バッチサイズ）×出力ノード数」で平均\n",
        "        epoch_loss = acm_loss / (data_size * out_count)\n",
        "\n",
        "        # 訓練状況を出力\n",
        "        if verbose != 0 and \\\n",
        "            (epoch_i % verbose == 0 or epoch_i == 1 or epoch_i == EPOCHS):\n",
        "            print(f'[Epoch {epoch_i}/{EPOCHS}] train_loss: {epoch_loss}')\n",
        "\n",
        "        loss_history.append(epoch_loss)  # 損失値の履歴として保存\n",
        "\n",
        "    return model, loss_history\n",
        "\n",
        "\n",
        "# サンプル実行用の仮のモデルとデータ\n",
        "layers = [2, 2, 2]\n",
        "weights = [\n",
        "    np.array([[0.15, 0.2], [0.25, 0.3]]),\n",
        "    np.array([[0.4, 0.45], [0.5,0.55]])\n",
        "]\n",
        "biases = [\n",
        "    np.array([0.35, 0.35]),\n",
        "    np.array([0.6, 0.6])\n",
        "]\n",
        "model = (layers, weights, biases)\n",
        "x = np.array([[0.05, 0.1]])\n",
        "y = np.array([[0.01, 0.99]])\n",
        "\n",
        "# モデルを訓練する\n",
        "BATCH_SIZE = 2  # バッチサイズ\n",
        "EPOCHS = 1  # エポック数\n",
        "LEARNING_RATE = 0.02 # 学習率（lr）\n",
        "model, loss_history = train(model, x, y, BATCH_SIZE, EPOCHS, LEARNING_RATE)\n",
        "# 出力例：\n",
        "# [Epoch 1/1] train_loss: 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj3iui6Oy0X7"
      },
      "outputs": [],
      "source": [
        "def accumulate(list1, list2):\n",
        "    \"2つのリストの値を足し算する関数。\"\n",
        "    new_list = []\n",
        "    for item1, item2 in zip(list1, list2):\n",
        "        # ※全体の重み勾配は行数と列数が同じではないので層ごとに処理する必要がある。\n",
        "        np_sum = np.array(item1) + np.array(item2)  # NumPyなら行列データをまとめて処理できる\n",
        "        new_list.append(np_sum)\n",
        "    return new_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QUcfFfxzq00"
      },
      "outputs": [],
      "source": [
        "def mean_element(list1, data_count):\n",
        "    \"1つのリストの値をデータ数で平均する関数。\"\n",
        "    new_list = []\n",
        "    for item1 in list1:\n",
        "        # ※全体の重み勾配は行数と列数が同じではないので層ごとに処理する必要がある。\n",
        "        np_mean = np.array(item1) / data_count  # NumPyなら行列データをまとめて処理できる\n",
        "        new_list.append(np_mean)\n",
        "    return new_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLnd_nyxuVnO"
      },
      "outputs": [],
      "source": [
        "def optimize(model, x, y, data_i, last_i, batch_i, batch_size, acm_g, lr=0.1):\n",
        "    \"train()親関数から呼ばれる、最適化のための子関数。\"\n",
        "\n",
        "    layers = model[0]  # レイヤー構造\n",
        "    each_x = np.array(x[data_i])  # 1件分の訓練データ\n",
        "    y_true = np.array(y[data_i])  # 1件分の正解値\n",
        "\n",
        "    # ステップ（1）順伝播\n",
        "    y_pred, outs, sums = forward_prop(*model, each_x, cache_mode=True)\n",
        "\n",
        "    # ステップ（2）逆伝播\n",
        "    gw, gb = back_prop(*model, y_true, outs, sums)\n",
        "\n",
        "    # 各勾配を蓄積（accumulate）していく\n",
        "    if batch_i == 0:\n",
        "        acm_gw = gw\n",
        "        acm_gb = gb\n",
        "    else:\n",
        "        acm_gw = accumulate(acm_g[0], gw)\n",
        "        acm_gb = accumulate(acm_g[1], gb)\n",
        "    batch_i += 1  # バッチ番号をカウントアップ＝現在のバッチ数\n",
        "\n",
        "    # 訓練状況を評価するために、損失値を取得\n",
        "    loss = 0.0\n",
        "    for output, target in zip(y_pred, y_true):\n",
        "        loss += sseloss(output, target)\n",
        "\n",
        "    # バッチサイズごとで後続の処理に進む\n",
        "    if batch_i % BATCH_SIZE != 0 and data_i != last_i:\n",
        "        return model, loss, batch_i, (acm_gw, acm_gb)  # バッチ内のデータごと\n",
        "\n",
        "    layers = model[0]  # レイヤー構造\n",
        "    out_count = layers[-1]  # 出力層のノード数\n",
        "\n",
        "    # 平均二乗誤差なら平均する（損失関数によって異なる）\n",
        "    grads_w = mean_element(acm_gw, batch_i * out_count)  # 「バッチサイズ ×\n",
        "    grads_b = mean_element(acm_gb, batch_i * out_count)  #   出力ノード数」で平均\n",
        "    batch_i = 0  # バッチ番号を初期化して次のイテレーションに備える\n",
        "\n",
        "    # ステップ（3）パラメーター（重みとバイアス）の更新\n",
        "    weights, biases = update_params(*model, grads_w, grads_b, lr)\n",
        "\n",
        "    # モデルをアップデート（＝最適化）\n",
        "    model = (layers, weights, biases)\n",
        "\n",
        "    return model, loss, batch_i, (acm_gw, acm_gb)  # イテレーションごと\n",
        "\n",
        "\n",
        "# サンプル実行\n",
        "model, loss_history = train(model, x, y, BATCH_SIZE, EPOCHS, LEARNING_RATE)\n",
        "# 出力例：\n",
        "# [Epoch 1/1] train_loss: 0.31404948868496607"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYJQqNtsPlim"
      },
      "source": [
        "## ■回帰問題を解くデモ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAP3OvdiGT0i"
      },
      "source": [
        "特徴量（入力データ）は$x_1$（X軸）と$x_2$（Y軸）の座標です。その座標点における色、具体的にはオレンジ色（**-1**）～灰色（**0**）～青色（**1**）を予測する回帰問題となります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DFBc6foGjxF"
      },
      "source": [
        "まず訓練データを用意します。このデモの訓練データは、「[回帰問題をディープラーニング（基本のDNN）で解こう](https://atmarkit.itmedia.co.jp/ait/articles/2005/25/news011.html)」でも使っているライブラリー「playground-data」の平面（Plain）データセットです。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSDpoPptvLyk"
      },
      "outputs": [],
      "source": [
        "!pip install playground-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rkn4rwV-vNtz"
      },
      "outputs": [],
      "source": [
        "# playground-dataライブラリのplygdataパッケージを「pg」という別名でインポート\n",
        "import plygdata as pg\n",
        "\n",
        "# 問題種別で「分類（Classification）」を選択し、\n",
        "# データ種別で「2つのガウシアンデータ（TwoGaussData）」を選択する場合の、\n",
        "# 設定値を定数として定義\n",
        "PROBLEM_DATA_TYPE = pg.DatasetType.RegressPlane\n",
        "\n",
        "# 各種設定を定数として定義\n",
        "TRAINING_DATA_RATIO = 0.5  # データの何％を訓練【Training】用に？ (残りは精度検証【Validation】用) ： 50％\n",
        "DATA_NOISE = 0.0           # ノイズ： 0％\n",
        "\n",
        "# 定義済みの定数を引数に指定して、データを生成する\n",
        "data_list = pg.generate_data(PROBLEM_DATA_TYPE, DATA_NOISE)\n",
        "\n",
        "# データを「訓練用」と「精度検証用」を指定の比率で分割し、さらにそれぞれを「データ（X）」と「教師ラベル（y）」に分ける\n",
        "X_train, y_train, _, _ = pg.split_data(data_list, training_size=TRAINING_DATA_RATIO)\n",
        "\n",
        "# それぞれ5件ずつ出力\n",
        "print('X_train:'); print(X_train[:5])\n",
        "print('y_train:'); print(y_train[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODXzS-VOG4KX"
      },
      "source": [
        "次に、モデルを定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyGB3RpcxT79"
      },
      "outputs": [],
      "source": [
        "layers = [2, 3, 1]\n",
        "weights = [\n",
        "    np.array([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]),\n",
        "    np.array([[0.0, 0.0, 0.0]])\n",
        "]\n",
        "biases = [\n",
        "    np.array([0.0, 0.0, 0.0]),\n",
        "    np.array([0.0])\n",
        "]\n",
        "model = (layers, weights, biases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I08F5zRgHJ1S"
      },
      "source": [
        "訓練「前」のモデルによる予測状態を図示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOnl4C1svRCt"
      },
      "outputs": [],
      "source": [
        "# `draw_decision_boundary()`関数がクラス内の`predict()`メソッドを呼び出す仕様のため\n",
        "class MyModel:\n",
        "    def __init__(self, l, w, b):\n",
        "        self.layers = l\n",
        "        self.weights = w\n",
        "        self.biases = b\n",
        "\n",
        "    def predict(self, x, batch_size=1, verbose=False):\n",
        "        probability = []\n",
        "        for each_x in x:\n",
        "            y = forward_prop(self.layers, self.weights, self.biases, each_x)\n",
        "            probability.append(y[0])\n",
        "        return probability\n",
        "\n",
        "# 出力のグラフ表示\n",
        "trained_model = MyModel(*model)\n",
        "fig, ax = pg.plot_points_with_playground_style(X_train, y_train, None, None, figsize = (6, 6), dpi = 100)\n",
        "pg.draw_decision_boundary(fig, ax, trained_model=trained_model, discretize=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRzM-KZdHPIi"
      },
      "source": [
        "それぞれの丸い点の座標は、訓練データ1件1件の特徴量を表します。その点の色が正解ラベルです。例えば左下の座標点でれば、色はオレンジ色、つまり**-1.0**に近い値が正解となります。右上が青色、つまり**1.0**に近い値が正解です。\n",
        "\n",
        "モデルによる予測値は、背景色として描画されています。上の図は全面が灰色です。これは、どの座標を入力しても、**0.0**が予測されることを意味します。これをニューラルネットで学習することで、オレンジ色の座標点の背景色はオレンジ色に、青色の座標点の背景色は青色に描画されるようにします。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9TyZr0_HBSH"
      },
      "source": [
        "最後に、訓練処理の`train()`関数を呼び出すだけです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBqxFJ_2izHm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "BATCH_SIZE = 4   # バッチサイズ\n",
        "EPOCHS = 100     # エポック数\n",
        "LERNING_RATE = 0.02  # 学習係数\n",
        "\n",
        "model, loss_history = train(model, X_train, y_train, BATCH_SIZE, EPOCHS, LEARNING_RATE)\n",
        "\n",
        "# 学習結果（損失）のグラフを描画\n",
        "epochs = len(loss_history)\n",
        "plt.plot(range(1, epochs + 1), loss_history, marker='.', label='loss (Training data)')\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PaQCZIlHn8g"
      },
      "source": [
        "重みやバイアスはモデル（タプル型オブジェクト）の中に格納されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "067jKgb1vasf"
      },
      "outputs": [],
      "source": [
        "print(f'weights={model[1]}'.replace('\\n      ', ''))\n",
        "print(f'biases={model[2]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMVBGWgH0l6"
      },
      "source": [
        "訓練「後」のモデルによる予測状態を図示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy3fXcIZvc3Q"
      },
      "outputs": [],
      "source": [
        "# 出力のグラフ表示\n",
        "trained_model = MyModel(*model)\n",
        "fig, ax = pg.plot_points_with_playground_style(X_train, y_train, None, None, figsize = (6, 6), dpi = 100)\n",
        "pg.draw_decision_boundary(fig, ax, trained_model=trained_model, discretize=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJqY8cPIH-ow"
      },
      "source": [
        "線形代数（NumPy）なしで作ってきた自作のニューラルネットで、確かに回帰問題を解けることが確認できました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY5L18RrOzHb"
      },
      "source": [
        "# お疲れさまでした。線形代数編（第4回）は修了です。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "nn_from_scratch_with_numpy.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "5ed96b83439cf1e3c8183b9c3ff97091f052d98f8413dd1ef2377ac290f63c48"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('Kaggle')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}